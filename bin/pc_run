#!/bin/sh
# CVS: $Id$
#                       pc_run
#                      ---------
#   Run src/run.x (timestepping for src/run.x).
#   Run parameters are set in run.in.
#
# Run this script with sh:
#PBS -S /bin/sh
#$ -S /bin/sh
#@$-s /bin/sh
#
# Join stderr and stout:
#$ -j y -o run.log
#@$-eo
#@$-o run.log
#
# Work in submit directory (SGE):
#$ -cwd
# Work in submit directory (PBS):
[ -e "$PBS_O_WORKDIR" ] && cd $PBS_O_WORKDIR
# Work in submit directory (SUPER-UX's nqs):
[ -e "$QSUB_WORKDIR" ] && cd $QSUB_WORKDIR

# Common setup for start.csh, run.csh, start_run.csh:
# Determine whether this is MPI, how many CPUS etc.
# This implicitly sources pc_functions.sh too.
debug=yes
. pc_config.sh

# Prevent code from running twice (and removing files by accident)
check_not_locked
# If local disc is used, write name into $datadir/directory_snap.
# This will be read by the code, if the file exists.
# Remove file, if not needed, to avoid confusion.
prepare_scratch_disk
# Prevent code from running twice (and removing files by accident)
create_lock

job_finished=no
until [ "$job_finished" = "yes" ]  # Allow NEWDIR job migration
do
  #  If necessary, distribute var.dat from the server to the various nodes
  distribute_data_to_nodes
  run_finished=no
  until [ "$run_finished" = "yes" ]  # Allow RERUN code restart
  do
    # Clean up control and data files
    tidy_rundir
    # On machines with local scratch directory, initialize automatic
    # background copying of snapshots back to the data directory.
    background_copy_snapshots
    # Copy output from `top' on run host to a file we can read from login server
    background_remote_top
    # If necessary copy executable to $SCRATCH_DIR of master node
    distribute_binary src/run.x

    # Write $PBS_JOBID to file (important when run is migrated within the same job)
    save_jobid "RUN STARTED "
    # Run run.x timestamping and timing appropriately
    pencil_code_run
    # Write $PBS_JOBID to file (important when run is migrated within the same job)
    save_jobid "RUN FINISHED"

    check_RERUN
  done

  # On machines with local scratch disc, copy var.dat back to the data directory
  final_copy_snapshots
  # Kill all backgrounded copy-snapshots
  unbackground_copy_snapshots

  check_reference_data
  check_NEWDIR
done

remove_lock
tidy_scratch_disk

# Shut down lam if we have started it
[ "$booted_lam" = "yes" ] && lamhalt
exit $run_status                # propagate status of mpirun

# cut & paste for job submission on the mhd machine
# bsub -n  4 -q 4cpu12h -o run.`timestr` -e run.`timestr` run.csh
# bsub -n  8 -q 8cpu12h mpijob dmpirun src/run.x
# bsub -n 16 -q 16cpu8h mpijob dmpirun src/run.x
# bsub -n  8 -q 8cpu12h -o run.log -w 'exit(123456)' mpijob dmpirun src/run.x

# qsub -l ncpus=64,mem=32gb,walltime=500:00:00 -W group_list=UK07001 -q UK07001 run.csh
# qsub -l nodes=4:ppn=1,mem=500mb,cput=24:00:00 -q p-long run.csh
# qsub -l ncpus=16,mem=1gb,cput=400:00:00 -q parallel run.csh
# qsub -l nodes=128,walltime=10:00:00 -q workq run.csh
# eval `env-setup lam`; qsub -v PATH -pe lam 8 -j y -o run.log run.csh
