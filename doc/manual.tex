%%%%%%%%%%%%%%%%%%%%%%
%%%   manual.tex   %%%
%%%%%%%%%%%%%%%%%%%%%%
%%
%%  Date:   06-Mar-2002
%%  Authors: wd (Wolfgang.Dobler@kis.uni-freiburg.de)
%%           ab (Axel.Brandenburg@nordita.dk)
%%  CVS: $Id: manual.tex,v 1.366 2007-08-19 08:55:28 brandenb Exp $
%%  Description:
%     User manual for the Pencil Code 
%   Process with:
%     latex manual; makeindex -s dotted.idxsty manual; makeindex -s dotted.idxsty manual.vidx -o manual.vind; makeindex -s dotted.idxsty manual.fidx -o manual.find; latex manual
%   Macros for files, variables, etc (list loosely adopted from texinfo):
%
%     \code:    A fragment of code:
%                   use the line \code{call remove\_file()}
%     \kbd:     Keyboard input:
%                   type \kbd{M-x comment-region}
%     \key:     A key  or key combination on your keyboard:
%                   press \key{F1} of \key{C-h}
%     \samp:    Sample input (?)
%                   \samp{a}, \samp{e}, \samp{i}, \samp{o}, \samp{u}
%     \url:     A url:
%                   \url{http://www.nowhere.net/second_page.html}
%     \email:   An email address:
%                   \email{nobody@nowhere.nil}
%
% The following will also be automatically indexed:
%     \name:    Name of a program, object, etc.
%                   \name{object}
%               will put `Object' (capitalized) into the index;
%                   \name[Subject]{object}
%               will put `Subject' into the index;
%     \var:     A variable:
%                   is determined by \var{ivisc}
%     \env:     An environment variable:
%                   this sets \env{CVSROOT}
%     \file:    A file or directory name:
%                   written to \file{var.dat} in \file{data/}
%               Please make sure that the index entries for directory
%               names end in `/'.
%               Like \name, \file accepts an optional argument that will
%               override the index entry:
%                   \file[var.dat]{data/var.dat}
%     \File     Same formatting as \file, but generates no index entry
%               (useful for section headings to avoid weird header lines)
%     \command,
%     \cmd:     A command line:
%                   use \command{rm -f *} at your own risk
%     \option:  An option (command line or similar):
%                   use CVS with the option \option{-q}
%     \dfn:     A definition:
%                   a \dfn{definition} is a specification sufficiently
%                   obfuscated to be misunderstood 
%     \acronym: An acronym (hardly ever used here):
%                   should we call the code \acronym{PROMPT}?

\input{driver_switch}              % sets \mydriver, set up by Makefile

\RequirePackage{ifpdf}
\ifpdf
  \def\mydriver{pdftex}         % anything else make no sense
\else
\fi

\documentclass[\mydriver,12pt,twoside,notitlepage,a4paper]{article}


%\usepackage{url} %(do we not need this; but it's not working anyway)
%\usepackage{german,a4}
%\usepackage[german,british]{babel}
\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{ae}                 % To get good PDF with the T1 encoding
\usepackage{newcent,helvet}
\renewcommand{\ttdefault}{cmtt} % Courier is too broad
\usepackage{amsmath}

\usepackage[it,footnotesize]{caption2}
\setlength{\abovecaptionskip}{5pt} % Space before caption
\setlength{\belowcaptionskip}{5pt} % Space after caption

\usepackage[bf,sf,small,nonindentfirst]{titlesec}

\newcommand{\sectionbreak}{\clearpage} % starts new page for new section
\titleformat{\subsubsection}{\normalfont\itshape}{\thesubsubsection}{.5em}{}
%\titlespacing{\subsubsection}{0pt}{*1}{*-1}
\usepackage{fancyhdr}
\usepackage{fancybox}
\setcounter{tocdepth}{3} % Older versions of fancybox very annoyingly (and
                         % unnecessarily) resets this and make table of
                         % contents disappear

\usepackage{amssymb}
\usepackage{expdlist,booktabs,units,longtable}

\usepackage{fancyvrb}
%\DefineShortVerb{\|}
\usepackage{alltt}
\usepackage{underscore}

\usepackage{graphicx}
\graphicspath{{figs/}}

\usepackage{parskip}%,vmargin}
%\setmargrb{20mm}{25mm}{20mm}{15mm}
\usepackage[hmargin=20mm,top=25mm,bottom=15mm,twosideshift=3mm]{geometry}
\usepackage{multicol}

%% Load hyperref after titlesec, or else (with dvipdfm) the \section links
%% are one or two pages off
\usepackage[\mydriver]{hyperref}
%\usepackage{makeidx}
\usepackage{index}      % Allow for multiple indexes (load after hyperref)

\renewcommand{\textfraction}{0}
\renewcommand{\bottomfraction}{1}
\renewcommand{\floatpagefraction}{1}

\frenchspacing
\sloppy

%%% Multiple, three-column indexes
\makeindex
\newindex{var}{vidx}{vind}{Variable Index}
\newindex{file}{fidx}{find}{File Index}
%% The following is adapted from hyperref.sty and fixes hyperrefs in the
%% index after all of our nasty manipulations:
\makeatletter
  \@ifpackageloaded{hyperref}{%
    \let\HyInd@org@wrindex\@wrindex
    \def\@wrindex#1#2{\HyInd@@wrindex{#1}#2||\\}%
    \def\HyInd@@wrindex#1#2|#3|#4\\{%
      \ifx\\#3\\%
        \HyInd@org@wrindex{#1}{#2|hyperpage}%
      \else
        \def\Hy@temp@A{#3}%
        \ifx\Hy@temp@A\HyInd@ParenLeft
          HyInd@org@wrindex{#1}{#2|#3hyperpage}%
        \else
          \HyInd@org@wrindex{#1}{#2|#3}%
        \fi
      \fi
    }%
  }{}
\makeatother
%% Redefine index to be in three columns (adapted from `index.sty'):
\makeatletter
\renewenvironment{theindex}{%
  \edef\indexname{\the\@nameuse{idxtitle@\@indextype}}%
  \if@twocolumn\@restonecolfalse
  \else\@restonecoltrue
  \fi
  \columnseprule \z@
  \columnsep 35\p@
  \begin{multicols}{3}[\section*{\indexname}%
    \ifx\index@prologue\@empty%
    \else\index@prologue\bigskip
    \fi
  ]%
  \@mkboth{\MakeUppercase\indexname}%
          {\MakeUppercase\indexname}%
  \thispagestyle{plain}%
  \parindent\z@
  \parskip\z@ \@plus .3\p@\relax
  \let\item\@idxitem
}
{ \end{multicols}
  \if@restonecol\onecolumn\else\clearpage\fi
}
\makeatother


%%% Page headings
\pagestyle{fancy}
\renewcommand{\sectionmark}[1]{% Don't upcase the section title
  \markright{\thesection.\ #1}}
\fancyhead{}                    % clear header
\fancyhead[LE,RO]{\thepage}
\fancyhead[CE]{\textsc{The Pencil Code}}
\fancyhead[CO]{\rightmark}
%
\fancyfoot{}

% ---------------------------------------------------------------------- %

%%% Macros

%% Centered table cells for headings
\newcommand{\mcc}[1]{\multicolumn{1}{c}{#1}}

%% Bold face \tt prompts (only works within `alltt' or \tt environment)
\newcommand{\prompt}[1]{{\ttfamily\bfseries{}#1}}

%% Margin and inline notes and remarks
\newcommand\note[1]{\marginpar{\renewcommand{\baselinestretch}{0.8}
        \raggedright\scriptsize\usefont{OT1}{phv}{mc}{n} #1}}
\newcommand{\Note}[1]{\emph{[#1]}}


%% Symbols for dependency tables
%% required
\newcommand{\req}{$\bullet$}  
%% optional
\newcommand{\opt}{$\diamond$}  

%% keys, names, paths, files, etc.
%% Examples: \name{Greeks} \name[Trojans]{Greeks}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\cmd}{\command}
\newcommand{\kbd}[1]{\texttt{\textsl{#1}\/}}
\newcommand{\key}[1]{{\setlength{\fboxsep}{1pt}\ovalbox{\sf #1}}}
\newcommand{\samp}[1]{`\code{#1}'}
%\newcommand{\dfn}[1]{\textsl{#1}\index{#1}\/}
%\newcommand{\cite}[1]{}
\newcommand{\acronym}[1]{\textsc{#1}\index{#1}}
%\newcommand{\url}[1]{}
\newcommand{\email}[1]{\code{#1}}

%\newcommand{\command}[1]{\code{#1}\index{#1}}
%\newcommand{\env}[1]{\code{#1}\index[var]{#1}}
%\newcommand{\file}[1]{`\texttt{#1}'\index{#1@\texttt{#1}}}
%\newcommand{\name}[1]{\textsl{#1}\index{#1}\/}
%\newcommand{\option}[1]{`\code{#1}'\index{Option #1@Option \emph{`#1'}}}
%\newcommand{\var}[1]{\textsl{#1}\index[var]{#1@\emph{#1}}\/}
\makeatletter
\newcommand{\command}[2][]{%
  \def\index@{#1}%
  \code{#2}%
  \ifx\index@\@empty\index{#2@\emph{#2}}%
  \else\index{#1@\emph{#1}}%
  \fi%
}
\newcommand{\env}[2][]{%
  \def\index@{#1}%
  \code{#2}%
  \ifx\index@\@empty\index[var]{#2}%
  \else\index[var]{#1}%
  \fi%
}
\newcommand{\file}[2][]{%
  \def\index@{#1}%
  `\texttt{#2}'%
  \ifx\index@\@empty\index[file]{#2@\texttt{#2}}%
  \else\index[file]{#1@\texttt{#1}}%
  \fi%
}
\newcommand{\File}[2][]{%
  `\texttt{#2}'%
}
\newcommand{\name}[2][]{%
  \def\index@{#1}%
  \textsl{#2\/}%
  \ifx\index@\@empty\index{#2@\MakeUppercase #2}%
  \else\index{#1}%
  \fi%
}
\newcommand{\option}[2][]{%
  \def\index@{#1}%
  `\code{#2}'%
  \ifx\index@\@empty\index{Option #2@Option \emph{`#2'}}%
  \else\index{Option #1@Option \emph{`#1'}}%
  \fi%
}
\newcommand{\var}[2][]{%
  \def\index@{#1}%
  \textsl{#2\/}%
  \ifx\index@\@empty\index[var]{#2@\emph{#2}}%
  \else\index[var]{#1@\emph{#1}}%
  \fi%
}
\makeatother
\newcommand{\dfn}{\name}
\newcommand{\Path}[1]{\file{#1}}

%
\newcommand{\bsT}{{\fontencoding{T1}\selectfont{\symbol{92}}}}
\newcommand{\bcks}{\texttt{\symbol{92}}}
\newcommand{\bs}{\bcks}       % Save us creation of a couple of fonts

% ---------------------------------------------------------------------- %
%% Maths operators

\newcommand{\de}      {\mathrm{d}}
\newcommand{\De}      {\mathrm{D}}
% \newcommand{\arcosh} {\mathop{\rm arcosh}\nolimits}
% \newcommand{\arcoth} {\mathop{\rm arcoth}\nolimits}
\newcommand{\artanh}  {\mathop{\rm artanh}\nolimits}
% \newcommand{\sgn}    {\mathop{\rm sgn}\nolimits}
% \newcommand{\grad}    {\mathop{\rm grad}\nolimits}
% \newcommand{\Div}     {\mathop{\rm div}\nolimits}
% \newcommand{\curl}    {\mathop{\rm curl}\nolimits}
% \newcommand{\Laplace} {\mathop{\Delta}\nolimits}
\newcommand{\grad}    {\nabla}
\newcommand{\Div}     {\nabla\cdot}
\newcommand{\curl}    {\nabla\times}
\newcommand{\Laplace} {\nabla^2}
\newcommand{\rot}     {\curl}
\newcommand{\erfc}    {\mathop{\rm erfc}\nolimits}
\newcommand{\erf}     {\mathop{\rm erf}\nolimits}

\newcommand{\vekt}[1] {\mathbf{#1}}
\renewcommand{\vec}[1]{{\boldsymbol #1}}
\newcommand{\const}   {\mbox{\rm const}}

%% Maths variables

%% Vectors
\newcommand{\Av}            {\vekt{A}}
% \newcommand{\av}            {\vekt{a}}
\newcommand{\Bv}            {\vekt{B}}
\newcommand{\Jv}            {\vekt{J}}
\newcommand{\Uv}            {\vekt{U}}
% \newcommand{\bv}            {\vekt{b}}
\newcommand{\ev}            {\vekt{e}}
% \newcommand{\Ev}            {\vekt{E}}
% \newcommand{\ex}            {\ev_{x}}
% \newcommand{\ey}            {\ev_{y}}
% \newcommand{\ez}            {\ev_{z}}
\newcommand{\Fv}            {\vekt{F}}
\newcommand{\fv}            {\vekt{f}}
\newcommand{\gv}            {\vekt{g}}
\newcommand{\jv}            {\vekt{j}}
\newcommand{\kv}            {\vekt{k}}
\newcommand{\uv}            {\vekt{u}}
\newcommand{\xv}            {\vekt{x}}
\newcommand{\zerovect}      {\vekt{0}}
\newcommand{\omv}           {\boldsymbol{\omega}}

% Reynolds numbers
\newcommand{\Ra}            {\mathrm{Ra}}
\newcommand{\Reynolds}      {\mathrm{Re}}
\newcommand{\Rm}            {\mathrm{Rm}}

% Heating and Cooling
\newcommand{\Heat}          {{\cal H}}
\newcommand{\Heavi}         {\theta}
\newcommand{\Cool}          {{\cal C}}

% Sound Speed
\newcommand{\cs}            {c_{\rm s}}
\newcommand{\csnull}        {c_{{\rm s},0}}

% Strain Tensor
\newcommand{\Strain}        {\boldsymbol{\mathsf{S}}}


% \newcommand{\Vol}           {{\cal V}}
% Alfven Speed
\newcommand{\vA}            {v_{\rm A}}

\newcommand{\bra}[1]{\langle #1\rangle}
\newcommand{\Eq}[1]{Eq.~(\ref{#1})}
\newcommand{\nab}{\mbox{\boldmath $\nabla$} {}}
\newcommand{\dd}{{\rm d} {}}

\newcommand{\Bhat}{\hat{B}}
\newcommand{\BBhat}{\hat{\vekt{B}}}

\newcommand{\EE}[1]{\,{\times}\,10^{#1}}

\def\la{\mathrel{\mathchoice {\vcenter{\offinterlineskip\halign{\hfil
$\displaystyle##$\hfil\cr<\cr\sim\cr}}}
{\vcenter{\offinterlineskip\halign{\hfil$\textstyle##$\hfil\cr<\cr\sim\cr}}}
{\vcenter{\offinterlineskip\halign{\hfil$\scriptstyle##$\hfil\cr<\cr\sim\cr}}}
{\vcenter{\offinterlineskip\halign{\hfil$\scriptscriptstyle##$\hfil\cr<\cr\sim\cr}}}}}
\def\ga{\mathrel{\mathchoice {\vcenter{\offinterlineskip\halign{\hfil
$\displaystyle##$\hfil\cr>\cr\sim\cr}}}
{\vcenter{\offinterlineskip\halign{\hfil$\textstyle##$\hfil\cr>\cr\sim\cr}}}
{\vcenter{\offinterlineskip\halign{\hfil$\scriptstyle##$\hfil\cr>\cr\sim\cr}}}
{\vcenter{\offinterlineskip\halign{\hfil$\scriptscriptstyle##$\hfil\cr>\cr\sim\cr}}}}}

% ---------------------------------------------------------------------- %

%\title{{\sffamily\bfseries Installing and Using the High-Order Pencil MPI code}}
%\subtitle{A very preliminary manual}
%\author{Wolfgang Dobler \& Axel Brandenburg}
%\date{$ $Date: 2007-08-19 08:55:28 $ $,~ $ $Revision: 1.366 $ $}

% ====================================================================== %

\begin{document}
\pagestyle{empty}
\pagestyle{plain}
\pagenumbering{roman}

%\maketitle

\begin{titlepage}
  \begin{center}

  \large

  \vspace*{2cm}

  {\Large\sffamily\bfseries The Pencil Code:\\[2\parskip]
    A High-Order MPI code for MHD Turbulence}

  \vspace{3ex}

  {\sffamily User's and Reference Manual}

  \vspace{\stretch{2}}

  \centerline{\includegraphics[angle=-90,width=0.9\textwidth]{pencils}}

  \vspace{\stretch{2}}

  %\htmladdnormallink{Wolfgang Dobler}{http://www.kis.uni-freiburg.de/~dobler/}
  %\&
  %\htmladdnormallink{Axel Brandenburg}{http://www.nordita.dk/~brandenb/}

  \vspace{\stretch{1}}

  \emph{$ $Date: 2007-08-19 08:55:28 $ $,~ $ $Revision: 1.366 $ $}\\
  \url{http://www.nordita.dk/software/pencil-code/}

  \vspace{\stretch{3}}


\end{center}

\end{titlepage}


\newpage
\mbox{}

\begin{center}
  {\bf The Pencil Code: multi-purpose and multi-user maintained}\\
  \url{http://www.nordita.dk/~brandenb/talks/misc/PencilCode04.htm}
\end{center}

% ---------------------------------------------------------------------- %
\begin{figure}[htb]
  \centering
  \includegraphics%
    [width=.99\textwidth,keepaspectratio]%
    {cvsstat}
  \caption{Check-in patterns as a function of time for different
  subroutines. The different users are marked by different symbols
  and different colors.
  }
  \label{cvsstat}
\end{figure}
% ---------------------------------------------------------------------- %

\newpage

\begin{center}
  {\bf Contributors to the code}\\
  (in inverse alphabetical order according to their CVS user name)\\
\end{center}

\begin{tabular}{lll}
  wlyra & \htmladdnormallink{Wladimir Lyra}{http://www.astro.uu.se/~wlyra/} & University of Uppsala\\
  weezy & \htmladdnormallink{S. Louise Wilkin}{http://www.mas.ncl.ac.uk/~n9405169/} & University of Newcastle\\
  vpariev & \htmladdnormallink{Vladimir Pariev}{http://www.pas.rochester.edu/~vpariev/} & University of Rochester\\
  torkel & \htmladdnormallink{Ulf Torkelsson}{http://fy.chalmers.se/~torkel/} & Chalmers University\\
  theine & \htmladdnormallink{Tobias (Tobi) Heinemann}{http://www.nordita.dk/~theine/} & Nordita and University of Copenhagen\\
  tarek & \htmladdnormallink{Tarek A. Yousef}{http://www.pvv.ntnu.no/~tarek/} & University of Trondheim\\
  steveb & \htmladdnormallink{Steve Berukoff}{http://www.physics.ucla.edu/~steveb/} & UCLA \\
  snod & \htmladdnormallink{Andrew Snodin}{http://www.ncl.ac.uk/math/postgrad/postgrads.htm} & University of Newcastle\\
  pkapyla & \htmladdnormallink{Petri K\"apyl\"a}{http://cc.oulu.fi/~pkapyla/} & Oulu University and Kiepenheuer Institute\\
  nils & \htmladdnormallink{Nils Erland L. Haugen}{http://www.phys.ntnu.no/~nilshau/index2.html} & University of Trondheim\\
  ngrs & \htmladdnormallink{Graeme R. Sarson}{http://www.mas.ncl.ac.uk/~ngrs/home.html} & University of Newcastle\\
  nbabkovs & \htmladdnormallink{Natalia Babkovskaia}{http://www.nordita.dk/~nbabkovs/} & Nordita\\
  mkorpi  & \htmladdnormallink{Maarit J.\ Korpi}{http://www.helsinki.fi/~mkorpi/} & University of Helsinki\\
  mee  & \htmladdnormallink{Antony (tOnY) Mee}{http://www.mas.ncl.ac.uk/~n7026413/pencil-code/movies/} & University of Newcastle\\
  mcmillan & \htmladdnormallink{David McMillan}{http://brunhes.eas.yorku.ca/dave/CV/} & York University, Toronto\\
  mattias & \htmladdnormallink{Mattias Christensson}{http://www.nordita.dk/~mattias/} & formerly at Nordita\\
  karlsson & \htmladdnormallink{Torgny Karlsson}{http://www.nordita.dk/people/people.php?variant=single\&cn=Torgny+Karlsson} & Nordita\\
  dhruba & \htmladdnormallink{ Dhrubaditya Mitra}{http://www.maths.qmul.ac.uk/~dhruba/} & Queen Mary College\\
  dorch & \htmladdnormallink{Bertil Dorch}{http://www.astro.ku.dk/~dorch/} & University of Copenhagen\\
  dobler & \htmladdnormallink{Wolfgang Dobler}{http://www.kis.uni-freiburg.de/~dobler/} & University of Calgary\\
  dintrans & \htmladdnormallink{Boris Dintrans}{http://www.ast.obs-mip.fr/dintrans} &  Observatoire Midi-Pyr\'en\'ees, Toulouse\\
  christer & \htmladdnormallink{Christer Sandin}{http://www.astro.uu.se/~christer/CS_index.html} & University of Uppsala\\
  brandenb & \htmladdnormallink{Axel Brandenburg}{http://www.nordita.dk/~brandenb/} & Nordita\\
  bing & \htmladdnormallink{Sven Bingert}{http://www.kis.uni-freiburg.de/~peter/corona/} & Kiepenheuer Institute\\
  apichat & \htmladdnormallink{Apichat Neamvonk}{Apichat.Neamvonk@ncl.ac.uk} & University of Newcastle\\
  amjed & \htmladdnormallink{Amjed Mohammed}{http://ehf.uni-oldenburg.de/member.php?nav=staff\&sprache=english\&show=43} & University of Oldenburg\\
  ajohan & \htmladdnormallink{Anders Johansen}{http://www.mpia-hd.mpg.de/homes/johansen/} & Max Planck Institute for Astronomy, Heidelberg\\
\end{tabular}

\vfill

Copyright \copyright{} 2001--2007 Wolfgang Dobler \& Axel Brandenburg
\bigskip

Permission is granted to make and distribute verbatim copies of
this manual provided the copyright notice and this permission notice
are preserved on all copies.

Permission is granted to copy and distribute modified versions
of this manual under the conditions for verbatim copying,
provided that the entire resulting derived work is distributed under the
terms of a permission notice identical to this one.

\clearpage

\begin{center}
  {\bf License agreement and giving credit}\\
\end{center}

The content of all files under
\code{:pserver:\$USER@130.225.213.198:/var/cvs/brandenb}
are under the GNU General Public License
(\url{http://www.gnu.org/licenses/gpl.html}).

We, the {\textsc Pencil Code} community, ask that in publications
and presentations the use of the code (or parts of it)
be acknowledged with reference to the web site
\url{http://www.nordita.dk/software/pencil-code/}.
As a courtesy to the people involved in developing particularly important
parts of the program (use \cmd{cvs annotate src/*.f90} to find out who did what!)
we suggest to give appropriate reference to one or
several of the following papers:

\newcommand{\astroph}[1]{\htmladdnormallink{\sf astro-ph/#1}{http://arXiv.org/abs/astro-ph/#1}}
\newcommand{\condmat}[1]{\htmladdnormallink{\sf cond-mat/#1}{http://arXiv.org/abs/abs/cond-mat/#1}}
\newcommand{\physics}[1]{\htmladdnormallink{\sf physics/#1}{http://arXiv.org/abs/abs/physics/#1}}
\newcommand{\qbio}[1]{\htmladdnormallink{\sf q-bio/#1}{http://arXiv.org/abs/abs/q-bio/#1}}
\newcommand{\yannr}[5]{:~#1, ``#5,'' {\em Ann.\ Rev.\ Astron.\ Astrophys. }{\bf #2}, #3-#4}
\newcommand{\yica}[5]{:~#1, ``#5,'' {\em Icarus }{\bf #2}, #3-#4}
\newcommand{\ysph}[5]{:~#1, ``#5,'' {\em Solar Phys. }{\bf #2}, #3-#4}
\newcommand{\ysphs}[5]{:~#1, ``#5'' {\em Solar Phys. }{\bf #2}, #3-#4}
\newcommand{\ymn}[5]{:~#1, ``#5,'' {\em Monthly Notices Roy. Astron. Soc. }{\bf #2}, #3-#4}
\newcommand{\yan}[5]{:~#1, ``#5,'' {\em Astron. Nachr. }{\bf #2}, #3-#4}
\newcommand{\yana}[5]{:~#1, ``#5,'' {\em Astron. Astrophys. }{\bf #2}, #3-#4}
\newcommand{\yanas}[5]{:~#1, ``#5'' {\em Astron. Astrophys. }{\bf #2}, #3-#4}
\newcommand{\yass}[5]{:~#1, ``#5,'' {\em Astrophys. Spa. Sci. }{\bf #2}, #3-#4}
\newcommand{\yapj}[5]{:~#1, ``#5,'' {\em Astrophys. J. }{\bf #2}, #3-#4}
\newcommand{\yapjl}[5]{:~#1, ``#5,'' {\em Astrophys. J. Lett. }{\bf #2}, #3-#4}
\newcommand{\yapjlS}[5]{:~#1, ``#5'' {\em Astrophys. J. Lett. }{\bf #2}, #3-#4}
\newcommand{\yjfm}[5]{:~#1, ``#5,'' {\em J. Fluid Mech. }{\bf #2}, #3-#4}
\newcommand{\ypepi}[5]{:~#1, ``#5,'' {\em Phys. Earth Planet. Int.}{\bf #2}, #3-#4}
\newcommand{\ygafd}[5]{:~#1, ``#5,'' {\em Geophys. Astrophys. Fluid Dyn. }{\bf #2}, #3-#4}
\newcommand{\sproc}[5]{:~#1, ``#2,'' in {\em #3}, ed.\ #4, #5 (submitted)}
\newcommand{\eproc}[6]{:~#1, ``#2,'' in {\em #3}, \url{#4}, #5}
\newcommand{\pproc}[5]{:~#1, ``#2,'' in {\em #3}, ed.\ #4, #5 (in press)}
\newcommand{\ypr}[5]{:~#1, ``#5,'' {\em Phys. Rev. }{\bf #2}, #3-#4}
\newcommand{\yprN}[5]{:~#1, ``#5,'' {\em Phys. Rev. }{\bf #2}, #3, #4}
\newcommand{\yproc}[7]{:~#1, ``#4,'' in {\em #5}, ed.\ #6, #7, pp.~#2-#3}
\newcommand{\yjourS}[6]{:~#1, ``#6'' {\em #2} {\bf #3}, #4-#5}
\newcommand{\yjour}[6]{:~#1, ``#6,'' {\em #2} {\bf #3}, #4-#5}
\newcommand{\sjour}[3]{:~#1, ``#3,'' {\em #2} (submitted)}
\newcommand{\sjourS}[3]{:~#1, ``#3'' {\em #2} (submitted)}
\newcommand{\pjour}[3]{:~#1, ``#3,'' {\em #2} (in press)}
\newcommand{\pjourS}[3]{:~#1, ``#3'' {\em #2} (in press)}
\newcommand{\ybook}[3]{:~#1,  {\em #2}. #3}
\newcommand{\ypf}[5]{:~#1, ``#5,'' {\em Phys. Fluids }{\bf #2}, #3-#4}
\newcommand{\ypp}[5]{:~#1, ``#5,'' {\em Phys. Plasmas }{\bf #2}, #3-#4}
\newcommand{\yepl}[5]{:~#1, ``#5,'' {\em Europhys. Lett. }{\bf #2}, #3-#4}
\newcommand{\yprl}[5]{:~#1, ``#5,'' {\em Phys. Rev. Lett. }{\bf #2}, #3-#4}
\newcommand{\ybif}[5]{:~#1, ``#5,'' {\em Int. J. Bifurc. Chaos }{\bf #2}, #3-#4}
\newcommand{\ycsf}[5]{:~#1, ``#5,'' {\em Chaos, Solitons \& Fractals }{\bf #2}, #3-#4}
\newcommand{\ycsfS}[5]{:~#1, ``#5'' {\em Chaos, Solitons \& Fractals }{\bf #2}, #3-#4}
\newcommand{\sprl}[2]{:~#1, ``#2,'' {\em Phys. Rev. Lett.} (submitted)}
\newcommand{\pprl}[2]{:~#1, ``#2,'' {\em Phys. Rev. Lett.} (in press)}
\newcommand{\tpr}[3]{:~#1, ``#3,'' {\em Phys. Rev. }{\bf #2} (to be submitted)}
\newcommand{\spr}[3]{:~#1, ``#3,'' {\em Phys. Rev. }{\bf #2} (submitted)}
\newcommand{\ppr}[3]{:~#1, ``#3,'' {\em Phys. Rev. }{\bf #2} (in press)}
\newcommand{\pppp}[3]{:~#1, ``#2,'' {\em Phys. Plasmas} (in press, scheduled for the #3 issue)}
\newcommand{\ppp}[2]{:~#1, ``#2,'' {\em Phys. Plasmas} (in press)}
\newcommand{\spp}[2]{:~#1, ``#2,'' {\em Phys. Plasmas} (submitted)}
\newcommand{\tpp}[2]{:~#1, ``#2,'' {\em Phys. Plasmas} (to be submitted)}
\newcommand{\tppS}[2]{:~#1, ``#2'' {\em Phys. Plasmas} (to be submitted)}
\newcommand{\san}[2]{:~#1, ``#2,'' {\em Astron. Nachr.} (submitted)}
\newcommand{\pan}[2]{:~#1, ``#2,'' {\em Astron. Nachr.} (in press)}
\newcommand{\sana}[2]{:~#1, ``#2,'' {\em Astron. Astrophys.} (submitted)}
\newcommand{\tana}[2]{:~#1, ``#2,'' {\em Astron. Astrophys.} (to be submitted)}
\newcommand{\sanas}[2]{:~#1, ``#2'' {\em Astron. Astrophys.} (submitted)}
\newcommand{\pana}[2]{:~#1, ``#2,'' {\em Astron. Astrophys.} (in press)}
\newcommand{\panas}[2]{:~#1, ``#2'' {\em Astron. Astrophys.} (in press)}
\newcommand{\sgafd}[2]{:~#1, ``#2,'' {\em Geophys. Astrophys. Fluid Dyn.} (submitted)}
\newcommand{\pgafd}[2]{:~#1, ``#2,'' {\em Geophys. Astrophys. Fluid Dyn.} (in press)}
\newcommand{\ppgafd}[3]{:~#1, ``#3,'' {\em Geophys. Astrophys. Fluid Dyn.} {\bf #2}}
\newcommand{\tapj}[2]{:~#1, ``#2,'' {\em Astrophys. J.} (to be submitted)}
\newcommand{\ppapj}[2]{:~#1, ``#2,'' {\em Astrophys. J.} (in press)}
\newcommand{\ppapjl}[2]{:~#1, ``#2,'' {\em Astrophys. J. Lett.} (in press)}
\newcommand{\sapj}[2]{:~#1, ``#2,'' {\em Astrophys. J.} (submitted)}
\newcommand{\sapjS}[2]{:~#1, ``#2'' {\em Astrophys. J.} (submitted)}
\newcommand{\ppapjS}[2]{:~#1, ``#2'' {\em Astrophys. J.} (in press)}
\newcommand{\sapjl}[2]{:~#1, ``#2,'' {\em Astrophys. J. Lett.} (submitted)}
\newcommand{\ppapjlS}[2]{:~#1, ``#2'' {\em Astrophys. J. Lett.} (in press)}
\newcommand{\sapjlS}[2]{:~#1, ``#2'' {\em Astrophys. J. Lett.} (submitted)}
\newcommand{\papj}[4]{:~#1, ``#3,'' {\em Astrophys. J. }{\bf #2} (scheduled for the #4 issue)}
\newcommand{\papjS}[4]{:~#1, ``#3'' {\em Astrophys. J. }{\bf #2} (scheduled for the #4 issue)}
\newcommand{\papjl}[4]{:~#1, ``#3,'' {\em Astrophys. J. Lett. }{\bf #2} (scheduled for the #4 issue)}
\newcommand{\papjlS}[4]{:~#1, ``#3'' {\em Astrophys. J. Lett. }{\bf #2} (scheduled for the #4 issue)}
\newcommand{\spf}[2]{:~#1, ``#2,'' {\em Phys. Fluids} (submitted)}
\newcommand{\ppf}[2]{:~#1, ``#2,'' {\em Phys. Fluids} (in press)}
\newcommand{\ssph}[2]{:~#1, ``#2,'' {\em Solar Phys.} (submitted)}
\newcommand{\psph}[2]{:~#1, ``#2,'' {\em Solar Phys.} (in press)}
\newcommand{\sbif}[2]{:~#1, ``#2,'' {\em Int. J. Bifurc. Chaos} (submitted)}
\newcommand{\pbif}[2]{:~#1, ``#2,'' {\em Int. J. Bifurc. Chaos} (in press)}
\newcommand{\pjfm}[2]{:~#1, ``#2,'' {\em J. Fluid Mech. } (in press)}
\newcommand{\sjfm}[2]{:~#1, ``#2,'' {\em J. Fluid Mech. } (submitted)}
\newcommand{\tjfm}[2]{:~#1, ``#2,'' {\em J. Fluid Mech. } (to be submitted)}
\newcommand{\pmn}[2]{:~#1, ``#2,'' {\em Monthly Notices Roy. Astron. Soc.} (in press)}
\newcommand{\smn}[2]{:~#1, ``#2,'' {\em Monthly Notices Roy. Astron. Soc.} (submitted)}
\newcommand{\tmn}[2]{:~#1, ``#2,'' {\em Monthly Notices Roy. Astron. Soc.} (to be submitted)}

\begin{list}{}{\leftmargin 3em \itemindent -3em\listparindent \itemindent
\itemsep 0pt \parsep 1pt}\item[]

Dobler, W., Haugen, N. E. L., Yousef, T. A., \&
Brandenburg, A.\yprN{2003}{E 68}{026304}
{1-8}{Bottleneck effect in three-dimensional turbulence simulations}
(\astroph{0303324})

Haugen, N. E. L., Brandenburg, A., \& Dobler, W.\yapjlS{2003}{597}{L141}
{L144}{Is nonhelical hydromagnetic turbulence peaked at small scales?}
(\astroph{0303372})

Brandenburg, A., K\"apyl\"a, P., \& Mohammed, A.\ypf{2004}{16}{1020}
{1027}{Non-Fickian diffusion and tau-approximation from numerical turbulence}
(\astroph{0306521})

Johansen, A., Andersen, A. C., \& Brandenburg, A.\yana{2004}{417}{361}
{371}{Simulations of dust-trapping vortices in protoplanetary discs}
(\astroph{0310059})

Haugen, N. E. L., Brandenburg, A., \& Mee, A. J.\ymn{2004}{353}{947}
{952}{Mach number dependence of the onset of dynamo action}
(\astroph{0405453})

Brandenburg, A., \& Multam\"aki, T.\yjourS{2004}{Int.\ J.\ Astrobiol.}{3}{209}
{219}{How long can left and right handed life forms coexist?}
(\qbio{0407008})

McMillan, D. G., \& Sarson, G. R.\ypepi{2005}{153}{124}
{135}{Dynamo simulations in a spherical shell of ideal gas using a 
high-order cartesian magnetohydrodynamics code}

Heinemann, T., Dobler, W., Nordlund, \AA., \&
Brandenburg, A.\yana{2006}{448}{731}
{737}{Radiative transfer in decomposed domains}
(\astroph{0503510})

Dobler, W., Stix, M., \& Brandenburg, A.\yapj{2006}{638}{336}
{347}{Convection and magnetic field generation in fully convective spheres}
(\astroph{0410645})

Snodin, A. P., Brandenburg, A., Mee, A. J., \& Shukurov, A.\ymn{2006}{373}{643}
{652}{Simulating field-aligned diffusion of a cosmic ray gas}
(\astroph{0507176})

Johansen, A., Klahr, H., \& Henning, Th.\yapj{2006}{636}{1121}
{1134}{Dust sedimentation and self-sustained Kelvin-Helmholtz turbulence
in protoplanetary disc mid-planes}
(\astroph{0512272})

de Val-Borro, M. and 22 coauthors (incl.\ Lyra, W.)\ymn{2006}{370}{529}
%Edgar, R. G., Artymowicz, P., Ciecielag, P., Cresswell,
%P., D'Angelo, G., Delgado-Donate, E. J., Dirksen, G., Fromang, S.,
%Gawryszczak, A., Klahr, H., Kley, W., Lyra, W., Masset, F., Mellema, G.,
%Nelson, R. P., Paardekooper, S. -J., Peplinski, A., Pierens, A., Plewa,
%T., Rice, K., Schaefer, C., \& Speith, R.\pmn{2006}
{558}{A comparative study of disc-planet interaction}
(\astroph{0605237})

\end{list}

\clearpage

% ====================================================================== %

\section*{Foreword}

This code was originally developed at the Turbulence Summer School of the
Helmholtz Institute in Potsdam (2001).
While some SPH and PPM codes for hydrodynamics and magnetohydrodynamics
are publicly available, this does not generally seem to be
the case for higher order finite-difference or spectral codes.
Having been approached by people interested in using our code, we
decided to make it as flexible as possible and as user-friendly as seems
reasonable, and to put it onto a public \name{CVS} repository.
The code can certainly not be treated as a black box (no code can), and in
order to solve a new problem in an optimal way, users will need to find their
own optimal set of parameters.
In particular, you need to be careful in choosing
the right values of viscosity, magnetic diffusivity, and radiative
conductivity.

The Pencil Code is primarily designed to deal with weakly compressible
turbulent flows, which is why we use high-order first and second derivatives.
To achieve good parallelization, we use explicit
(as opposed to compact) finite differences.
Typical scientific targets include driven MHD turbulence in a periodic box,
convection in a slab with non-periodic upper and lower boundaries,
a convective star embedded in a fully nonperiodic box, accretion disc
turbulence in the shearing sheet approximation, etc.

Magnetic fields are implemented in terms of the magnetic vector potential
to ensure that the field remains solenoidal (divergence-free).
At the same time, having the magnetic
vector potential readily available is a big advantage if
one wants to monitor the magnetic helicity, for example.
The code is therefore particularly well suited for all kinds of
dynamo problems.

The code is non-conservative; thus, conserved quantities should only be
conserved up to the discretization error of the scheme (not to machine
accuracy).
There is no guarantee that a conservative code is more accurate with
respect to quantities that are not explicitly conserved, such as entropy.
Another important quantity that is (to our knowledge) not strictly
conserved by ordinary flux conserving schemes is \name{magnetic helicity}.

There are currently no plans to implement adaptive mesh refinement
into the code, which would cause major technical complications.
Given that turbulence is generically space-filling, local refinement
to smaller scales would often not be very useful anyway.
On the other hand, in some geometries
turbulence may well be confined to certain regions in space, so one
could indeed gain by solving the outer regions with fewer points.

In order to be cache-efficient, we solve the equations along
\name{pencils} in the $x$ direction.
One very convenient side-effect is that auxiliary and derived variables
use very little memory, as they are only ever defined on one pencil.
The domain can be tiled in the $y$ and $z$ directions.
On multiprocessor computers, the code can use \name{MPI}
(Message Passing Interface) calls to communicate between processors.
An easy switching mechanism allows the user to run the code on a machine
without MPI libraries (e.g.~a notebook computer).
Ghost zones are used to implement boundary conditions on physical and
processor boundaries.

A high level of flexibility is achieved by encapsulating individual
physical processes and variables in individual \name{modules}, which can
be switched on or off in the file \file{Makefile.local} in the local
\file[src/]{src} directory.
This approach avoids the use of difficult-to-read preprocessor directives,
at the price of requiring one dummy module for each physics module.
For nonmagnetic hydrodynamics, for example, one will use the module
\file{nomagnetic.f90} and specifies
\begin{Verbatim}
  MAGNETIC=nomagnetic
\end{Verbatim}
in \file{Makefile.local},
while for MHD simulations, \file{magnetic.f90} will be used:
\begin{Verbatim}
  MAGNETIC=magnetic
\end{Verbatim}
Note that the term \name{module} as used here is only loosely related to
Fortran modules:
both \file{magnetic.f90} and \file{nomagnetic.f90} define an F90
module named \emph{Magnetic} --- this is the basis of the switching
mechanism we are using.

Input parameters (which are set in the files \file{start.in},
\file{run.in}) can be changed without recompilation.
Furthermore, one can change the list of variables for monitoring
(diagnostic) output on the fly, and there are mechanisms for making the
code reload new parameters or exit gracefully at runtime.

The requirements for using the Pencil-MPI code are modest: you can use it
on any Unix system with a F90/F95 compiler. If you have \name{IDL} as
well, you will be able to visualize the results (a number of sample
procedures are provided), but other tools such as
\name{DX} (OpenDX, data explorer) can also be used.

\bigskip

If you want to make creative use of the code, this manual will contain far
too little information.
Its major aim is to give you an idea of the way the code is organized, so
you can more efficiently \emph{read the source code}, which contains a
reasonable amount of comments.
You might want to read through the various run directories that are checked in.
Choose one that is closest to your application and start modifying.
For further enhancements that you may want to add to the code, you can
take as an example the lines in the code that deal with related variables,
functions, diagnostics, equations etc., which have already been implemented.
Just remember: \cmd{grep} is one of your best friends when you want to
understand how certain variables or functions are used in the code.

\bigskip

We will be happy to include user-supplied changes and updates to the code
in future releases and welcome any feedback.

\vspace{5mm}
%\noindent
\email{Wolfgang.Dobler@kis.uni-freiburg.de}\hfill Freiburg\\
\email{Axel.Brandenburg@nordita.dk}\hfill Copenhagen


% ====================================================================== %

\section*{Acknowledgments}

Many people have contributed in different ways to the development of this
code. We thank first of all {\AA}ke Nordlund (Copenhagen Observatory)
and Bob Stein (University of Michigan) who introduced us to the idea of
using high-order schemes in compressible flows and who taught us a lot
about simulations in general.

The calculation of the power spectra, structure functions,
the remeshing procedures,
routines for changing the number of processors, as well as
the shearing sheet approximation and the flux-limited diffusion
approximation for radiative transfer
were implemented by Nils Erland L.\ Haugen (University of Trondheim).
Tobi Heinemann added the long characteristics method for
radiative transfer as well as hydrogen ionization.
He also added and/or improved shock diffusion for other variables
and improved the resulting timestep control.
Anders Johansen and Anja Andersen contributed to the implementation of
the dust equations (which now comprises an array of different components).
Antony (Tony) Mee (University of Newcastle) implemented shock viscosity
and added the interstellar module together with
Graeme R.\ Sarson (also University of Newcastle), who also implemented
the geodynamo set-up together with David McMillan
(currently also at the University of Newcastle).
Tony also included a method for outputting auxiliary variables and
enhanced the overall functionality of the code and related idl and dx
procedures.
He also added, together with Andrew Snodin, the evolution equations
for the cosmic ray energy density.
Vladimir Pariev (University of Rochester) contributed to the development
and testing of the potential field boundary condition.

Use of the PPARC supported supercomputers in St Andrews (Mhd) and
Leicester (Ukaff) is acknowledged. We also acknowledge the Danish Center
for Scientific Computing for granting time on Horseshoe, which is a
512+140 processor Beowulf cluster in Odense (Horseshoe).

\clearpage
\tableofcontents
\cleardoublepage
\pagestyle{fancy}
\pagenumbering{arabic}


% ====================================================================== %

\part{Using the Pencil Code}

\section{System Requirements}

To use the code, you will need the following:

\begin{enumerate}

  \item Absolutely needed:
    \begin{itemize}
    \item F90 or F95 compiler
    \end{itemize}

\item Used heavily (if you don't have one of these, you will need to
  adjust many things manually):
  \begin{itemize}
  \item a \name{Unix}-type system with \name{make} and \name{csh}
  \item \name{Perl} (remember: if it doesn't run Perl, it's not a
    computer)
  \end{itemize}

\item The following are dispensable, but enhance functionality in one
  way or the other:
  \begin{itemize}
  \item an \name{MPI} implementation (for parallelization on
    multiprocessor systems)
  \item a \name{C} compiler (for some debugging functionality)
  \item \name{DX} alias \name{OpenDX} or \name{data explorer} (for
    3-D visualization of results)
  \item \name{IDL} (for visualization of results; the 7-minute demo
    license will do for many applications)
  \end{itemize}

\end{enumerate}

%If you like the exotic and get the code running in a \name{Cygwin}
%environment, please let us know.


% ====================================================================== %

\section{Obtaining the Code}

There are two ways to obtain the code:
you can access the latest version via anonymous \name{CVS}\index{CVS},
or you can download the tarball of a stable release.
Older tarballs are kept, and the correspondingly old version
of the manual is also checked in, but the manual on the web
refers of course to a recent version.

Since we are still actively developing the code (and probably always
will), it is important to be able to update your version to the current
one.
You can do so via \name{CVS}, regardless of whether you have initially
installed the tarball or have used \name{CVS} from the very start.

To ensure at least some level of stability of the \name{CVS}
versions, a set of test problems (listed in
\file[auto-test]{\$PENCIL_HOME/bin/auto-test})
are routinely tested.
This includes all problems in
\file[samples/]{\$PENCIL_HOME/samples}.

% ---------------------------------------------------------------------- %

\subsection{Obtaining the code via anonymous CVS}
\index{CVS}

\begin{enumerate}

\item Many machines have \name{CVS} installed
  (try \code{cvs -v} or \code{which cvs}).
  If yours have not, get a \name{CVS} client from
  \url{http://www.cvshome.org/} and
  install it on any machine where you want to run the code.
  Alternatively, just copy the executable from a binary compatible
  machine.

\item Get the password for anonymous \name{CVS} access to the Pencil Code
  from the download section in
  \url{http://www.nordita.dk/software/pencil-code}.
  Alternatively, just email \email{Wolfgang.Dobler@kis.uni-freiburg.de}
  or \email{Axel.Brandenburg@nordita.dk}.
  Privileged users with write access to the original repository should
  continue to use it.\footnote{%
    In that case you'd say
    \code{setenv CVSROOT :pserver:\$USER@130.225.213.198:/var/cvs/brandenb},
    assuming that \env[USER]{\$USER} is the userid registered in the original
    repository. The content of this repository is copied automatically every
    hour to the anonymous cvs repository.
    Be sure to run \code{auto-test} before you check anything back in again.
    It can be very annoying for someone else to figure out what's wrong,
    especially if you are just up to something else.}

\item Set your environment variable \env{CVSROOT}:
  \begin{small}
  \begin{alltt}
  \prompt{csh> } setenv CVSROOT :pserver:anonymous@130.225.213.198:/var/cvs/pencil-public \
  \end{alltt}
  \end{small}
  or (if you are running \name{bash} or \name{Bourne shell})
  \begin{small}
  \begin{alltt}
  \prompt{sh> } CVSROOT=:pserver:anonymous@norserv.nordita.dk:/home/cvs/public
  \prompt{sh> } export CVSROOT \
  \end{alltt}
  \end{small}

\item Log in:
  \begin{alltt}
  \prompt{unix> } cvs login
  cvs password: ........ \
\end{alltt}
(you only need to do this once; your CVS password is saved in the file
\file{.cvspass} in your home directory)

\item Go to wherever you want the code:
  \begin{alltt}
  \prompt{unix> } cd somewhere/ \
  \end{alltt}
  and check out the code:
  \begin{alltt}
  \prompt{unix> } cvs checkout pencil-code \
  \end{alltt} 
  This creates a subdirectory \file[pencil-code/]{somewhere/pencil-code}
  and populates it with the Pencil Code's subdirectories.
  If you change your mind about the location of the code, you can just
  move the whole directory,
  \begin{alltt}
  \prompt{unix> } mv somewhere/pencil-code elsewhere/ \
  \end{alltt}
  since \name{CVS} keeps all information in the local \file{CVS/}
  subdirectories below \file{pencil-code/}.
\end{enumerate}


% ---------------------------------------------------------------------- %

\subsection{Obtaining and unpacking the tarball}

\begin{enumerate}
\item Download the tarball \texttt{pencil-code_\emph{X}.tar.gz}
  (where \emph{X} is the release number) from the download section in
  \url{http://www.nordita.dk/software/pencil-code}

\item Put it into a convenient directory and unpack it:
  \begin{alltt}
  \prompt{unix> } mv pencil-code_\emph{X}.tar.gz somewhere/
  \prompt{unix> } cd somewhere
  \prompt{unix> } gunzip pencil-code_\emph{X}.tar.gz
  \prompt{unix> } tar xf pencil-code_\emph{X}.tar \
  \end{alltt}
\end{enumerate}

% ---------------------------------------------------------------------- %

\subsection{Updating via CVS}
\index{CVS}

Independent of how you installed the code in the first place (from tarball
or via \name{CVS}), you can update your version using \name{CVS}.
If you have done nontrivial alterations to your version of the code, you
ought to be careful about upgrading: although \name{CVS} is an excellent
tool for distributed programming,
conflicts are quite likely, since we are going to
touch many parts of the code while we develop it further.
Thus, despite the fact that the code is under \name{CVS}, you should probably
back up your changes before upgrading.

Here is the upgrading procedure:
\begin{enumerate}
%\item Go to the top directory of the code:
%  \begin{alltt}
%  \prompt{unix> } cd somewhere/pencil-code \
%  \end{alltt}
\item Perform a \cmd{cvs} update of the tree:
  \begin{alltt}
  \prompt{unix> } pc_cvsup \
  \end{alltt}
  (This is equivalent to typing \cmd{cvs -q update -dP} from the top directory
  of the code, except that it only performs a full update of the core 
  Pencil Code directories.  This for example avoids the problem of having
  a partially checked out \file{runs} directory under your Pencil Code
  which would otherwise be checked out in full if the simple cvs command 
  were used.)
%  (the option \option{-q} means ``somewhat quiet'', \option{-d} checks out
%  new directories, and \option{-P} removes empty directories)
\item Fix any conflicts you encounter and make sure the examples in the
  directory \file{samples/} are still working.
\end{enumerate}

% ---------------------------------------------------------------------- %

\subsection{Getting older versions}
\index{CVS}

You may find that the latest \name{CVS} version of the code produces errors.
If you have reasons to believe that this is due to changes introduced on
27 November 2002 (to give an example), you can check out the version prior to
this by:
\begin{enumerate}
\item Go to the top directory of the code:
  \begin{alltt}
  \prompt{unix> } cd somewhere/pencil-code \
  \end{alltt}
\item Run \cmd{pc_cvsup}:
  \begin{alltt}
  \prompt{unix> } pc_cvsup -D "26 Nov 2002"\
  \end{alltt}
\end{enumerate}

% ---------------------------------------------------------------------- %

\subsection{Obtaining and unpacking the tarball}

\begin{enumerate}
\item Download the tarball file, pencil-code_X.tar.gz,
  where X is the release number, after registering on
  \url{http://www.nordita.dk/software/pencil-code/}
\item Put it into a convenient directory and unpack it:
  \begin{alltt}
  \prompt{unix> } mv pencil-code_X.tar.gz somewhere/; cd somewhere
  \prompt{unix> } gunzip pencil-code_X.tar.gz
  \prompt{unix> } tar xf pencil-code_X.tar \
  \end{alltt}
\end{enumerate}


% ====================================================================== %

\section{Getting Started}
\label{S-getting-started}

To get yourself started, you should run one or several examples which are
provided in one of the \file{samples/} subdirectories.
Note that you will only be able to fully assess the numerical solutions if you
visualize them with \name{IDL}, \name{DX} or other tools (see
Sect.~\ref{S-Visualization}).

% ---------------------------------------------------------------------- %

\subsection{Setup}

\subsubsection{Environment settings}

The functionality of helper scripts and IDL routines relies on a few
environment variables being set correctly.
The simplest way to achieve this is to go to the top directory of the code
and source one of
the two scripts \file{sourceme.csh} or \file{sourceme.sh} (depending on
the type of shell you are using):
\begin{alltt}
  \prompt{csh> } cd pencil-code
  \prompt{csh> } source ./sourceme.csh
\end{alltt}
for \name{tcsh} or \name{csh} users; or
\begin{alltt}
  \prompt{sh> } cd pencil-code
  \prompt{sh> } . ./sourceme.sh
\end{alltt}
for users of \name{bash}, \name{Bourne shell}, or similar shells.
You should get output similar to
\begin{alltt}
  PENCIL_HOME = </home/dobler/f90/pencil-code>
  Adding /home/dobler/f90/pencil-code/bin to PATH
\end{alltt}
Apart from the \env{PATH} variable, the environment variable
\env{IDL_PATH} is set to something like
\texttt{./idl:../idl:+\$PENCIL_HOME/idl:./data:<IDL_DEFAULT>} .

\paragraph{Note 1}
The \code{<IDL_DEFAULT>} mechanism does not work for IDL versions 5.2 or
older. In this case, you will have to edit the path manually, or adapt
the \file{sourceme} scripts.

\paragraph{Note 2}
If you  don't want to rely on the \file{sourceme} scripts' (quite
heuristic) ability to correctly identify the code's main directory, you
can set the environment variable
\env{PENCIL_HOME} explicitly before you run the source command.

\paragraph{Note 3}
Do not just source the \file{sourceme} script from your shell startup file
(\file[.cshrc]{\~{}/.cshrc} or \file[.bashrc]{\~{}/.bashrc}, because it
outputs a few lines of diagnostics for each sub-shell, which will break
many applications.
To suppress all output, follow the instructions given in the header
documentation of \file{sourceme.csh} and \file{sourceme.sh}.

\paragraph{Note 4}
The second time you source \file{sourceme}, it will not add
anything to your \env{PATH} variable.
This is on purpose to avoid cluttering of your environment: you can
source the file as often as you like (in your shell startup script, then
manually and in addition in some script you have written), without
thinking twice.
If, however, at the first sourcing, the setting of \env{PENCIL_HOME} was
wrong, this mechanism would keep you from ever adding the right directory
to your \env{PATH}.
In this case, you need to first undefine the environment variable
\env{PENCIL_HOME}:
\begin{alltt}
  \prompt{csh> } unsetenv PENCIL_HOME
  \prompt{csh> } source ./sourceme.csh
  {\sl{}or}
  \prompt{sh> } unset PENCIL_HOME
  \prompt{sh> } . ./sourceme.sh
\end{alltt}



\subsubsection{Linking scripts and source files}

With your environment set up correctly, you can now go to the directory
you want to work in and set up subdirectories and links.
This is accomplished by the script \file{setup-src}, which is located in
\file[bin/]{\$PENCIL_HOME/bin} and is thus now in your executable path.

For concreteness, let us assume you want to use
\file[conv-slab/]{samples/conv-slab}
as your \dfn{run directory}, i.e.\ you want to run a three-layer slab model
of solar convection.
You then do the following:
\begin{alltt}
  \prompt{unix> } cd samples/conv-slab
  \prompt{unix> } setup-src
  src already exists
  2 files already exist in src
\end{alltt}
The script has linked a number of scripts from \file[bin/]{\$PENCIL_HOME/bin},
generated a directory \file[src/]{src} for the source code and linked the
Fortran source files (plus a few more files) from \file[src/]{\$PENCIL_HOME/src}
to that directory:
\begin{alltt}
  \prompt{unix> } ls -F
  CVS/           src/
  start.csh@     run.csh@  getconf.csh@ 
  start.in       run.in    print.in            
  reference.out     
\end{alltt}


\subsubsection{Adapting \file{Makefile.src}}

This step requires some input from you, but you only have to do this once
for each machine you want to run the code on.
See Sect.~\ref{adapt-mkfile} for a description of the steps you need to
take here.

\paragraph{Note:} If you are lucky and use compilers similar to the ones
we have, you may be able to skip this step; but blame yourself if things
don't compile, then.
If not, you can run \cmd{make} with explicit flags, see
Sec.~\ref{S-make-flags} and in particular Table~\ref{Tab-make-flags}.


\subsubsection{Running \cmd{make}}

Next, you run \cmd{make} in the \file[src/]{src} subdirectory of your run
directory.
Since you are using one of the predefined test problems, the settings in
\file[Makefile.local]{src/Makefile.local} and
\file[cparam.local]{src/cparam.local} are all reasonable, and you just do
\begin{alltt}
  \prompt{unix> } make \
\end{alltt}
If you have set up the compiler flags correctly, compilation should
complete successfully.


\subsubsection{Choosing a data directory}

The code will by default write data like snapshot files to the subdirectory
\file[data/]{data} of the run directory.
Since this will involve a large volume of IO-operations (at least for
large grid sizes), one will normally try to avoid writing the data via
NFS.
The recommended way to set up a \file[data/]{data} data directory is to generate
a corresponding directory on the local disc of the computer you are
running on and (soft-)link it to \file[data/]{./data}.
Even if the link is part of an NFS directory, all the IO operations will
be local.
For example, if you have a local disc \file[/scratch/]{/scratch}, you can
do the following
\begin{alltt}
  \prompt{unix> } mkdir -p /scratch/\$USER/pencil-data/samples/conv-slab
  \prompt{unix> } ln -s /scratch/\$USER/pencil-data/samples/conv-slab ./data \
\end{alltt}

Even if you don't have an NFS-mounted directory (say, on your notebook
computer), it is probably still a good idea to have code and data well
separated by a scheme like the one described above.

An alternative to symbolic links, is to provide a file called 
\file{datadir.in} in the root of the run directory.  This file
should contain one line of text specifying the absolute or relative data 
directory path to use.  This facility is useful if one wishes to switch
one run directory between different data directories.  It is suggested
that in such cases symbolic links are again made in the run directory
to the various locations, then the \file{datadir.in} need
contain only a short relative path.


\subsubsection{Running the code}

You are now ready to start the code:
\begin{alltt}
  \prompt{unix> } start.csh\small
  Linux cincinnatus 2.4.18-4GB #1 Wed Mar 27 13:57:05 UTC 2002 i686 unknown
  Non-MPI version
  datadir = data
  Fri Aug  8 21:36:43 CEST 2003
     src/start.x
  CVS: io_dist.f90        v. 1.61         (brandenb  ) 2003/08/03 09:26:55
  {\sl{}[\ldots]}  
  CVS: start.in           v. 1.4          (dobler    ) 2002/10/02 20:11:14
   nxgrid,nygrid,nzgrid=          32          32          32
   thermodynamics: assume cp=1
   
   uu: up-down
   piecewise polytropic vertical stratification (lnrho)
   init_lnrho: cs2bot,cs2top=   1.450000      0.3333330    
   e.g. for ionization runs: cs2bot,cs2top not yet set
   piecewise polytropic vertical stratification (ss)
   
   start.x has completed successfully
   
  0.070u 0.020s 0:00.14 64.2%     0+0k 0+0io 180pf+0w

  Fri Aug  8 21:36:43 CEST 2003
\end{alltt}
This runs \file[start.x]{src/start.x} to construct an initial condition based on
the parameters set in \file{start.in}.
This initial condition is stored in \file[var.dat]{data/proc0/var.dat} (and
in \file[var.dat]{data/proc1/var.dat}, etc.\ if you run the multiprocessor
version).

If you visualize the profiles using \name{IDL} (see below),
the result should bear some resemblance to Fig.~\ref{Fig-pvert1}, but with
different values in the ghost zones (the correct values are set at
run-time only) and a simpler velocity profile.

\bigskip

Now we run the code:
\begin{alltt}
  \prompt{unix> } run.csh
\end{alltt}
This runs \file[run.x]{src/run.x} and carries out \var{nt} time steps, where
\var{nt} and other run time parameters are specified in \file{run.in}.
On a decent PC (1.7\,GHz), 50 time steps take about 10 seconds.

The relevant part of the code's output looks like\\
\begin{minipage}{1\textwidth}
\bigskip
\begin{small}
\begin{alltt}
 --it----t-------dt-------urms----umax----rhom------ssm-----dtc----dtu---dtnu---dtchi-
    0   0.34  6.792E-03  0.0060  0.0452  14.4708  -0.4478  0.978  0.013  0.207  0.346
   10   0.41  6.787E-03  0.0062  0.0440  14.4707  -0.4480  0.978  0.013  0.207  0.345
   20   0.48  6.781E-03  0.0064  0.0429  14.4705  -0.4481  0.977  0.012  0.207  0.345
   30   0.54  6.777E-03  0.0067  0.0408  14.4703  -0.4482  0.977  0.012  0.207  0.345
   40   0.61  6.776E-03  0.0069  0.0381  14.4702  -0.4482  0.977  0.011  0.207  0.346
\end{alltt}
\end{small}
\bigskip
\end{minipage}
and lists
\begin{enumerate}
\item the number \var{it} of the current time step; 
\item the time, \var{t};
\item the time step, \var{dt};
\item the rms velocity, \var{urms}$\,=\sqrt{\left<\uv^2\right>}$;
\item the maximum velocity, \var{umax}$\,=\max |\uv|$;
\item the mean density, \var{rhom}$\,=\left<\varrho\right>$;
\item the mean entropy, \var{ssm}$\,=\left<s\right>/c_p$;
\item the time step in units of the acoustic Courant step,
  \var{dtc}$\,=\delta t/({\cs}_0\,\delta x_{\rm min})$;
\item the time step in units of the advective time step,
  \var{dtu}$\,=\delta t/(c_{\delta t}\,\delta x/\max|\mathbf{u}|)$;
\item the time step in units of viscous time step,
  \var{dtnu}$\,=\delta t/(c_{\delta t,{\rm v}}\,\delta x^2/\nu_{\rm max})$;
\item the time step in units of the conductive time step,
  \var{dtchi}$\,=\delta t / (c_{\delta t,{\rm v}}\,\delta x^2/\chi_{\rm max})$.
\end{enumerate}
The entries in this list can be added, removed or reformatted in the file
\file{print.in}, see Sects~\ref{diagnostic-IO} and \ref{S-print.in-params}.
The output is also saved in \file[time_series.dat]{data/time_series.dat}
and should be identical to the content of \file{reference.out}.

% ---------------------------------------------------------------------- %
\begin{figure}[hbtp]
  \centering
  \includegraphics%
    [width=1\textwidth,keepaspectratio]%
    {pvert1}
  \caption{Stratification of the three-layer convection model in
    \file[conv-slab/]{samples/conv-slab} after $50$ timesteps ($t=0.428$).
    Shown are (from left to right) density $\varrho$, vertical velocity
    $u_z$, entropy $s/c_p$ and temperature $T$ as functions of the
    vertical coordinate $z$ for about ten different vertical lines in the
    computational box.
    The dashed lines denote domain boundaries:
    $z<-0.68$ is the lower ghost zone (points have no physical significance);
    $-0.68<z<0$ is a stably stratified layer ($ds/dz>0$);
    $0<z<1$ is the unstable layer ($ds/dz<0$);
    $1<z<1.32$ is the isothermal top layer;
    $z>1.32$ is the upper ghost zone (points have no physical significance).
  }
  \label{Fig-pvert1}
\end{figure}
% ---------------------------------------------------------------------- %

If you have \name{IDL}, you can visualize the stratification with (see
Sect.~\ref{S-IDL-routines} for details)
\begin{alltt}
  \prompt{unix > } idl
  \prompt{IDL >  } .r start
  \prompt{IDL >  } .r r
  \prompt{IDL >  } .r thermo
  \prompt{IDL >  } .r pvert \
\end{alltt}
The result should look like Fig.~\ref{Fig-pvert1}.


\paragraph{Note:}
If you want to run the code with \name{MPI}, you will probably need to
adapt \file{getconf.csh}, which defines the commands and flags used to
run MPI jobs (and which is sourced by the scripts \file{start.csh} and
\file{run.csh}).
Try
\begin{alltt}
  csh -v getconf.csh
  {\sl or}
  csh -x getconf.csh
\end{alltt}
to see how \file{getconf.csh} makes its decisions. You would add a
section for the host name of your machine with the particular settings.
Since \file{getconf.csh} is linked from the central directory
\file[bin/]{pencil-code/bin}, your changes will be
useful for all your other runs too.

% ---------------------------------------------------------------------- %

\subsection{Further tests}

There is a number of other tests in the \file{samples/} directory.
You can use the script \file[auto-test]{bin/auto-test} to automatically run
these tests and have the output compared to reference results.

% ====================================================================== %

\section{Code structure}

\subsection{Directory tree}

 % ---------------------------------------------------------------------- %
\begin{figure}[hbtp]
  \centering
  \includegraphics%
    [width=0.9\textwidth,height=0.65\textheight,keepaspectratio]%
    {struct}
  \caption{The basic structure of the code}
  \label{Fig-Structure}
\end{figure}
% ---------------------------------------------------------------------- %

The overall directory structure of the code is shown in
Fig.~\ref{Fig-Structure}.
Under \file[pencil-code/]{pencil-code}, there are currently the following
files and directories:
\begin{verbatim}
  CVS/      bin/    bugs/           doc/
  dx/       idl/    samples/        src/
  README    TODO    sourceme.csh    sourceme.sh
\end{verbatim}

Almost all of the source code is contained in the directory \file{src/},
but in order to encapsulate individual applications, the code is compiled
separately for each run in a local directory \file[src/]{src} below the
individual run directory, like
e.\,g.~\file[conv-slab/src/]{samples/conv-slab/src}.

It may be a good idea to keep your own runs also under \name{CVS}, but
this would normally be a different repository. On the machine where you
are running the code, you may want to check them out into a subdirectory
of \file{pencil-code/}.
For example, we have our own runs in a repository called
\file[pencil-runs/]{pencil-runs}, so we do
\begin{alltt}
  \prompt{unix> } cd \$PENCIL_HOME
  \prompt{unix> } cvs co -d runs pencil-runs
\end{alltt}
In this case, \file[runs/]{runs} contains individual run directories,
grouped in
classes (like \file[spher/]{spher} for spherical calculations, or
\file[kinematic/]{kinematic} for kinematic dynamo simulations).
The current list of classes in our own \file[pencil-runs/]{pencil-runs}
repository is
\begin{verbatim}
  1d-tests/   disc/          kinematic/  rings/
  2d-tests/   discont/       Misc/       slab_conv/ 
  3d-tests/   discussion/    OLD/        test/
  buoy_tube/  forced/        pass_only/
  convstar/   interstellar/  radiation/
\end{verbatim}
The directory \file{forced/} contains some forced turbulence runs (both
magnetic and nonmagnetic);
\file{gravz/} contains runs with vertical gravity;
\file{rings/} contains decaying MHD problems (interlocked flux rings as
initial condition, for example);
and \file{kinematic/} contains kinematic dynamo problems where the
hydrodynamics is turned off entirely.
The file \file{samples/README} should contain an up-to-date list and
short description of the individual classes.\footnote{Our
\file[pencil-runs/]{pencil-runs} directory also contains runs that were
done some time ago. Occasionally, we try to update these, especially if we
have changed names or other input convections.}

The subdirectory \file[src/]{src} of each run directory contains a few local
configuration files (currently these are \file{Makefile.local} and
\file{cparam.local}) and possibly \file{ctimeavg.local}.
To compile the samples, links the files \file[*.f90]{.f90},
\file[*.c]{.c} and \file{Makefile.src} need to be linked from the top
file[src/]{src} directory to the local directory \file[src/]{./src}.
These links are set up by the script
\cmd{setup-src}) when used in a the root of run directory.

General-purpose visualization routines for \name{IDL} or \name{DX} are in the
directories \file[idl/]{idl} and \file[dx/]{dx}, respectively.
There are additional and more specialized \name{IDL} directories in the
different branches under \file[pencil-runs/]{pencil-runs}.

The directory \file[doc/]{doc} contains this manual;
\file[bin/]{bin} contains a number of utility scripts (mostly written in
\name{csh} and \name{Perl}), and in particular the \file{start.csh},
\file{run.csh}, and \file{getconf.csh} scripts.
The \file[CVS/]{CVS} directory is used (you guessed it) by \name{CVS}, and is
not normally directly accessed by the user;
\file[bugs/]{bugs}, finally is used by us for internal purposes.

\bigskip

The files \file{sourceme.csh} and \file{sourceme.sh} will set up some
environment variables --- in particular \env{PATH} --- and aliases/shell
functions for your convenience.
If you do not want to source one of these files, you need to make sure
your \name{IDL} path is set appropriately (provided you want to use
\name{IDL}) and you will need to address the scripts from
\file[bin/]{bin} with their explicit path name, or adjust your \env{PATH}
manually.



% ---------------------------------------------------------------------- %

\subsection{Basic concepts}

\subsubsection{Data access in pencils}
\index{Pencil design}

Unlike the CRAY computers that dominated supercomputing in the 80s and
early 90s, all modern computers have a cache that constitutes a significant
bottleneck for many codes.
This is the case if large three-dimensional arrays are constantly used
within each time step, which has the obvious advantage of working on
long arrays and allows vectorization of elementary machine operations.
This approach also implies
conceptual simplicity of the code and allows extensive use of the
intuitive F90 array syntax.
However, a more  cache-efficient way of coding is to calculate an entire
time step
(or substep of a multi-stage time-stepping scheme) only
along a one-dimensional pencil of data within the numerical grid.
This technique is more efficient for modern RISC processors:
on Linux PCs and SGI workstations, for example, we have found a speed-up
by about 60\%{} in some cases.
An additional advantage is a drastic reduction in temporary storage for
auxiliary variables within each time step.

\subsubsection{Modularity}
\label{S-modularity}
\index{Modules}

Each run directory has a file \file[Makefile.local]{src/Makefile.local} in
which you
choose certain \name{modules}\footnote{%
  We stress once more that we are not talking about F90 modules here,
  although there is some connection, as most of our modules define F90
  modules:
  For example each of the modules \name{gravity_simple}, \name{grav_r} and
  \name{nogravity} defines a Fortran module \name{Gravity}.
}, which tell the code whether or not entropy, magnetic fields,
hydrodynamics, forcing, etc.\ should be invoked.
For example, the settings for forced turbulent MHD simulations are
\begin{verbatim}
  HYDRO     =   hydro
  DENSITY   =   density
  ENTROPY   = noentropy
  MAGNETIC  =   magnetic
  GRAVITY   = nogravity
  FORCING   =   forcing
    
  MPICOMM   = nompicomm
  GLOBAL    = noglobal
  IO        =   io_dist
  FOURIER       = nofourier
\end{verbatim}
This file will be processed by \name{make} and the settings are thus
assignments of \name{make} variables.
Apart from the physics modules (equation of motion: yes, density
[pressure]: yes, entropy equation: no, magnetic fields: yes, gravity: no,
forcing: yes), a few technical modules can also be used or deactivated; in
the example above, these are \name{MPI} (switched off), additional global
variables (none), input/output (distributed), and \name{FFT} (not used).
The table in Sect.~\ref{Tab-modules} in the Appendix lists all currently available modules.

Note that most of these \name{make} variables \emph{must} be set, but they
will normally obtain reasonable default values in \file{Makefile} (so you
only need to set the non-standard ones in \file{Makefile.local}).
It is by using this switching mechanism through \cmd{make} that we achieve
high flexibility without resorting to excessive amounts of cryptic
preprocessor directives or other switches within the code.
 
Many possible combinations of modules have already been tested
and examples are part of the distribution, but you may be interested in a
combination which was never tried before and which may not work yet, since the
modules are not fully orthogonal.
In such cases, we depend on user feedback for fixing problems
and documenting the changes for others.


% ---------------------------------------------------------------------- %

\subsection{Files in the run directories}

\subsubsection{\file{start.in}, \file{run.in}, \file{print.in}}
These files specify the startup and runtime parameters (see
Sects.~\ref{S-start-params} and \ref{S-all-run-params}), and the list of
diagnostic variables to print (see \ref{diagnostic-IO}).
They specify the setup of a given simulation and are kept under
\name{CVS}\index{CVS} in
the individual \file[samples/]{samples} directories.

\subsubsection{\file{datadir.in}}
\label{Ss-datadir-in}
If this file exists, it must contain the name of an existing directory,
which will be used as \dfn{data directory},
i.\,e.~the directory where all results are written.
If \file{datadir.in} does not exist, the data directory is \file{data/}.

\subsubsection{\file{reference.out}}
If present, \file{reference.out} contains the output you should obtain in
the given run directory, provided you have not changed any parameters.
To see whether the results of your run are OK, compare \file{time_series.dat} to
\file{reference.out}:
\begin{alltt}
  \prompt{unix> } diff data/time_series.dat reference.out
\end{alltt}

\subsubsection{\file{start.csh}, \file{run.csh}, \file{getconf.csh}}
\label{start-run-getconf}
These are links to \file[bin/]{\$PENCIL_HOME/bin}.
You will be constantly using the scripts \file{start.csh} and
\file{run.csh} to initialize the code.
Things that are needed by both (like the name of the \cmd{mpirun}
executable, \name{MPI} options, or the number of processors) are located in
\file{getconf.csh}, which is never directly invoked.

\subsubsection{\file{src/}}
The \file[src/]{src} directory contains two local files,
\file[Makefile.local]{src/Makefile.local} and
\file[cparam.local]{src/cparam.local}, which allow the user to choose
individual modules (see \ref{S-modularity}) and to set parameters like the
grid size and the number of processors for each direction.
These two files are part of the setup of a given simulation and are kept
under \name{CVS} in the individual \file[samples/]{samples} directories.

The file \file[cparam.inc]{src/cparam.inc} is automatically generated by
the script \file{mkcparam} and contains the number of fundamental
variables for a given setup.

All other files in \file{src/} are either links to source files (and
\file{Makefile.src}) in the \file[src/]{\$PENCIL_HOME/src} directory,
or object and module files generated by the compiler.

\subsubsection{\file{data/}}
This directory (the name of which will actually be overwritten by the first
line of \file{datadir.in}, if that file is present; see
\S\ref{Ss-datadir-in})
contains the output from the code:

\paragraph{\file[dim.dat]{data/dim.dat}}
The global array dimensions.

\paragraph{\file[legend.dat]{data/legend.dat}}
The header line specifying the names of the diagnostic variables in
\file{time_series.dat}.

\paragraph{\file[time_series.dat]{data/time_series.dat}}
Time series of diagnostic variables (also printed to stdout).
You can use this file directly for plotting with \name{Gnuplot},
\name{IDL}, \name{Xmgrace} or similar tools (see also
\S\ref{S-Visualization}).

\paragraph{\file[tsnap.dat]{data/tsnap.dat}, \file[tvid.dat]{data/tvid.dat}}
Time when the next snapshot \file{VAR$N$} or animation slice should be
taken.

\paragraph{\file[params.log]{data/params.log}}
Keeps a log of all your parameters: \file{start.x} writes the startup
parameters to this file, \file{run.x} appends the runtime parameters and
appends them anew, each time you use the \file{RELOAD} mechanism (see
\S\ref{S-RELOAD}).

\paragraph{\file[param.nml]{data/param.nml}}
Complete set of startup parameters, printed as Fortran namelist.
This file is read in by \file{run.x} (this is how values of startup
parameters are propagated to \file{run.x}) and by \name{IDL} (if you use
it).

\paragraph{\file[param2.nml]{data/param2.nml}}
Complete set of runtime parameters, printed as Fortran namelist.
This file is read by \name{IDL} (if you use it).

\paragraph{\file[index.pro]{data/index.pro}}
Can be used as include file in \name{IDL} and contains the column in which
certain variables appear in the diagnostics file (\file{time_series.dat}).
It also contains the positions of variables in the \file{VAR$N$} files.
These positions depend on whether \name{entropy} or \name{noentropy}, etc,
are invoked.
This is a temporary solution and the file may disappear in future
releases.

\paragraph{\file[interstellar.dat]{data/interstellar.dat}} 
Unformatted file containing the time at which the next supernova event will 
occur, under certain supernova schemes.  
(Only needed by the \name{interstellar} module.)

\paragraph{\file[proc$N$]{data/proc0}, \file[proc1]{data/proc1}, \ldots}
These are the directories containing data from the individual processors.
So after running an \name{MPI} job on two processors, you will have the
two directories \file[proc0]{data/proc0} and \file[proc1]{data/proc1}.
Each of the directories can contain the following files:
\begin{description}
\item[\file{var.dat}] binary file containing the latest snapshot;
\item[\file{VAR$N$}] binary file containing individual snapshot number $N$;
\item[\file{dim.dat}] ASCII file containing the array dimensions as seen
  by the given processor;
\item[\file{time.dat}] ASCII file containing the time corresponding to
  \file{var.dat} (not actually \emph{used} by the code, unless you use the
  \name{io_mpiodist.f90} module);
\item[\file{grid.dat}] binary file containing the part of the grid seen by
  the given processor;
\item[\file{seed.dat}] the random seed for the next time step (saved for
  reasons of reproducibility).
  For multi-processor runs with velocity forcing, the files
  \file[seed.dat]{proc$N$/seed.dat} must all contain the same numbers,
  because globally coherent waves of given wavenumber are used;
\item[{\file[X.xy]{$X$.xy}, \file[X.xz]{$X$.xz}, \file[X.yz]{$X$.yz}}]
  two-dimensional sections of variable $X$, where $X$ stands for the
  corresponding variable. The current list includes
\begin{verbatim}
  bx.xy  bx.xz  by.xy  by.xz  bz.xy  bz.xz  divu.xy  lnrho.xz
  ss.xz  ux.xy  ux.xz  uz.xy  uz.xz
\end{verbatim}
  Each processor writes its own slice, so these need to
  be reassembled if one wants to plot a full slice.
\end{description}


% ====================================================================== %

\section{Using the Code}
\label{Input-params}

\subsection{Adapting \file{Makefile.src}}
\label{adapt-mkfile}

By default, one \file{Makefile} will only work on a given class of
computers with identical (or at least similar) compilers.
One aim when building the Pencil Code was however to allow running the
code on different machines without needing to modify any file.
This, together with \name{CVS} allows you for example to very quickly
compare results for identical runs on different machines.

Our mechanism for picking the right set of compiler flags is based on the
\name{Perl} script \file{adapt-mkfile} in \file[bin/]{\$PENCIL_HOME/bin}, which
processes the file \file{Makefile.src} to a (localized) \file{Makefile}.
You can view the full documentation for \cmd{adapt-mkfile} with the
command
\begin{alltt}
  \prompt{unix > } perldoc adapt-mkfile
\end{alltt}
provided \file[bin/]{\$PENCIL_HOME/bin} is in your path.

To give you an idea of how to add your own machines, let us assume you have
several Linux boxes running a compiler \cmd{f95} that needs the options
\option{-O4 -u}, while one of them, \name{Janus}, runs a compiler \cmd{f90}
which needs the flags \option{-O3} and requires the additional
options \option[-lmpi]{-lmpi -llam} for \name{MPI}.

The \file{Makefile.src} you need will have the following section:
\begin{alltt}
  ### Begin machine dependent
  
  ## IRIX64:
  {\sl{}[\ldots]   (leave as it is in the original Makefile)}
  ## OSF1:
  {\sl{}[\ldots]   (leave as it is in the original Makefile)}
  
  ## Linux:
  {\sl{}[\ldots]   (leave everything from original Makefile and add:)}
  #FC=f95
  #FFLAGS=-O4 -u
  #FC=f90             #(Janus)
  #FFLAGS=-O3         #(Janus)
  #LDMPI=-lmpi -llam  #(Janus)
  
  ## SunOS:
  {\sl{}[\ldots]   (leave as it is in the original Makefile)}
  ## UNICOS/mk:
  {\sl{}[\ldots]   (leave as it is in the original Makefile)}
  ## HI-UX/MPP:
  {\sl{}[\ldots]   (leave as it is in the original Makefile)}
  ## AIX:
  {\sl{}[\ldots]   (leave as it is in the original Makefile)}
  
  ### End machine dependent
\end{alltt}


\paragraph{Note 1}
If you have trouble with \cmd{make} or \file{adapt-mkfile}, look at the
\file{Makefile} written by \file{adapt-mkfile} and check that it has the
entries for your machine activated.
In the example above, \code{\#(Janus)} is {\it not} a comment, but marks
this line to be activated (uncommented) by \cmd{adapt-mkfile} if your
hostname (\cmd[uname]{`uname -n`}) matches `Janus' or `janus'
(capitalization is irrelevant).
You can combine machine names with a vertical bar:
a line containing \code{\#(onsager|Janus)} will be activated on both,
\name{Janus} and \name{Onsager}.

\paragraph{Note 2}
\label{S-make-flags}
If you want to experiment with compiler flags, or if you
want to get things running without setting up the machine-dependent
section of the \file{Makefile}, you can set \name{make} variables at the
command line in the usual manner:
\begin{alltt}
  \prompt{src> } make FC=f90 FFLAGS='-fast -u'
\end{alltt}
will use the compiler \cmd{f90} and the flags
\option[-fast]{-fast -u} for both compilation and linking.
Table~\ref{Tab-make-flags} summarizes flags we use for common compilers.

\begin{table}
  \centering
  \caption{Compiler flags for common compilers.
    Note that some combinations of OS and compiler require much more
    elaborate settings; also, if you use MPI, you will have to set
    \texttt{LDMPI}.}
  \label{Tab-make-flags}
  \begin{tabular}{llll}
  \toprule
    \emph{Compiler}       & \texttt{FC}    & \texttt{FFLAGS}                     & \texttt{CFLAGS} \\
  \midrule
    \emph{Linux defaults} & \texttt{f95}   & \texttt{-O3}                        & \texttt{-O3 -DFUNDERSC=1}\\
    Intel ifort           & \texttt{ifort} & \texttt{-O3}                        & \texttt{-O3 -DFUNDERSC=1}\\
    Absoft                & \texttt{f90}   & \texttt{-O3 -YEXT_NAMES=LCS}        & \texttt{-O3 -DFUNDERSC=0}\\
    Portland group        & \texttt{pgf90} & \texttt{-O4}                        & \texttt{-O3 -DFUNDERSC=1}\\
    Gnu F95               & \texttt{g95}   & \texttt{-O3 -fno-second-underscore} & \texttt{-O3 -DFUNDERSC=1}\\
                          & \texttt{g95}   & \texttt{-O3}                        & \texttt{-O3 -DFUNDERSC=2}\\
  \midrule
    IRIX Mips             & \texttt{f90}   & \texttt{-64 -O3 -mips4}             & \texttt{-O3 -64 -DFUNDERSC=1}\\
  \midrule
    Compaq                & \texttt{f90}   & \texttt{-fast -O5 }                 & \texttt{-O3 -DFUNDERSC=1}\\
  \midrule
    IBM                   & \texttt{xlf90} & \texttt{-qsuffix=f=f90 -O5 -w}      & \texttt{-O3 -DFUNDERSC=0}\\
  \bottomrule
  \end{tabular}
\end{table}


% ---------------------------------------------------------------------- %
\subsection{Changing the resolution}

It is advisable to produce a new run directory each time you run a new case.
(This does not include restarts from an old run, of course.)
If you have a $32^3$ run in some directory \file{runA_32a}, then go to
its parent directory, i.e.\
\begin{alltt}
  \prompt{runA_32a> } cd ..
  \prompt{forced> } pc_newrun runA_32a runA_64a
  \prompt{forced> } cd runA_64a/src
  \prompt{forced> } vi cparam.local
\end{alltt}
and edit the \file{cparam.local} for the new resolution.

% ---------------------------------------------------------------------- %
\subsection{Using a non-equidistant grid}
\index{mesh, nonuniform}\index{grid, nonuniform}\index{nonuniform grid}

We introduce a non-equidistant grid $z_i$
(by now, this is also implemented for the other directions)
as a function $=z(\zeta)$ of an equidistant grid
$\zeta_i$ with grid spacing $\Delta\zeta = 1$.

The way the parameters are handled, the box size and position are
\emph{not} changed when you switch to a non-equidistant grid, i.e.~they are
still determined by \var{xyz0} and \var{Lxyz}.

The first and second derivatives can be calculated by
\begin{equation} \label{Eq-nonequi-fprime}
  \frac{df}{dz} = \frac{df}{d\zeta}\frac{d\zeta}{dz}
                = \frac{1}{z'}f'(\zeta) \; ,
  \qquad
  \frac{d^2f}{dz^2} =   \frac{1}{z'^2}f''(\zeta)
                      - \frac{z''}{z'^3}f'(\zeta) \; ,
\end{equation}
which can be written somewhat more compactly using the inverse function
$\zeta(z)$:
\begin{equation} \label{Eq-nonequi-fprime-zeta-z}
  \frac{df}{dz}     =   \zeta'(z)\, f'(\zeta) \; ,
  \qquad
  \frac{d^2f}{dz^2} =   \zeta'^2(z)\,f''(\zeta)
                        + \zeta''(z) f'(\zeta) \; .
\end{equation}
Internally, the code uses the quantities
$\var{dz_1} \equiv 1/z' = \zeta'(z)$ and
$\var{dz_tilde} \equiv -z''/z'^2 = \zeta''/\zeta'$, and stores them in
\file[grid.dat]{data/proc$N$/grid.dat}.

The parameters \var{lequidist} (a 3-element logical array),
\var{grid_func}, \var{coef_grid} (a $\ge 2$-element real array)
are used to choose a non-equidistant grid and define the function $z(\zeta)$.
So far, one can choose between \code{grid_function='sinh'},
\code{grid_function='linear'} (which produces an equidistant grid for
testing purposes), and \code{grid_function='step-linear'}.

\subparagraph{The \code{sinh} profile:}
For \code{grid_function='sinh'}, the function $z(\zeta)$ is given by
\begin{equation}
  z(\zeta)
  = z_0
    + L_z \frac{\sinh a(\zeta  {-}\zeta_*) + \sinh a(\zeta_*{-}\zeta_1)}
               {\sinh a(\zeta_2{-}\zeta_*) + \sinh a(\zeta_*{-}\zeta_1)} \; ,
\end{equation}
where $z_0$ and $z_0+L_z$ are the lowest and uppermost levels, 
$\zeta_1$, $\zeta_2$ are the $\zeta$ values representing those levels
(normally $\zeta_1 = 0, \zeta_2 = N_z-1$ for a grid of $N_z$ vertical
layers [excluding ghost layers]), and $\zeta_*$ is the
$\zeta$ value of the inflection point of the $\sinh$ function.
The $z$ coordinate and $\zeta$ value of the inflection point are related
via
\begin{equation}
  z_*
  = z_0
    + L_z \frac{\sinh a(\zeta_*{-}\zeta_1)}
               {\sinh a(\zeta_2{-}\zeta_*) + \sinh a(\zeta_*{-}\zeta_1)} \; ,
\end{equation}
which can be inverted (``after some algebra'') to
\begin{equation}
  \zeta_*
  = \frac{\zeta_1{+}\zeta_2}{2}
    + \frac{1}{a}
      \artanh \left[  \left(2\frac{z_*{-}z_0}{L_z}-1\right)\;
                      \tanh a \frac{\zeta_2{-}\zeta_1}{2}
              \right] \; .
\end{equation}

\subparagraph{General profile:}
For a general monotonic function $\psi()$ instead of $\sinh$ we get,
\begin{equation}
  z(\zeta)
  = z_0
    + L_z \frac{\psi[a(\zeta  {-}\zeta_*)] + \psi[a(\zeta_*{-}\zeta_1)]}
               {\psi[a(\zeta_2{-}\zeta_*)] + \psi[a(\zeta_*{-}\zeta_1)]} \; ,
\end{equation}
and the reference point $\zeta_*$ is found by inverting
\begin{equation}
  z_*
  = z_0
    + L_z \frac{\psi[a(\zeta_*{-}\zeta_1)]}
               {\psi[a(\zeta_2{-}\zeta_*)] + \psi[a(\zeta_*{-}\zeta_1)]} \; ,
\end{equation}
numerically.

\subparagraph{Example:}
To apply the \code{sinh} profile, you can set the following in
\file{start.in} (this example is from
\file[sound-spherical-noequi]{samples/sound-spherical-noequi}):
\begin{Verbatim}
  &init_pars
    [...]
    xyz0  = -2., -2., -2.       ! first corner of box
    Lxyz  =  4.,  4.,  4.       ! box size
    lperi =  F ,  F ,  F        ! periodic direction?
    lequidist = F, F, T         ! non-equidistant grid in z
    xyz_star   = , , -2.        ! position of inflection point
    grid_func  = , , 'sinh'     ! sinh function: linear for small, but
                                ! exponential for large z
    coeff_grid = , , 0.5
  /
\end{Verbatim}
The parameter array \var{coeff_grid} represents $z_*$ and $a$; the bottom
height $z_0$ and the total box height $L_z$ are taken from \var{xyz0} and
\var{Lxyz} as in the equidistant case.
The inflection point of the $\sinh$ profile (the part where it is linear)
is not in the middle of the box, because we have set
\var[xyz_star]{xyz_star(3)} (i.\,e.~$z_*$) to $-2$.


% ---------------------------------------------------------------------- %

\subsection{Diagnostic output}
\label{diagnostic-IO}

Every \var{it1} time steps (\var{it1} is a runtime parameter, see
Sect.~\ref{S-all-run-params}), the code writes monitoring output to
\name{stdout}
and, parallel to this, to the file \file[time_series.dat]{data/time_series.dat}.
The variables that appear in this listing and the output format are
defined in the file \file{print.in} and can be changed without touching
the code (even while the code is running).
A simple example of \file{print.in} may look like this:
\begin{verbatim}
  t(F10.3)
  urms(E13.4)
  rhom(F10.5)
  oum
\end{verbatim}
which means that the output table will contain time \var{t} in the first
column formatted as \verb|(F10.3)|, followed by the mean squared velocity,
\var{urms} (i.e.~$\left<\uv^2\right>^{1/2}$) in the second column with format
\verb|(E13.4)|, the average density \var{rhom} ($\left<\varrho\right>$,
which allows to monitor mass conservation) formatted \verb|(F10.5)| and
the kinetic helicity \var{oum}
(that is $\left<\vekt{\omega}\cdot\uv\right>$) in the last column with the
default format \verb|(E10.2)|.\footnote{
  The format specifiers are like in Fortran, apart from the fact that the
  \texttt{E} format will use standard scientific format, corresponding to
  the Fortran \texttt{1pE} syntax.
  Seasoned Fortran IV programmers may use formats like \texttt{(0pE13.4)}
  to enjoy nostalgic feelings, or \texttt{(1pF10.5)} if they depend on
  getting wrong numbers.
}
The corresponding diagnostic output will look like
\begin{verbatim}
 ----t---------urms--------rhom------oum----
     0.000   4.9643E-03  14.42457 -8.62E-06
     0.032   3.9423E-03  14.42446 -5.25E-06
     0.063   6.8399E-03  14.42449 -3.50E-06
     0.095   1.1437E-02  14.42455 -2.58E-06
     0.126   1.6980E-02  14.42457 -1.93E-06
\end{verbatim}

In the file \file{xyaver.in}, $z$-dependent (horizontal) averages are listed.
They are written to the file \file[xyaverages.dat]{data/xyaverages.dat}.
We can also output in \file{print.in} the associated mean square value
of the horizontal field, but this requires that in \file{xyaver.in} the
quantities \code{bxmz} and \code{bymz} are set.
In \code{idl} such $xy$-averages can be read using the procedure
\file{pc_read_xyaver}.

% ---------------------------------------------------------------------- %

\subsection{Data files}
\index{datafiles}

\subsubsection{Snapshot files}
\label{snapshots}

Snapshot files contain the values of all evolving variables and are
sufficient to restart a run.
In the case of an MHD simulation with entropy equation, for example, the
snapshot files will contain the values of velocity, logarithmic density,
entropy and the magnetic vector potential.

There are two kinds of snapshot files: the current snapshot and permanent
snapshots, both of which reside in the directory
\file[data/proc$N$/]{data/proc$N$/}.
The parameter \var{isav} determines the frequency at which the
\emph{current snapshot} \file[var.dat]{data/proc$N$/var.dat} is written.
If you keep this frequency too high, the code will spend a lot of time on
I/O, in particular for large jobs; too low a frequency makes it
difficult to follow the evolution interactively during test runs.

The \emph{permanent snapshots} \file[VAR$N$]{data/proc*/VAR$N$} are
written every \var{dsnap} time units.
These files are numbered consecutively from $N=1$ upward and for long
runs they can occupy quite some disk space.
On the other hand, if after a run you realize that some additional
quantity $q$ would have been important to print out, these files are the
only way to reconstruct the time evolution of $q$ without re-running the
code.


\paragraph{File structure}
Snapshot files consist of the following
\name[Fortran record]{Fortran records}\footnote{
  \label{Footnote-Fortran-record}%
  A \name{Fortran record} is marked by the 4-byte integer byte count of
  the data in the record at the beginning and the end, i.e.~has the form
  $\left< N_{\rm bytes}, \mathtt{raw\_data}, N_{\rm bytes} \right>$
}:
\begin{enumerate}
\item variable vector \verb|f| [$\var{mx}{\times}\var{my}{\times}\var{mz}{\times}\var{nvar}$]
\item time $t$ [1],
  coordinate vectors $x$ [\var{mx}], $y$ [\var{my}], $z$ [\var{mz}],
  grid spacings $\delta x$ [1], $\delta y$ [1], $\delta z$ [1],
  shearing-box shift $\Delta y$ [1]
\end{enumerate}
All numbers (apart from the record markers) are single precision (4-byte)
floating point numbers, unless you use double precision (see
\S\ref{double-precision}, in which case all numbers are 8-byte floating
point numbers, while the record markers remain 4-byte integers.

The script \cmd{tsnap} allows you to determine the time $t$ of a snapshot
file:
\begin{alltt}
  \prompt{unix> } tsnap data/proc0/var.dat
  data/proc0/var.dat:        t = 8.32456
  \prompt{unix> } tsnap data/proc0/VAR2
  data/proc0/VAR2:           t = 2.00603     
\end{alltt}

% ---------------------------------------------------------------------- %

\subsection{Video files and slices}
\label{S-slices}

We use the terms \name{video files} and \name{slice files} interchangeably.
These files contain a time series of values of one variable in a given
plane.
The output frequency of these video snapshots is set by the parameter
\var{dvid} (in code time units).

When output to video files is activated (by some settings in \file{run.in}
and the existence of \file{video.in}, slices are written for four planes:
\begin{enumerate}
\item $x$-$z$ plane ($y$ index \var{iy}; file suffix \code{.xz}) 
\item $y$-$z$ plane ($y$ index \var{ix}; suffix \code{.yz})
\item $x$-$y$ plane ($y$ index \var{iz}; suffix \code{.xy})
\item another slice parallel to the $x$-$y$ plane
  ($y$ index \var{iz2}; suffix \code{.Xy})
\end{enumerate}
You can specify the position of the slice planes by setting the parameters
\var{ix}, \var{iy}, \var{iz} and \var{iz2} in the namelist \name{run_pars} 
in \file{run.in}.
Alternatively, you can set the input parameter \var{slice_position} to one
of \code{'p'} (periphery of box) or \code{'m'} (middle of box).

In the file \file{video.in} of your run directory, you can choose for
which variables you want to get video files;
valid choices are listed in \S~\ref{S-video.in-params}.

The \name{slice files} are written in each processor directory
\file{data/proc*/} and have a file name indicating the individual variable
(e.\,g.~\file{slice_ux.yz} for a slice of $u_x$ in the $y$-$z$ plane).
Before visualizing slices one normally wants to combine the sub-slices
written by each processor into one global slice (for each plane and
variable).
This is done by running \file{src/read_videofiles.x}, which will prompt
for the variable name, read the individual sub-slices and write global
slices to \file{data/}
Once all global slices have been assembled you may want to remove the
local slices \file[]{data/proc*/slice*}.

For visualization of slices, you can use the \name{IDL} routine
\file{rvid_box}, which allows the flag \option{/png} for writing
\name{PNG} images that can then be combined into an \name{MPEG} movie
using \name{mpeg_encode}.
Based on \file{rvid_box}, you can write your own video routines in
\name{IDL}.


\paragraph{An example}
Suppose you have set up a run using \name{entropy.f90} and
\name{magnetic.f90} (most probably together with \name{hydro.f90} and
other modules).
In order to animate slices of entropy $s$ and the $z$-component $B_z$ of
the magnetic field, in planes passing through the center of the box, do the
following:
\begin{enumerate}

\item Write the following lines to \file{video.in} in your run directory:
  \begin{Verbatim}
  ss
  bb
  \end{Verbatim}

\item Edit \file{run.in} and in the namelist \name{run_pars} set
  \var{dvid} and \var{slice_position} to
  reasonable values, say
  \begin{Verbatim}
  dvid=0.05
  slice_position='m'
  \end{Verbatim}

\item Run the Pencil Code:
  \begin{alltt}
  \prompt{unix> } start.csh
  \prompt{unix> } run.csh
  \end{alltt}

\item Run \file{src/read_videofiles.x} to assemble global slice files from
  those scattered across \file{data/proc*/}:
  \begin{alltt}
  \prompt{unix> } src/read_videofiles.x
    enter name of variable (lnrho, ux, ..., bz):  \emph{ss}
  \prompt{unix> } src/read_videofiles.x
    enter name of variable (lnrho, ux, ..., bz):  \emph{bz} \
  \end{alltt}

\item Start \name{IDL} and run \file{rvid_box}:
  \begin{alltt}
  \prompt{unix> } idl
  \prompt{IDL> } rvid_box,'bz'
  \prompt{IDL> } rvid_box,'ss',min=-0.3,max=2. \
  \end{alltt}
  etc.

\end{enumerate}


\paragraph{File structure}
\name[slice files]{Slice files} consist of one Fortran record (see footnote
on page \pageref{Footnote-Fortran-record}) for each slice, which contains
the data of the variable (without ghost zones), the time $t$ of the
snapshot and the position of the sliced variable (e.\,g.~the $x$ position
for a $y$-$z$ slice):
\begin{enumerate}
\item data$_1$ [$\var{nx}{\times}\var{ny}{\times}\var{nz}$],
  time $t_1$ [1],
  position$_1$ [1]
\item data$_2$ [$\var{nx}{\times}\var{ny}{\times}\var{nz}$],
  time $t_2$ [1],
  position$_2$ [1]
\item data$_3$ [$\var{nx}{\times}\var{ny}{\times}\var{nz}$],
  time $t_3$ [1],
  position$_3$ [1]
\item[etc.]
\end{enumerate}


% ---------------------------------------------------------------------- %

\subsection{Averages}
\label{S-averages}
\index{Averages}

\subsubsection{Horizontal averages}
\label{S-hor-averages}
See Secs.~\ref{diagnostic-IO}, \ref{S-new-hor-averages} and
\ref{S-other-averages} for how to use horizontal averages [we should
reorganize this material].

\subsubsection{Azimuthal averages}
\label{S-phi-averages}
\index{Azimuthal averages}
\index{Toroidal averages}

Azimuthal averages are controlled by the file \file{phiaver.in}, which
currently supports the quantities listed in Sect.~\ref{S-phiaver.in-params}.
For example, if \file{phiaver.in} contains the single line
\begin{Verbatim}
  b2mphi
\end{Verbatim}
then you will get azimuthal averages of the squared magnetic field
$\Bv^2$.

Azimuthal averages are written every \var{d2davg} time units to the files
\file[PHIAVG$N$]{data/averages/PHIAVG$N$}.
The file format of azimuthal-average files consists of the following
\name[Fortran record]{Fortran records}:
\begin{enumerate}
\item number of radial points $N_{r,\rm \varphi-avg}$ [1],
      number of vertical points $N_{z,\rm \varphi-avg}$ [1],
      number of variables $N_{\rm var,\varphi-avg}$[1],
      number of processors in $z$ direction [1]
\item time $t$ [1],
      positions of cylindrical radius $r_{\rm cyl}$ [$N_{r,\rm \varphi-avg}$]
      and $z$ [$N_{z,\rm \varphi-avg}$] for the grid,
      radial spacing $\delta r_{\rm cyl}$ [1],
      vertical spacing $\delta z$ [1]
\item averaged data [$N_{r,\rm \varphi-avg} {\times} N_{z,\rm \varphi-avg}$]
\item label length [1], labels of averaged variables [$N_{\rm var,\varphi-avg}$]
\end{enumerate}
All numbers are 4-byte numbers (floating-point numbers or integers),
unless you use double precision (see \S\ref{double-precision}).

To read and visualize azimuthal averages in \name{IDL}, use
\file[read_phiavg.pro]{\$PENCIL_HOME/idl/read_phiavg.pro}
\begin{alltt}
  \prompt{IDL> } avg = read_phiavg('data/averages/PHIAVG1')
  \prompt{IDL> } contour, avg.b2mphi, avg.rcyl, avg.z, TITLE='!17B!U2!N!X'
\end{alltt}
or have a look at \file[phiavg.pro]{\$PENCIL_HOME/idl/phiavg.pro} for a
more sophisticated example.


\subsubsection{Time averages}
\label{S-time-averages}
\index{Averages}
\index{Time averages}

Time averages need to be prepared in the file
\file[ctimeavg.local]{src/ctimeavg.local}, since they use extra memory.
They are controlled by the averaging time $\tau_{\rm avg}$ (set
by the parameter \var{tavg} in \file{run.in}), and by the indices
\var{idx_tavg} of variables to average.

Currently, averaging is implemented as exponential (memory-less)
average,\footnote{
  At some point we may also implement the more straight-forward average
  \begin{equation}
    \left<f\right>_{t+\delta t}
    = \left<f\right>_t
      + \frac{\delta t}{t{-}t_0{+}\delta t}
        [f(t{+}\delta t)-\left<f\right>_t] \; ,
  \end{equation}
  which is equivalent to
  \begin{equation}
    \left<f\right>_t
    = \frac{1}{t-t_0}
      \int\limits_{t_0}^t f(t') \, dt' \; ,
  \end{equation}
  but we do not expect large differences.
}
\begin{equation} \label{Eq-timeavg-exp-alg}
  \left<f\right>_{t+\delta t}
  = \left<f\right>_t
    + \frac{\delta t}{\tau_{\rm avg}} [f(t{+}\delta t)-\left<f\right>_t] \; ,
\end{equation}
which is equivalent to
\begin{equation} \label{Eq-timeavg-exp-int}
  \left<f\right>_t
  = \int\limits_{t_0}^t e^{-(t-t')/\tau_{\rm avg}}\, f(t') \, dt' \; .
\end{equation}
Here $t_0$ is the time of the snapshot the calculation started with,
i.e.~the snapshot read by the last \cmd{run.x} command.
Note that the implementation (\ref{Eq-timeavg-exp-alg}) will approximate
Eq.~(\ref{Eq-timeavg-exp-int}) only to first-order accuracy in $\delta t$.
In practice, however, $\delta t$ is small enough to make this accuracy
suffice.

In \file[ctimeavg.local]{src/ctimeavg.local}, you need to set the number
of slots used for time averages.
Each of these slots uses
$\mathtt{mx}\times\mathtt{my}\times\mathtt{mz}$ floating-point numbers,
i.e.~half as much memory as each fundamental variable.

For example, if you want to get time averages of all variables, set
\begin{Verbatim}
  integer, parameter :: mtavg=mvar
\end{Verbatim}
in \file[ctimeavg.local]{src/ctimeavg.local}, and don't set \var{idx_tavg}
in \file{run.in}.

If you are only interested in averages of variables $1$--$3$ and $6$--$8$
(say, the velocity vector and the magnetic vector potential in a run with
\file{hydro.f90}, \file{density.f90}, \file{entropy.f90} and
\file{magnetic.f90}), then set
\begin{Verbatim}
  integer, parameter :: mtavg=6
\end{Verbatim}
in \file[ctimeavg.local]{src/ctimeavg.local}, and set
\begin{Verbatim}
  idx_tavg = 1,2,3,6,7,8      ! time-average velocity and vector potential
\end{Verbatim}
in \file{run.in}.

\bigskip

Permanent snapshots of time averages are written every \var{tavg} time
units to the files \file[TAVG$N$]{data/proc*/TAV$N$}.
The current time averages are saved periodically in
\file[timeavg.dat]{data/proc*/timeavg.dat} whenever
\file[var.dat]{data/proc*/var.dat} is written.
The file format for time averages is equivalent to that of the snapshots;
see \S~\ref{snapshots} above.


% ---------------------------------------------------------------------- %

\subsection{Helper scripts}
\index{Scripts}

The \file[bin/]{bin} directory contains a collection of utility scripts,
some of which are discussed elsewhere,
Here is a list of the more important ones.

\begin{description}
\item[adapt-mkfile] Activate the settings in a \file{Makefile} that apply
  to the given computer, see \S\ref{adapt-mkfile}.
\item[auto-test] Verify that the code compiles and runs in a set of run
  directories and compare the results to the reference output.
  These tests are carried out routinely to ensure that the \name{CVS}
  version of the code is in a usable state.
\item[cleanf95] Can be use to clean up the output from the Intel x86 Fortran
  95 compiler (ifc).
\item[copy-proc-to-proc] Used for restarting in a different directory.
  Example \cmd{copy-proc-to-proc seed.dat ../hydro256e}.
\item[copy-snapshots] Copy snapshots from a processor-local directory to
  the global directory.
  To be started in the background before \file{run.x} is invoked.
  Used by \file{start.csh} and \file{run.csh} on network connected processors.
\item[copy-var-to-var] Copies all \file{var.dat} files from current directory
  to file{var.dat} in another run directory.
  Used for restarting in a different directory.
\item[copy-VAR-to-var] Used to restart a run from a particular snapshot 
  \file{VAR$N$}. Copies a specified snapshot \file{VAR$N$} to \file{var.dat}
  where $N$ and the target run directory are given on the command line.
\item[cvs-add-rundir] Add the current run directory to the \name{CVS} repository.
\item[cvsci_run] Similar to \cmd{cvs-add-rundir}, but it also checks in
  the \file{*.in} and \file{src/*.local} files. It also checks in the files
  \file[time_series.dat]{data/time_series.dat}, \file[dim.dat]{data/dim.dat} and
  \file[index.pro]{data/index.pro} for subsequent processing in
  \name{IDL} on another machine.
  This is particularly useful if collaborators want to check each others' runs.
\item[dx_*] These script perform several data collection or reformatting 
  exercises required to read particular files into \name{DX}.  They are
  called internally by some of the \name{DX} macros in the 
  \file[dx/macros/]{dx/macros/} directory.
\item[getconf.csh] See \S~\ref{start-run-getconf}
\item[gpgrowth] Plot simple time evolution with Gnuplot's ASCII graphics
  for fast orientation via a slow modem line.
\item[local] Materialize a symbolic link
\item[mkcparam] Based on \file{Makefile} and \file{Makefile.local},
  generate \file[cparam.inc]{src/cparam.inc}, which specifies the number
  \var{mvar} of fundamental variables, and \var{maux} of auxiliary 
  variables. Called by the \file{Makefile}.
\item[mkdatadir] Creates a link to a data directory in a suitable workspace.
  By default this is on \samp{/var/tmp/}, but different locations
  are specified for different machines.
\item[mkdotin] Generate minimal \file{start.in}, \file{run.in} files
  based on \file{Makefile} and \file{Makefile.local}.
\item[mkinpars] Wrapper around \samp{mkdotin} --- needs proper
  documentation.
\item[mkproc-tree] Generates a multi-processor(\file{proc$N$}/), directory 
  structure. Useful when copying data files in a processor tree, 
  such as slice files.
\item[mkwww] Generates a template HTML file for describing a run of the code,
  showing input parameters and results.
\item[move-slice] Moves all the slice files from a processor tree structure,
  \file[proc$N$]{proc$N$/}, to a new target tree creating directories where
  necessary.
\item[nl2idl] Transform a Fortran \name{namelist} (normally the files
  \file{param.nml}, \file{param2.nml} written by the code) into an
  \name{IDL} structure.
  Generates an \name{IDL} file that can be sourced from \file{start.pro}
  or \file{run.pro}.
\item[pacx-adapt-makefile] Version of adapt-makefile for highly 
  distributed runs using PACX MPI.
\item[pc_newrun] Generates a new run directory from an old one. The new one
  contains a copy of the old \file{*.local} files, runs \cmd{setup-src}, and
  makes also a copy of the old \file{*.in} and \file{k.dat} files.
\item[pc_inspect_run] Check the execution of the current run: prints legend
  and the last few lines of the \file{time_series.dat} file. It also appends
  this result to a file called \file{SPEED}, which contains also the current
  wall clock time, so you can work out the speed of the code (without being
  affected by i/o time).
\item[read_videofiles.csh] The script for running read_videofiles.x.
\item[remote-top] Create a file \file{top.log} in the relevant 
  \file[proc$N$]{proc$N$/} directory containing the output of \code{top}
  for the appropriate processor. Used in batch scripts for 
  multi-processor runs.
\item[run.csh] The script for producing restart files with the initial
  condition; see \S~\ref{start-run-getconf}
\item[scpdatadir] Make a tarball of data directory, \file[data/]{data/} and 
  use \code{scp} to secure copy to copy it to the specified destination.
\item[setup-src] Link \file{start.csh}, \file{run.csh} and
  \file{getconf.csh} from \file[bin/]{\$PENCIL_HOME/bin}.
  Generate \file{src/} if necessary and link the source code files from
  \file[src/]{\$PENCIL_HOME/src} to that directory.
\item[start.csh] The script for initializing the code; see
  \S~\ref{start-run-getconf}
\item[summarize-history] Evaluate \file{params.log} and print a history 
  of changes.
\item[tst.csh] Execute \code{start.csh} and \code{run.csh} placing any output
  into a file called \code{.TEST.{`\it timestr}`}.
\item[timestr] Generate a unique time string that can be appended to file
  names from shell scripts through the backtick mechanism.
\item[tsnap] Extract time information from a snapshot file, \file{VAR$N$}.
\end{description}

\vspace{5cm}

% ---------------------------------------------------------------------- %

\subsection{RELOAD and STOP files}
\label{S-RELOAD}

The code periodically (every \var{it} time steps) checks 
for the existence of two files, \file{RELOAD}
and \file{STOP}, which can be used to trigger certain behavior.


\paragraph{Reloading run parameters}
In the directory where you started the code, create the file
\file{RELOAD} with
\begin{alltt}
  \prompt{unix> } touch RELOAD \
\end{alltt}
to force the code to re-read the runtime parameters from \file{run.in}.
This will happen the next time the code is writing monitoring output (the
frequency of this happening is controlled by the input parameter \var{it},
see Sect.~\ref{S-start-params}).

[Occasionally, however, and only on some
machines, one of the processors may not survive the MPI barrier and
stops; we will need to look further into this.]

Each time the parameters are reloaded, the new set of parameters is
appended (in the form of \name{namelists}) to the file
\file[params.log]{data/params.log} together with the time $t$, so you have
a full record of your changes.
If \file{RELOAD} contains any text, its first line will be written to
\file[params.log]{data/params.log} as well, which allows you to annotate
changes:
\begin{alltt}
  \prompt{unix> } echo "Reduced eta to get fields growing" > RELOAD \
\end{alltt}

Use the command \cmd{summarize-history} to print a history of changes.

\paragraph{Stopping the code}
In the directory where you started the code, create the file
\file{STOP} with
\begin{alltt}
  \prompt{unix> } touch STOP \
\end{alltt}
to stop the code in a controlled manner (it will write the latest
snapshot).
Again, the action will happen the next time the code is writing monitoring
output.
% The \file{run.csh} removes any \file{STOP} files before starting
% \file{src/run.x}. If you run without the \file{run.csh} script you need
% to remember doing this by hand.

% ---------------------------------------------------------------------- %

\subsection{RERUN and NEWDIR files}
\label{RERUN}
\index{RERUN file}
\index{NEWDIR file}

After the code finishes (e.g., when the final timestep number is reached
or when a \file{STOP} file is found), the \file{run.csh} script checks
whether there is a \file{RERUN} file.
If so, so code will simply run again, which may be after you have
recompiled the code.
This is useful in the development phase when you changed something in
the code, so you don't need to wait for a new slot in the queue!

Even more naughty, as Tony says, is the \file{NEWDIR} file, where
you can enter a new directory path (relative path is ok, e.g.\
\code{../conv-slab}).
If nothing is written in this file (e.g.\ via \cmd{touch NEWDIR})
it stays in the same directory.
On distributed machines, the \file{NEWDIR} method will copy all the
\file{VAR\#} and \file{var.dat} files back to and from the sever.
This can be useful if you want to run with new data files, but you
better do it in a separate directory, because with \file{NEWDIR}
the latest data from the code are written back to the server before
running again.

% ---------------------------------------------------------------------- %

\subsection{Start and run parameters}
\label{S-start-params}

All input parameters in \file{start.in} and \file{run.in}are grouped in
Fortran \name{namelists}.
This allows arbitrary order of the parameters (\emph{within} the given
namelist; the namelists need to appear in the correct order), as well as
enhanced readability through 
inserted Fortran comments and whitespace.
One namelist (\name{init_pars} / \name{run_pars}) contains general
parameters for initialization/running and is always read in.
All other namelists are specific to individual modules and will only
be read if the corresponding module is used.

The syntax of a namelist (in an input file like \file{start.in}) is
\begin{verbatim}
  &init_pars
    ip=5, Lxyz=2,4,2
  /
\end{verbatim}
--- in this example, the name of the namelist is \name{init_pars}, and we
read just two variables (all other variables in
the namelist retain their previous value): \var{ip}, which is set to $5$,
and \var{Lxyz}, which is a vector of length three and is set to $(2,4,2)$.

While all parameters from the namelists can be set, in most cases
reasonable default values are preset.
Thus, the typical file \file{start.in} will only contain a minimum set of
variables or (if you are \emph{very} minimalistic) none at all.
If you want to run a particular problem, it is best to start by
modifying an existing example that is close to your application.

As an example, we give here the start parameters for
\file{samples/helical-MHDturb}
\begin{alltt}
  &init_pars
    cvsid='\${}Id:\$',                 \textsl{! identify version of start.in}
    xyz0  = -3.1416, -3.1416, -3.1416, \textsl{! first corner of box}
    Lxyz  =  6.2832,  6.2832,  6.2832, \textsl{! box size}
    lperi =  T     ,  T     ,  T     , \textsl{! periodic in x, y, z}
    random_gen='nr_f90'
  /
  &hydro_init_pars
  /
  &density_init_pars
    gamma=1.
  /
  &magnetic_init_pars
    initaa='gaussian-noise', amplaa=1e-4
  /
\end{alltt}
The three entries specifying the location, size and periodicity of the box
are just given for demonstration purposes here --- in fact a periodic box
from $-\pi$ to $-\pi$ in all three directions is the default.
In this run, for reproducibility, we use a random number generator from
the Numerical Recipes \cite{NR}, rather than the compiler's built-in
generator.
The adiabatic index $\gamma$ is set explicitly to $1$ (the default would have
been 5/3) to achieve an isothermal equation of state.
The magnetic vector potential is initialized with uncorrelated, normally
distributed random noise of amplitude $10^{-4}$.


The run parameters for \file{samples/helical-MHDturb} are
\begin{alltt}
  &run_pars
    cvsid='\${}Id:\$',                 \textsl{! identify version of start.in}
    nt=10, it1=2, cdt=0.4, cdtv=0.80, isave=10, itorder=3,
    dsnap=50, dvid=0.5,
    random_gen='nr_f90'
  /
  &hydro_run_pars
  /
  &density_run_pars
  /
  &forcing_run_pars
    iforce='helical', force=0.07, relhel=1.,
  /
  &magnetic_run_pars
    eta=5e-3,
  /
  &viscosity_run_pars
    nu=5e-3
  /
\end{alltt}
Here we run for \var{nt}$=10$ timesteps, every second step, we write a
line of diagnostic output; we require the time step to keep the advective
\name{Courant number} $\le 0.4$ and the diffusive \name{Courant number}
$\le 0.8$, save \file{var.dat} every 20 time steps, and
use the 3-step time-stepping scheme described in Appendix~\ref{S-2N-scheme}
(the Euler scheme \var{itorder}$=1$ is only useful for tests).
We write permanent snapshot file \file{VAR$N$} every \var{dsnap}$=50$ time
units and 2d slices for animation every \var{dvid}$=0.5$ time units.
Again, we use a deterministic random number generator.
Viscosity $\nu$ and magnetic diffusivity $\eta$
are set to $5\times10^{-3}$ (so the mesh Reynolds number is about
$u_{\rm rms}\delta x/\nu=0.3\times(2\pi/32)/5\times10^{-3}\approx12$,
which is in fact rather a bit to high).
The parameters in \name{forcing_run_pars} specify fully helical forcing of
a certain amplitude.

\bigskip

A full list of input parameters is given in Appendix~\ref{S-all-parameters}.

% ---------------------------------------------------------------------- %

\subsection{Physical units}
\label{physdim}
\index{Units}
\index{SI units}
\index{cgs units}

Many calculations are unit-agnostic, in the sense that all results remain
the same independent of the unit system in which you interpret the numbers.
E.\,g.~if you simulate a simple hydrodynamical flow in a box of length $L=1.$
and get a maximum velocity of $u_{\rm max}=0.5$ after $t=3$ time units,
then you may interpret this as
$L=\unit[1]{m}$, $u_{\rm max}=\unit[0.5]{m/s}$, $t=\unit[3]{s}$,
or as $L=\unit[1]{pc}$, $u_{\rm max}=\unit[0.5]{pc/Myr}$, $t=\unit[3]{Myr}$,
depending on the physical system you have in mind.
The units you are using must of course be consistent, thus in the second
example above, the units for diffusivities would be $\unit{pc^2/Myr}$,
etc.

The units of magnetic field and temperature are determined by the values
$\mu_0=1$ and $c_p=1$ used internally by the code\footnote{
  Note that $c_p=1$ is only assumed if you use the module
  \name{noionization.f90}.
  If you work with \name{ionization.f90}, temperature units are specified
  by \var{unit_temperature} as described below.
}.
This means that if your units for density and velocity are
$[\varrho]$ and $[v]$, then magnetic fields will be in
\begin{equation} \label{unit-B}
  [B] = \sqrt{\mu_0\,[\varrho]\,[v]^2} \; ,
\end{equation}
and temperatures are in
\begin{equation} \label{unit-T}
  [T] = \frac{[v]^2}{c_p}
      = \frac{\gamma{-}1}{\gamma}\,\frac{[v]^2}{\mathcal{R}/\mu} \; .
\end{equation}

\begin{table}[htbp]
  \centering
  \caption{Units of magnetic field and temperature for some choices of
    $[\varrho]$ and $[v]$ according to Eqs.~(\ref{unit-B}) and
    (\ref{unit-T}).
    Values are for a monatomic gas ($\gamma=5/3$) of mean atomic
    weight $\bar{\mu}_{\rm g} = \bar{\mu}/\unit[1]{g}$ in grams.}
  \label{Tab-units-B-T}
  \newcommand{\fracstrut}{\rule[-2.0ex]{0pt}{5.2ex}}%
  \begin{tabular}{ll@{\quad}ll}
  \toprule
    $[\varrho]$    
        & $[v]$ 
            & $[B]$
                & $[T]$ \\
  \midrule
    $\unit[1]{kg/m^3}$
        & $\unit[1]{m/s}$
            & $\unit[1.12]{mT} = \unit[11.2]{G}$
                & $\fracstrut\left(\dfrac{\bar{\mu}_{\rm g}}{0.6}\right)
                   \times \unit[2.89\EE{-5}]{K}$\\
    $\unit[1]{g/cm^3}$
        & $\unit[1]{cm/s}$
            & $\unit[3.54\EE{-4}]{T} = \unit[3.54]{G}$
                & $\fracstrut\left(\dfrac{\bar{\mu}_{\rm g}}{0.6}\right)
                   \times \unit[2.89]{nK}$ \\
    $\unit[1]{g/cm^3}$
        & $\unit[1]{km/s}$
            & $\unit[35.4]{T} = \unit[354]{kG}$
                & $\fracstrut\left(\dfrac{\bar{\mu}_{\rm g}}{0.6}\right)
                   \times \unit[28.9]{K}$ \\
    $\unit[1]{g/cm^3}$
        & $\unit[10]{km/s}$
            & $\unit[354]{T} = \unit[3.54]{MG}$
                & $\fracstrut\left(\dfrac{\bar{\mu}_{\rm g}}{0.6}\right)
                   \times \unit[2\,890]{K}$ \\
  \bottomrule
  \end{tabular}  
\end{table}

For some choices of density and velocity units, Table~\ref{Tab-units-B-T}
shows the resulting units of magnetic field and temperature.

On the other hand, as soon as material equations are used (e.\,g.~one of
the popular parameterizations for radiative losses, Kramers opacity,
Spitzer conductivities or ionization, which implies well-defined
ionization energies), the corresponding routines in the code need to know
the units you are using.
This information is specified in \file{start.in} or \file{run.in} through
the parameters \var{unit_system},
\var{unit_length},  \var{unit_velocity},  \var{unit_density} and 
\var{unit_temperature}\footnote{
  Note: the parameter \var{unit_temperature} is currently only used in
  connection with \name{ionization.f90}.
  If you are working with \name{noionization.f90}, the temperature unit is
  completely determined by Eq.~(\ref{unit-T}) above. 
} like e.\,g.
\begin{Verbatim}
  unit_system='SI',
  unit_length=3.09e16, unit_velocity=978.  ! [l]=1pc, [v]=1pc/Myr
\end{Verbatim}

Note: The default unit system is \code{unit_system='cgs'} which is a
synonym for \code{unit_system='babylonian cubits'}.


%% You can run the code in physical units.  Suppose you want the unit length
%% to be $1\,{\rm kpc}$, then you would say \code{unit_length=3e21}.
%% It might then be handy to put \code{unit_velocity=1e5}, which means
%% 1~km/s. This implies a time unit of $3\times10^{16}\,{\rm s}$, which
%% is roughly $1\,{\rm Gyr}$.
%% (Of course, politically correct would be to say \code{unit_system='SI'}
%% in which case you say \code{unit_length=3e19}. More accurate would be
%% \code{unit_length=3.1e19}.)
%% For km/s you'd then say \code{unit_velocity=1e3}.
%% If you are used to work with hot stuff, you'd choose
%% \code{unit_temperature='1e6'} for mega-Kelvin.
%% If you prefer dimensionless units then just don't worry, because unity
%% is the default. But if you are doing things like ionization it is really
%% best to work with physical dimensions.

% ---------------------------------------------------------------------- %

\subsection{Minimum amount of viscosity}
\label{viscosity}

We emphasize that the code works with constant diffusion coefficients
(viscosity $\nu$, thermal diffusivity $\chi$, magnetic diffusivity $\eta$,
or passive scalar diffusivity ${\cal D}$).
If any of these numbers is too small, you would need to have more
meshpoints to get consistent numerical solutions; otherwise the code
develops wiggles (`ringing') and will eventually crash.
A useful criterion is given by the
mesh Reynolds number based on the maximum velocity,
\begin{equation}
  \mbox{Re}_{\rm mesh}=\max(|\uv|)\max(\delta x,\delta y,\delta z)/\nu,
\end{equation}
which should not exceed a certain value which can be problem-dependent.
Often the largest possible value of $\mbox{Re}_{\rm mesh}$ is around 5.
Similarly there exist mesh Peclet and mesh magnetic Reynolds numbers that
should not be too small.
%ajwm - not sure what to write here about the shock viscosity

Note that in some cases, `wiggles' in $\ln\varrho$ will develop despite
sufficiently large diffusion coefficients, essentially because the
continuity equation contains no dissipative term.
For convection runs (but not only for these), we have found that this can
often be prevented by \name[Upwinding]{upwinding}, see Sec.~\ref{S-upwind}.

% ---------------------------------------------------------------------- %

\subsection{The time step}
\label{time-step}
\index{Time step}

The time step is normally specified as Courant time step through the
coefficients \var{cdt} ($c_{\delta t}$), \var{cdtv} ($c_{\delta t,{\rm v}}$)
and \var{cdts} ($c_{\delta t,{\rm s}}$).
The resulting Courant step is given by
\begin{equation}
  \delta t
  = \min\left( c_{\delta t}\frac{\delta x_{\rm min}}
                    {U_{\rm max}} ,
               c_{\delta t,{\rm v}}
               \frac{\delta x_{\rm min}^2}
                    {D_{\rm max}},
               c_{\delta t,{\rm s}}
               \frac{1}
                    {H_{\rm max}}
         \right) \; ,
\end{equation}
where
\begin{equation}
  \delta x_{\rm min} \equiv \min(\delta x, \delta y, \delta z) \; ;
\end{equation}
\begin{equation}
  U_{\rm max} \equiv \max\left(|\uv|
                      + \sqrt{\cs^2{+}\vA^2}\right) \; ,
\end{equation}
$\cs$ and $\vA$ denoting sound speed and Alfv\'en speed, respectively;
\begin{equation} \label{Dmax}
  D_{\rm max} = \max(\nu,\gamma\chi,\eta,D) ,
\end{equation}
where
$\nu$ denotes kinematic viscosity,
$\chi = K/(c_p\varrho)$ thermal
diffusivity and $\eta$ the magnetic diffusivity;
and
\begin{equation} \label{Hmax}
  H_{\rm max} = \max\left(\frac{2\nu\Strain^2
+\zeta_{\rm shock}(\Div\uv)^2+...}{c_vT}\right),
\end{equation}
where dots indicate the presence of other terms on the rhs of the
entropy equation.

To fix the time step $\delta t$ to a value independent of velocities and
diffusivities, explicitly set the run parameter \var{dt}, rather than
\var{cdt} or \var{cdtv} (see p.~\pageref{dt-run}).

If the time step exceeds the viscous time step the simulation  may
actually run ok for quite some time. Inspection of images usually
helps to recognize the problem. An example is shown in
Fig.~\ref{Ftimestepoverviscous}.

\begin{figure}[h!]\begin{center}\includegraphics[width=.99\textwidth]
{timestepoverviscous}\end{center}\caption[]{
Example of a velocity slice from a run where the time step is too long.
Note the spurious checkerboard modulation in places, for example
near $x=-0.5$ and $-2.5<y<-1.5$.
This is an example of a hyperviscous turbulence simulations with
$512^3$ meshpoints and a third order hyperviscosity of
$\nu_3=5\times10^{-12}$.
}\label{Ftimestepoverviscous}\end{figure}

% ---------------------------------------------------------------------- %

\subsection{Boundary conditions}
\label{boundconds}

\subsubsection{Where to specify boundary conditions}
\label{S-boundconds-where}

In most tests that come with the Pencil Code, boundary conditions are set
in \file{run.in}, which is a natural choice.
However, this may lead to unexpected initial data written by
\file{start.x}, since when you start the code (via \file{start.csh}), the
boundary conditions are unknown and \file{start.x} will then fill the ghost
zones assuming periodicity (the default boundary condition) in all three
directions.
These ghost data will never be used in a calculation, as \file{run.x} will
apply the boundary conditions before using any ghost-zone values.

To avoid these periodic conditions in the initial snapshot, you
can set the boundary conditions in \file{start.in} already.
In this case, they will be inherited by \file{run.x}, unless you also
explicitly set boundary conditions in \file{run.in}.


\subsubsection{How to specify boundary conditions}
\index{Ghost zones}

Boundary conditions are implemented through three layers of
\name{ghost points} on either boundary, which is quite a natural choice
for an MPI code that uses ghost zones for representing values located on
the neighboring processors anyway.
The desired type of boundary condition is set through the parameters
\var{bc\{x,y,z\}}
\index[var]{bcx@\emph{bcx}}%
\index[var]{bcy@\emph{bcy}}%
\index[var]{bcz@\emph{bcz}}%
in \file{run.in}; the nomenclature used is as follows.
Set \var{bc\{x,y,z\}} to a sequence of letters like
\begin{alltt}
  bcx = 'p','p','p', 'p',  'p'
\end{alltt}
for periodic boundaries, or
\begin{alltt}
  bcz = 's','s','a','a2','c1:c2'
\end{alltt}
for non-periodic ones.
Each element corresponds to one of the
variables, which are those of the variables
$u_x$, $u_y$, $u_z$, $\ln\varrho$, $s/c_p$, $A_x$, $A_y$, $A_z$, $\ln c$
that are actually used \emph{in this order}.
The following conditions are available:
\begin{description}
\item[\option{p}] periodic boundary condition
\item[\option{a}] antisymmetric condition w.\,r.\,t.~the
  boundary, i.\,e.~vanishing value
\item[\option{s}] symmetric condition w.\,r.\,t.~the
  boundary, i.\,e.~vanishing first derivative
\item[\option{a2}] antisymmetry w.\,r.\,t.~the
  arbitrary value on the boundary, i.\,e.~vanishing
  second derivative
\item[\option{c1}] special boundary condition for
  $\ln\varrho$ and $s$: constant heat flux through the
  boundary
\item[\option{c2}] special boundary condition for
  $s$: constant temperature at the boundary --- requires boundary
  condition \code{a2} for $\ln\varrho$
\item[\option{cT}] special boundary condition for
  $s$ or $\ln\ T$: constant temperature at the boundary (for arbitrarily set
  $\ln\varrho$)
\item[\option{ce}] special boundary condition for
  $s$: set temperature in ghost points to value on boundary (for
  arbitrarily set $\ln\varrho$)
\item[\option{db}] low-order one-sided derivatives (``no
    boundary condition'') for density
\item[\option{she}] shearing-sheet boundary condition (default when the
  module \name{Shear} is used)
\item[\option{g}] force the value of the corresponding field on vertical
boundaries (should be used in combination with the force_lower_bound and
force_upper_bound flags set in the namelist {\it init_pars})
\item[\option{hs}] special boundary condition for $\ln\varrho$ and $s$
which enforces hydrostatic equilibrium on vertical boundaries
\end{description}
The extended syntax $a$:$b$ (e.\,g.~`\code{c1:c2}') means: use
boundary condition $a$ at the left/lower boundary, but
$b$ at the right/upper one.

If you build a new \file{run.in} file from another one with a different number
of variables (\name{noentropy} vs.~\name{entropy}, for example), you need
to remember to adjust the {\it length} of the arrays \var{bcx} to \var{bcz}.
The advantage of the present approach is that it is very easy to exchange
all boundary conditions by a new set of conditions in a particular
direction (for example, make everything periodic, or switch off shearing 
sheet boundary conditions and have stress-free instead).

% ---------------------------------------------------------------------- %

\subsection{Restarting a simulation}

When a run stops at the end of a simulation, you can just resubmit
the job again, and it will start from the latest snapshot saved in
\file{data/proc*/var.dat}. The value of the latest time is saved in a
separate file, \file[time.dat]{data/proc*/time.dat}.
On parallel machines it is possible that some (or just one) of the
\file{var.dat} are corrupt; for example after a system crash.
Check for file size and date, and restart from a good \file{VAR}$N$
file instead.

If you want to run on a different machine, you just need to copy the
\file[var.dat]{data/proc*/var.dat} (and, just to be sure)
\file[time.dat]{data/proc*/time.dat}) files into a new directory tree.
You may also need the \file[seed.dat]{data/proc*/seed.dat}
files for the random number generator. The easiest way to get all these
other files is to run \cmd{start.csh} again on the new machine (or in
a new directory) and then to overwrite the
\file[var.dat]{data/proc*/var.dat} files with the correct ones.

% ---------------------------------------------------------------------- %

\subsection{One- and two-dimensional runs}

If you want to run two-dimensional problems, set the number
of mesh points in one direction to unity, e.g.\ \var{nygrid}=1
or \var{nzgrid}=1 in \file{cparam.local}.
Remember that the number of mesh points is still divisible by
the number of processors.
For 2D-runs, it is also possible to write only 2D-snapshots (i.e. VAR
files written only in the considered $(x,y)$ or $(x,z)$ plane, with a size
seven times smaller as we do not write the third unused direction). To
do that, please add the logical flag `lwrite_2d=T' in the namelist {\it
init_pars} in \file{start.in}.

Similarly, for one-dimensional problems, set, for example,
\var{nygrid}=1 and \var{nzgrid}=1 in \file{cparam.local}.
You can even do a zero-dimensional run, but then you better
set \var{dt} (rather than \var{cdt}), because there is no
Courant condition for the time step.

See \emph{0d, 1d, 2d, and 3d tests} with examples.

% ---------------------------------------------------------------------- %

\subsection{Visualization}
\label{S-Visualization}

\subsubsection{Gnuplot}
\label{S-gnuplot}
Simple visualization can easily be done using \name{Gnuplot}
(\url{http://www.gnuplot.info}), an open-source plotting program suitable
for two-dimensional plots.

For example, suppose you have the variables
\begin{verbatim}
  ---it-----t-------dt-------urms-----umax-----rhom-----ssm------dtc---
\end{verbatim}
in \file{time_series.dat} and want to plot $u_{\rm rms}(t)$.
Just start gnuplot and type
\begin{alltt}
  \prompt{gnuplot> } plot "data/time_series.dat" using 2:4 with lines
\end{alltt}
If you work over a slow line and want to see both  $u_{\rm rms}(t)$ and
$u_{\rm max}(t)$, use ASCII graphics:
\begin{alltt}
  \prompt{gnuplot> } set term dump
  \prompt{gnuplot> } set logscale y
  \prompt{gnuplot> } plot "data/time_series.dat" using 2:4 title "urms", \bs
  \prompt{gnuplot> }      "data/time_series.dat" using 2:5 title "umax"
\end{alltt}


\subsubsection{Data explorer}
\label{S-openDX}

\name{DX} (\name{data explorer}; \url{http://www.opendx.org}) is an
open-source tool for visualization of three-dimensional data.

The Pencil Code provides a few networks for \name{DX}.
It is quite easy to read in a snapshot file from \name{DX} (the only
tricky thing is the four extra bytes at the beginning of the file,
representing a Fortran record marker), and whenever you run \file{start.x},
the code writes a file \file{var.general} that tells \name{DX} all it
needs to know about the data structure.

As a starting point for developing your own \name{DX} programs or
\name{networks}, you can use a few generic \name{DX} scripts provided in
the directory \file[dx/basic]{dx/basic/}.
From the run directory, start \name{DX} with
\begin{alltt}
  \prompt{unix> } dx -edit \$PENCIL_HOME/dx/basic/lnrho
\end{alltt}
to load the file \file[lnrho.net]{dx/basic/lnrho.net}, and
execute it with \key{Ctl-o} or \textsf{Execute $\rightarrow$ Execute Once}.
You will see a set of iso-surfaces of logarithmic density.
If the viewport does not fit to your data, you can reset it with
\key{Ctl-f}.
To rotate the object, drag the mouse over the Image window with the left or
right mouse button pressed.
Similar networks are provided for entropy (\file{ss.net}), velocity
(\file{uu.net}) and magnetic field (\file{bb.net}).

When you expand these simple networks to much more elaborate ones, it is
probably a good idea to separate the different tasks (like Importing and
Selecting, visualizing velocity, visualizing entropy, and Rendering) onto
separate pages through \textsf{Edit $\rightarrow$ Page}.


\paragraph{Note}
Currently, \name{DX} can only read in data files written by one single
processor, so from a multi-processor run, you can only visualize one
subregion at a time.


\subsubsection{IDL}
\label{S-IDL-routines}

\name{IDL} is a commercial visualization program for two-dimensional and
simple three-dimensional graphics.
It allows to access and manipulate numerical data in a fashion quite
similar to how Fortran handles them.

In \file[idl/]{\$PENCIL_HOME/idl}, we provide a number of general-purpose
\name{IDL} scripts that we are using all the time in connection with the
Pencil Code.
While \name{IDL} is quite an expensive software package, it is quite
useful for visualizing results from numerical simulations.
In fact, for many applications, the 7-minute demo version of \name{IDL} is
sufficient.

\bigskip

To see the time evolution of velocity and magnetic field (if they are
present in you run), start \name{IDL}\footnote{
  If you run IDL from the command line, you will highly appreciate the
  following tip:
  IDL's command line editing is broken beyond hope.
  But you can fix it, by running IDL under \cmd{rlwrap}, a wrapper for the
  excellent GNU \name{readline} library.

  Download and install \cmd{rlwrap} from
  \path{http://utopia.knoware.nl/~hlub/uck/rlwrap/} (on some systems you
  just need to run `\code{emerge rlwrap}', or `\code{apt-get install
    rlwrap}'), and alias your \cmd{idl} command:
\begin{alltt}
  \ \ \ \ \prompt{csh> } \ alias idl 'rlwrap -a -c idl'
\end{alltt}
\begin{alltt}
  \ \ \ \ \prompt{bash> } alias idl='rlwrap -a -c idl' 
\end{alltt}

  From now on, you can
  \begin{itemize}
  \setlength{\itemsep}{-0.5\parsep}
  \item use long command lines that correctly wrap around;
  \item type the first letters of a command and then \key{PageUp} to recall
    commands starting with these letters;
  \item capitalize, uppercase or lowercase a word with \key{Esc}-C,
    \key{Esc}-U, \key{Esc}-L;
  \item use command line history across IDL sessions (you might
    need to create \file{\~{}/.idl_history} for this);
  \item complete file names with \key{Tab} (works to some extent);
  \item \ldots use all the other \name{readline} features that you are
    using in \cmd{bash}, \cmd{octave}, \cmd{bc}, \cmd{gnuplot},
    \cmd{ftp}, etc.
  \end{itemize}
}
and run \file{ts.pro}:
\begin{alltt}
  \prompt{unix> } idl
  \prompt{IDL> }  .run ts
\end{alltt}
The \name{IDL} script \file{ts.pro} reads the time series data from
\file[time_series.dat]{data/time_series.dat} and sorts the column into
the structure \var{ts}, with the slot names corresponding to the
name of the variables (taken from the header line of
\file[time_series.dat]{data/time_series.dat}). 
Thus, you can refer to time as \code{ts.t}, to the rms velocity as
\code{ts.urms}, and in order to plot the mean density as a function of
time, you would simply type
\begin{alltt}
  \prompt{IDL> } plot, ts.t, ts.rhom
\end{alltt}

\bigskip

The basic command sequence for working with a snapshot is:
\begin{alltt}
  \prompt{unix> } idl
  \prompt{IDL> }  .run start
  \prompt{IDL> }  .run r
  \prompt{IDL> }  {\sl[specific commands]} \
\end{alltt}
You run \file{start.pro} once to initialize (or reinitialize,
if the mesh size has changed, for example) the fields and read in the
startup parameters from the code.
To read in a new snapshot, run \file{r.pro} (or \file{rall.pro}, see below).

If you are running in parallel on several processors,
the data are scattered over different directories.
To reassemble everything in \name{IDL}, use
\begin{alltt}
  \prompt{IDL> } .r rall
\end{alltt}
instead of \cmd[.run]{.r r}
(here, \cmd[.run]{.r} is a shorthand for \cmd{.run}).
The procedure \file{rall.pro} reads (and assembles) the data from all
processors and correspondingly requires large amounts of memory
for very large runs.
If you want to look at just the data from one processor, use \file{r.pro}
instead.

If you need the magnetic field or the current density, you can calculate
them in IDL by \footnote{
  Keep in mind that \code{jj=curl(bb)} would use iterated first derivatives
  instead of the second derivatives and thus be numerically less accurate
  than \code{jj=curl2(aa)}, particularly at small scales.
}
\begin{alltt}
  \prompt{IDL> } bb=curl(aa)
  \prompt{IDL> } jj=curl2(aa)
\end{alltt}

By default one is reading always the current snapshot
\file[var.dat]{data/proc$N$/var.dat}; if you want to read one of the
permanent snapshots, use (for example)
\begin{alltt}
  \prompt{IDL> } varfile='VAR2'
  \prompt{IDL> } .r r {\sl(or \texttt{.r rall})}
\end{alltt}
See Sect.~\ref{snapshots} for details on permanent snapshots.

With \file{r.pro}, you can switch the part of the domain by changing the
variable \var{datadir}:
\begin{alltt}
  \prompt{IDL> } datadir='data/proc3'
  \prompt{IDL> } .r r
\end{alltt}
will read the data written by processor 3.

\subsubsection{Reading data into IDL structures}

As an alternative to the method described above, there is also the possibility
to read the data into structures. This makes some more operations possible,
e.g.\ reading data from an IDL program where the command \cmd{.r} is not
allowed.

To read a snapshot \cmd{'VAR10'} into the IDL structure \cmd{ff}, type the
following command
\begin{alltt}
  \prompt{IDL> } pc_read_var, obj=ff, varfile='VAR10', /trimall
\end{alltt}
The option \cmd{/trimall} removes ghost zones from the data. A number of other
options are documented in the source code of \cmd{pc_read_var}. You can see
what data the structure contains by using the command \cmd{tag_names}
\begin{alltt}
  \prompt{IDL> } print, tag_names(ff)
  T X Y Z DX DY DZ UU LNRHO AA
\end{alltt}
One can access the individual variables by typing \cmd{ff.varname}, e.g.
\begin{alltt}
  \prompt{IDL> } help, ff.aa
  <Expression>    FLOAT     = Array[32, 32, 32, 3]
\end{alltt}
There are a number of files that read different data into structures. They are
placed in the directory \cmd{\$PENCIL_HOME/idl/files}. Here is a list of files
(including suggested options to call them with)
\begin{itemize}
  \item \cmd{pc_read_var, obj=ff, /trimall} \\
      Read snapshot into structure.
  \item \cmd{pc_read_ts, obj=ts} \\
      Read time series into structure.
  \item \cmd{pc_read_xyaver, obj=xya} \\
      \cmd{pc_read_xzaver, obj=xza} \\
      \cmd{pc_read_yzaver, obj=yza} \\
      Read 1-D time series into structure.
  \item \cmd{pc_read_const, obj=cst} \\
      Read code constants into structure.
  \item \cmd{pc_read_pvar, obj=fp} \\
      Read particle data into structure.
  \item \cmd{pc_read_param, obj=par} \\    
      Read startup parameters into structure.
  \item \cmd{pc_read_param, obj=par2, /param2} \\
      Read runtime parameters into structure.
\end{itemize}
Other options are documented in the source code of the files.

% ---------------------------------------------------------------------- %

\subsection{Running on multi-processor computers}
\label{MPI}

The code is parallelized using \name{MPI} (\dfn{message passing
interface}) for a simple domain decomposition (data-parallelism), which is
a straight-forward and very efficient way of parallelizing
finite-difference codes.
The current version has a few restrictions, which need to be kept in mind
when using the MPI features.

First, only the $y$ and $z$ directions can be distributed over different
processors.
Second, the global number of grid points (but excluding the ghost zones)
in a given direction must be an exact multiple of the number of processors
you use in that direction.
For example if you have \code{nprocy=8} processors for the $y$ direction, you
can run a job with \code{nygrid=64} points in that direction, but if you
try to run a problem with \code{nygrid=65} or \code{nygrid=94}, the code
will complain about an inconsistency and stop.
(So far, this has not been a serious restriction for us.)

\subsubsection{How to run a sample problem in parallel}

To run the sample problem in the directory
\file[conv-slab/]{samples/conv-slab} on 16
CPUs, you need to do the following (in that directory):

\begin{enumerate}

\item Edit \file[Makefile.local]{src/Makefile.local} and replace
  \begin{alltt}
  MPICOMM   = nompicomm \
  \end{alltt}
  by
  \begin{alltt}
  MPICOMM   =   mpicomm \
  \end{alltt}

\item Edit \file[cparam.local]{src/cparam.local} and replace
  \begin{alltt}
  integer, parameter :: ncpus=1,nprocy=1,nprocz=ncpus/nprocy,nprocx=1
  integer, parameter :: nxgrid=32,nygrid=nxgrid,nzgrid=nxgrid \
  \end{alltt}
  by
  \begin{alltt}
  integer, parameter :: ncpus=16,nprocy=4,nprocz=ncpus/nprocy,nprocx=1
  integer, parameter :: nxgrid=128,nygrid=nxgrid,nzgrid=nxgrid \
  \end{alltt}
  The first line specifies a $4{\times}4$ layout of the data in the $y$
  and $z$ direction.
  The second line increases the resolution of the run because
  running a problem as small as $32^3$ on 16 CPUs would be wasteful.
  Even $128^3$ may still be quite small in that respect.
  For performance timings, one should try and keep the size of the
  problem per CPU the same, so for example $256^3$ on 16 CPUs should
  be compared with $128^3$ on 2 CPUs.

\item Recompile the code
  \begin{alltt}
  \prompt{unix> } (cd src; make)
  \end{alltt}

\item Run it
  \begin{alltt}
  \prompt{unix> } start.csh
  \prompt{unix> } run.csh
  \end{alltt}

\end{enumerate}

Make sure that all CPUs see the same \file{data/} directory; otherwise
things will go wrong.

Remember that in order to visualize the full domain with IDL (rather than
just the domain processed and written by one processor), you need to use
\file{rall.pro} instead of \file{r.pro}.

\subsubsection{Hierarchical networks (e.g.~on Beowulf clusters)}
\label{Bandwidth}
\index{bandwidth}
\index{Beowulf clusters}

On big Beowulf clusters, a group of nodes is often connected with a switch
of modest speed, and all these groups are connected via a $n$ times
faster uplink switch.
When bandwidth-limited, it is important to make sure that
consecutive processors are mapped onto the mesh such that the load on
the uplink is $\lesssim n$ times larger than the load on the
slower switch within each group.
On a 512 node cluster, where groups of 24 processors are linked via fast
ethernet switches, which in turn are connected via a Gigabit uplink
($\sim10$ times faster), we found that \var{nprocy}=4 is optimal.
For 128 processors, for example we find that
$\var{nprocy}\times\var{nprocz}=4\times32$ is the optimal layout, while.
For comparison, $8\times16$ is 3 times slower, and $16\times8$
is 17 (!) times slower.
These results can be understood from the structure of the network, but the
basic message is to watch out for such effects and to try varying
\var{nprocy} and \var{nprocz}.

In cases where \code{nygrid}$>$\code{nzgrid} it may be advantageous to
swap the ordering of processor numbers.
This can be done by setting \var{lprocz_slowest}=\code{F}.

\subsubsection{Extra workload caused by the ghost zones}
\index{Ghost zones}

Normally, the workload caused by the ghost zones is negligible.
However, if one increases the number of processors, a significant
fraction of work is done in the ghost zones.
In other words, the effective mesh size becomes much larger than
the actual mesh size.

Consider a mesh of size $N_w=N_x\times N_y\times N_z$,
and distribute the task over $P_w=P_x\times P_y\times P_z$ processors.
If no communication were required, the number of points per processor
would be
\begin{equation}
{N_w\over P_w}={N_x\times N_y\times N_z\over P_x\times P_y\times P_z}.
\end{equation}
However, for finite difference codes some communication is required,
and the amount of communication depends depends on spatial order of the
scheme, $Q$.
The {\sc Pencil Code} works by default with sixth order finite differences,
so $Q=6$, i.e.\ one needs 6 ghost zones, 3 on each end of the mesh.
With $Q\neq0$ the number of points per processor is
\begin{equation}
{N_w^{\rm(eff)}\over P_w}=
\left({N_x\over P_x}+Q\right)\times
\left({N_y\over P_y}+Q\right)\times
\left({N_z\over P_z}+Q\right).
\end{equation}
There is efficient scaling only when
\begin{equation}
\min\left({N_x\over P_x}, {N_y\over P_y}, {N_z\over P_z}\right)\gg Q.
\end{equation}
In the special case were $N_x=N_y=N_z\equiv N=N_w^{1/3}$,
with $P_x=1$ and $P_y=P_z\equiv P=P_w^{1/2}$, we have 
\begin{equation}
{N_w^{\rm(eff)}\over P_w}=
\left(N+Q\right)\times
\left({N\over P}+Q\right)^2.
\end{equation}
For $N=128$ and $Q=6$ the effective mesh size exceeds the actual mesh
size by a factor
\begin{equation}
{N_w^{\rm(eff)}\over N_w}
=\left(N+Q\right)\times\left({N\over P}+Q\right)^2\times{P_w\over N_w}.
\end{equation}
These factors are listed in Table~\ref{EffectiveMesh}.

\begin{table}[h!]\caption{
$N_w^{\rm(eff)}/N_w$ versus $N$ and $P$.
}\vspace{12pt}\centerline{\begin{tabular}{rrrrrrrrrrrr}
$P\backslash N$ &    128  &    256  &    512  &   1024  &   2048 \\
\hline
    1  &   1.15  &   1.07  &   1.04  &   1.02  &   1.01 \\
    2  &   1.19  &   1.09  &   1.05  &   1.02  &   1.01 \\
    4  &\underline{1.25}&   1.12  &   1.06  &   1.03  &   1.01 \\
    8  &   1.34  &   1.16  &   1.08  &   1.04  &   1.02 \\
   16  &   1.48  &\underline{1.22}&   1.11  &   1.05  &   1.03 \\
   32  &   1.68  &   1.31  &   1.15  &   1.07  &   1.04 \\
   64  &   1.98  &   1.44  &\underline{1.21}&   1.10  &   1.05 \\
  128  &   2.45  &   1.64  &   1.30  &   1.14  &   1.07 \\
  256  &   3.21  &   1.93  &   1.43  &\underline{1.20}&   1.10 \\
  512  &   4.45  &   2.40  &   1.62  &   1.29  &   1.14 \\
\label{EffectiveMesh}\end{tabular}}\end{table}

Ideally, one wants to keep the work in the ghost zones at a minimum.
If one accepts that 20--25\% of work are done in the ghost zones,
one should use 4 processors for $128^3$ mesh points,
16 processors for $256^3$ mesh points,
64 processors for $512^3$ mesh points,
256 processors for $1024^3$ mesh points,
and 512 processors for $1536^3$ mesh points.

%\begin{figure}[h!]\begin{center}
%\includegraphics[width=\columnwidth]{figs/processor_scaling}
%\end{center}\caption[]{
%Scaling
%}\label{pcoolheat}\end{figure}

% ---------------------------------------------------------------------- %

\subsection{Running in double-precision}
\label{double-precision}
\index{double precision}

With many compilers, you can easily switch to double precision (8-byte
floating point numbers) as follows.

Add the lines
\begin{alltt}
  # Use double precision
  REAL_PRECISION = double
\end{alltt}
to \file[Makefile.local]{src/Makefile.local} and (re-)run
\cmd{pc_setupsrc}.

If \var{REAL_PRECISION} is set to `double', the flag \var{FFLAGS_DOUBLE}
is appended to the Fortran compile flags.
The default for \var{FFLAGS_DOUBLE} is \code{-r8}, which works for
\name{g95} or \cmd{ifort}; for \name{gfortran}, you need to make sure that
\var{FFLAGS_DOUBLE} is set to \code{-fdefault-real-8}.

You can see the flags in \file{src/Makefile.inc}, which is the first place
to check if you have problems compiling for double precision.

\medskip

Using double precision might be important in turbulence runs where
the resolution is $256^3$ meshpoints and above (although such runs
often seem to work fine at single precision).
The procedure \file[realtodouble.x]{postproc/src/realtodouble.x} can be
used to convert existing
\file{var.dat} and \file{grid.dat} files to double precision.

% ---------------------------------------------------------------------- %

\subsection{Power spectrum}
\label{power-spectrum}
Given a real variable $u$, its Fourier transform $\tilde{u}$
is given by
\begin{eqnarray}
  \tilde{u}(k_x,k_y,k_z)
  \ =\ \mathcal{F}(u(x,y,z))
  &=& \frac{1}{N_x N_y N_z} \sum_{p=0}^{N_x-1} \sum_{q=0}^{N_y-1} 
       \sum_{r=0}^{N_z-1} u(x_p,y_q,z_r)  \nonumber \\
  & & {} \times \exp(-i k_x  x_p) \exp(-i k_y y_q) 
         \exp(-i k_z z_r) ,
\end{eqnarray}
where
\[
   |k_x| < \frac{\pi N_x}{L_x} \, ,\qquad
   |k_y| < \frac{\pi N_y}{L_y} \, ,\qquad
   |k_z| < \frac{\pi N_z}{L_z} \, ,
\]
when $L$ is the size of the simulation box.
The three-dimensional power spectrum $P(k)$ is defined as
\begin{equation}
  P(k)=\frac{1}{2}\tilde{u}\tilde{u}^*,
\end{equation}
where
\begin{equation}
  k=\sqrt{k_x^2+k_y^2+k_z^2} .
\end{equation}
Note that we can only reasonably calculate $P(k)$ for $k < \pi N_x/L_x$.

To get power spectra from the code, edit \file{run.in} and add for example
the following lines
\begin{alltt}
  dspec=2.3, 
  vel_spec=T,mag_spec=T,
  fft_switch='fftpack',
  oned=T \
\end{alltt}
under \code{run_pars}.
The kinetic (\code{vel_spec}) and magnetic  (\code{mag_spec}) power spectra
will now be calculated every 2.3 (\code{dspec}) time unit. 
The Fourier spectra is calculated using \name{fftpack} 
(\code{fft_switch}). In addition to calculating the three-dimensional
power spectra also the one-dimensional power spectra will be calculated 
(\code{oned}).

In addition one must edit 
\file[Makefile.local]{src/Makefile.local} and add the lines
\begin{alltt}
   FOURIER=fft_pack
   POWER=power_spectrum
\end{alltt}
You should also make sure that \var{nxgrid}=\var{nygrid}=\var{nzgrid}.

Running the code will now create the files \file{powerb.dat} and 
\file{poweru.dat} containing the three-dimensional 
magnetic and kinetic power spectra respectively. In addition to these 
three-dimensional files we will also find the one-dimensional files
\file{powerbx_x.dat},
\file[powerbx_x.dat]{powerby_x.dat}, % one index entry is enough..
\file[powerbx_x.dat]{powerbz_x.dat},
\file{powerux_x.dat},
\file[powerux_x.dat]{poweruy_x.dat} and
\file[powerux_x.dat]{poweruz_x.dat}.
In these files the data are stored such that the first line contain the
time of the snapshot, the following $\var{nxgrid}/2$ numbers represent the
power at each wavenumber, from the smallest to the largest.
If several snapshots have been saved, they are being stored immediately 
following the preceding snapshot.

To visualize with \name{IDL} just type
\cmd{power} and you get the last snapshot of the three-dimensional 
power spectrum. See head of \file[power.pro]{\$PENCIL_HOME/idl/power.pro} 
for options to \cmd{power}.

% ---------------------------------------------------------------------- %

\subsection{Structure functions}
\label{structure-functions}
We define the p-th order longitudinal structure function of $\vec{u}$ as
\begin{equation}
S^p_{\rm long}(l)=\left< | u_x(x{+}l,y,z)-u_x(x,y,z)|^p \right> \; ,
\end{equation}
while the transversal is 
\begin{equation}
S^p_{\rm trans}(l)= \left< | u_y(x{+}l,y,z)-u_y(x,y,z)|^p \right> 
               + \left< | u_z(x{+}l,y,z)-u_z(x,y,z)|^p \right> \; .
\end{equation}

Edit \file{run.in} and add for example the following lines 
  \begin{alltt}
  dspec=2.3,
  lsfu=T,lsfb=T,lsfz1=T,lsfz2=T
  \end{alltt}
under \code{run_pars}.
The velocity (\code{lsfu}), magnetic (\code{lsfb}) and Elsasser 
(\code{lsfz1} and \code{lsfz2}) structure functions will
now be calculated every 2.3 (\code{dspec}) time unit. 

In addition one must edit 
\file[Makefile.local]{src/Makefile.local} and add the line
  \begin{alltt}
  STRUCT_FUNC  = struct_func
  \end{alltt}
You should also make sure that \var{nxgrid}=\var{nygrid}=\var{nzgrid}.

Running the code will now create the files: \\ 
\file{sfu-1.dat},
\file[sfu-1.dat]{sfu-2.dat},    % one index entry is enough
\file[sfu-1.dat]{sfu-3.dat} (velocity), \\ 
\file{sfb-1.dat},
\file[sfb-1.dat]{sfb-2.dat},
\file[sfb-1.dat]{sfb-3.dat} (magnetic field), \\ 
\file{sfz1-1.dat},
\file[sfz1-1.dat]{sfz1-2.dat},
\file[sfz1-1.dat]{sfz1-3.dat} (first Elsasser variable), \\ 
\file[sfz1-1.dat]{sfz2-1.dat},
\file[sfz1-1.dat]{sfz2-2.dat},
\file[sfz1-1.dat]{sfz2-3.dat} (second Elsasser variable), \\ 
which contains the data of interest. 
The first line in each file contains the time $t$ 
and the number \var{qmax}, such that the largest moment
calculated is $\var{qmax}-1$.
The next \var{imax} numbers represent the first moment 
structure function for the first snapshot, here
\begin{equation}
\var{imax}=2\frac{\ln (\var{nxgrid})}{\ln 2}-2.
\end{equation}
The next \var{imax} numbers contain the second moment structure function, 
and so on until $\var{qmax}-1$.
The following \var{imax} numbers then contain the data of the 
{\it signed} third order structure function i.e.  
$S^3_{\rm long}(l)=\left< [u_x(x{+}l,y,z)-u_x(x,y,z)]^3 \right>$.

The following $\var{imax} \times \var{qmax} \times 2$ numbers are zero if 
$\var{nr_directions}=1$ 
(default), otherwise they are the same data as above but for the 
structure functions calculated in the y and z directions.

If the code has been run long enough as to calculate several snapshots, these
snapshots will now follow, being stored in the same way as the first snapshot.

To visualize with \name{IDL} just type
\cmd{structure} and you get the time-average of the first order longitudinal 
structure function (be sure that
\file[forced/idl/]{pencil-runs/forced/idl/} is in \env{IDL_PATH}).
See head of \file[structure.pro]{pencil-runs/forced/idl/structure.pro} 
for options to \cmd{structure}.

% ---------------------------------------------------------------------- %

\subsection{Particles}

The Pencil Code has modules for tracer particles and for dust particles (see
Sect.~\ref{S-particles-equations}). The particle modules are chosen by setting
the value of the variable \code{PARTICLES} in \code{Makefile.local} to either
\code{particles_dust} or \code{particles_tracers}. In addition one must define
a few parameters in \code{cparam.local}. Here is a sample of
\code{cparam.local} for a parallel run with 2,000,000 particles:
\begin{verbatim}
integer, parameter :: ncpus=16,nprocy=4,nprocz=ncpus/nprocy,nprocx=1
integer, parameter :: nxgrid=128,nygrid=256,nzgrid=128
integer, parameter :: npar=2000000, mpar_loc=400000, npar_mig=1000
\end{verbatim}
The variable \code{npar} is the number of particles in the simulation,
\code{mpar_loc} is the number of particles that is allowed on each processor
and \code{npar_mig} is the number of particles that are allowed to migrate from
one processor to another in any time-step. For a non-parallel run it is enough
to specify \code{npar}. The two other parameters are explained in the following
section. The particle input parameters are given in \code{start.in} and
\code{run.in}. Here is a sample of the particle part of \code{start.in} for
dust particles:
\begin{verbatim}
/
&particles_init_pars
  initxxp='gaussian-z', initvvp='random'
  zp0=0.02, delta_vp0=0.01, eps_dtog=0.01, tausp=0.1
  lparticlemesh_tsc=T
/
\end{verbatim}
The initial positions and velocities of the dust particles are set in
\code{initxxp} and \code{initvvp}. The next four input parameters are further
specifications of the initial condition. Interaction between the particles and
the mesh, e.g.\ through drag force or self-gravity, require a mapping of the
particles on the mesh. The Pencil Code currently supports NGP (Nearest Grid
Point, default), CIC (Cloud in Cell, set \code{lparticlemesh_cic=T}) and TSC
(Triangular Shaped Cloud, set \code{lparticlemesh_tsc=T}).

Here is a sample of the particle part of \code{run.in} (also for dust
particles):
\begin{verbatim}
/
&particles_run_pars
  ldragforce_gas_par=T
  cdtp=0.2
/
\end{verbatim}
The logical \code{ldragforce_gas_par} determines whether the dust particles
influence the gas with a drag force. \code{cdtp} tells the code how many
friction times should be resolved in a minimum time-step.

The sample run \file{samples/2d-tests/Kelvin-Helmholtz-disc/} contains the
latest setup for dust particles.

\subsubsection{Particles in parallel}

The particle variables (e.g. $\vec{x}_i$ and $\vec{v}_i$) are kept in the arrays
\code{fp} and \code{dfp}.  For parallel runs, particles must be able to move
from processor to processor as they pass out of the $(x,y,z)$-interval of the
local processor. Since not all particles are present at the same processor at
the same time (hopefully), there is some memory optimization in making
\code{fp} not big enough to contain all the particles at once. This is achieved
by setting the code variable \code{mpar_loc} less than \code{npar} in
\code{cparam.local} for parallel runs. When running with millions of particles,
this trick is necessary to keep the memory need of the code down.

The communication of migrating particles between the processors happens as
follows (see the subroutine \code{redist_particles_procs} in
\code{particles_sub.f90}):
\begin{enumerate}
  \item In the beginning of each time-step all processors check if any of their
      particles have crossed the local $(x,y,z)$-interval. These particles are
      called migrating particles. A run can have a maximum of \code{npar_mig}
      migrating particles in each time-step. The value of \code{npar_mig} must
      be set in \code{cparam.local}. The number should (of course) be slightly
      larger than the maximum number of migrating particles at any time-step
      during the run. The diagnostic variable \code{nmigmax} can be used to
      output the maximum number of migrating particles at a given time-step.
      One can set\\ \code{lmigration_redo=T} in \code{\&particles_run_pars} to
      force the code to redo the migration step if more than \code{npar_mig}
      want to migrate. This does slow the code down somewhat, but has the
      benefit that the code does not stop when more than \code{npar_mig}
      particles want to migrate.
  \item The index number of the receiving processor is then calculated. This
      requires some assumption about the grid on other processors and will
      currently not work for nonequidistant grids. Particles don't always pass
      to neighboring processors as the global boundary conditions may send
      them to the other side of the global domains (periodic boundary
      conditions).
  \item The migrating particle information is copied to the end of $\code{fp}$,
      and the empty spot left behind is filled up with the particle of the
      highest index number currently present at the processor.
  \item Once the number of migrating particles is known, this information is
      shared between the processors so that they all know how may particles
      they have to receive and from which processors.
  \item The communication happens as directed MPI communication. That means
      that processors 0 and 1 can share migrating particles at the
      same time as processors 2 and 3 do it. The communication happens from a
      chunk at the end of \code{fp} (migrating particles) to a chunk that is
      present just after the particle of the highest index number that is
      currently at the receiving processor. Thus the particles are put directly
      at their final destination, and the migrating particle information at
      the source processor is simply overwritten by other migrating particles
      at the next time-step.
  \item Each processor keeps track of the number of particles that it is
      responsible for. This number is stored in the variable \code{npar_loc}.
      It must never be larger than \code{mpar_loc} (see above). When a
      particles leaves a processor, \code{npar_loc} is reduced by one, and then
      increased by one at the processor that receives that particle. The
      maximum number of particles at any processor is stored in the diagnostic
      variable \code{nparmax}. If this value is not close to
      \code{npar}/\code{ncpus}, the particles have piled up in such a way that
      computations are not evenly shared between the processors. One can then
      try to change the parallelization architecture (\code{nprocy} and
      \code{nprocz}) to avoid this problem.
\end{enumerate}

% ---------------------------------------------------------------------- %

\subsection{Non-cartesian coordinate systems}

Since the spring of 2007 spherical and cylindrical polar coordinates
have been implemented, although this development is not yet completed.
Spherical coordinates are invoked by adding the following line
in the file \file{start.in}
\begin{verbatim}
&init_pars
  coord_system='spherical_coords'
\end{verbatim}
Another possibility is to put \code{cylindrical_coords} instead.
In practice, the names $(x,y,z)$ are still used, but they refer then
to $(r,\theta,\phi)$ or $(r,\phi,z)$ instead.

Bug reports, corrections, and improvements on these are appreciated.

% ====================================================================== %

\section{The Equations}

The equations solved by the Pencil Code are basically the standard
compressible MHD equations. However, the modular structure allows
some variations of the MHD equations, as well as switching off
some of the equations or individual terms of the equation (nomagnetic, 
noentropy, etc.).

In this section the equations are presented in their most complete form.
It may be expected that the code can evolve most subsets or 
simplifications of these equations.

% ---------------------------------------------------------------------- %

\subsection{Continuity equation}

In the code the continuity equation,
$\partial\varrho/\partial t+\Div\varrho\uv=0$,
is written in terms of $\ln\varrho$,
\begin{equation}
  \frac{D\ln\varrho}{Dt}
  = - \Div\uv \; .
\end{equation}
Here $\varrho$ denotes density, $\uv$ the fluid velocity, $t$ is time and
$D/Dt \equiv \partial/\partial t + \uv\cdot\grad$ is the convective
derivative.

% ---------------------------------------------------------------------- %

\subsection{Equation of motion}
\label{S-Eqn-of-motion}

In the equation of motion, using a perfect gas, the pressure term,
can be expressed as
$-\varrho^{-1}\grad p = -\cs^2(\grad s/c_p+\grad\ln\varrho)$, 
where the squared sound speed is given by
\begin{equation}
  \cs^2 = \gamma \frac{p}{\varrho}
        = c_{\rm s0}^2\exp\left[\gamma s/c_p
                                + (\gamma{-}1)\ln\frac{\varrho}{\varrho_0}
                               \right],
\label{EOSsimple}
\end{equation}
and $\gamma=c_p/c_v$ is the ratio of specific heats, or \emph{adiabatic index}.

The equation of motion is accordingly
\begin{eqnarray}
  \frac{D\uv}{Dt}
   =& &-\cs^2\grad\biggl(\frac{s}{c_p} + \ln\varrho\biggr)
      - \grad\Phi_{\rm grav}
      + \frac{\jv\times\Bv}{\varrho}  \nonumber \\
    & &+ \nu \left( \Laplace\uv + \frac{1}{3}\grad\Div\uv
      + 2\Strain\cdot\grad\ln\varrho\right) + \zeta\left(\grad\Div\uv\right); 
\label{DuDt}
\end{eqnarray}
Here $\Phi_{\rm grav}$ is the gravity potential,
$\jv$ the electric current density, $\Bv$
the magnetic flux density, $\nu$ is kinematic viscosity,
\begin{equation} \label{Eq-S-traceless}
  {\mathsf S}_{ij} = \frac{1}{2}\left({\partial u_i\over\partial x_j}
                 + {\partial u_j\over\partial x_i}
                 -\frac{2}{3} \delta_{ij}\Div\uv\right)
\end{equation}
is the traceless rate-of-strain tensor and $\zeta$ describes some bulk
viscosity.

The interpretation of the two viscosity terms varies greatly depending upon 
the Viscosity module used, and indeed on the parameters given to the module.
See \S\ref{Bulkviscosity}.
%AB: Is this ref reasonably ok?

%ajwm - move this elsewhere? See section...
%ajwm - and add description of shock viscosity
%If \code{ivisc='nu-const'}, it is assumed that 
%$\nu = \const$ everywhere.
%
%If \code{ivisc='rho_nu-const'}, it is assumed that 
%$\mu \equiv \varrho\nu = \const$ everywhere; in that
%case the input parameter \var{nu} is used to give the value of
%$\mu/\varrho_0$ (\var{rho0} is another input parameter).
%The term involving the
%rate-of-strain tensor does not appear in the equation,
%and $\nu$ is then replaced by $\mu/\varrho$, which is no longer constant.
%
%If \code{ivisc='simplified'}, a simplified version, $\nu\Laplace\uv$, of the
%viscous term is used.
%This viscous operator is not physically consistent (linear and angular
%momentum conservation is violated for compressible media), but for weakly
%compressible flows (Mach numbers less than 10\%), there is little
%difference to the full one.
%In practice, one may decide to use the simplified viscous operator
%during the initial relaxation phase of a long run, provided this
%is not already part of a production run.



For isothermal hydrodynamics, see \S\ref{entropy} below.

% ---------------------------------------------------------------------- %

\subsection{Induction equation}

\begin{equation}
  \frac{\partial\Av}{\partial t}
  = \uv\times\Bv - \eta\mu_0\jv \; .
\end{equation}

Here $\Av$ is the magnetic vector potential\index{Vector potential},
$\Bv = \curl\Av$ the magnetic
flux density, $\eta = 1/(\mu_0\sigma)$ is the magnetic diffusivity
($\sigma$ denoting the electrical conductivity), and $\mu_0$ the
magnetic vacuum permeability.
This form of the induction equation corresponds to the \name{Weyl gauge}
$\Phi=0$, where $\Phi$ denotes the scalar potential.


% ---------------------------------------------------------------------- %

\subsection{Entropy equation}
\index{Entropy}%
\label{entropy}%

The current thermodynamics module \name{entropy} formulates the thermal
part of the physics in terms of \emph{entropy} $s$, rather than thermal
energy $e$, which you may be more familiar with.
Thus the two fundamental thermodynamical variables are $\ln\varrho$
and $s$.
The reason for this choice of variables is that entropy is the natural
physical variable for (at least) convection processes: the sign of the
entropy gradient determines convective (in)stability, the
\emph{Rayleigh number} is proportional to the entropy gradient
of the associated hydrostatic reference solution, etc.
The equation solved is
\begin{equation}
  \varrho T\frac{Ds}{Dt}
   =  \Heat - \Cool
      + \Div(K\grad T)
      + \eta\mu_0 \jv^2
      + 2\varrho\nu\Strain^2 + \zeta\varrho\left(\Div\uv\right)^2\; .
\end{equation}

Here, $T$ is temperature, $c_p$ the specific heat at constant pressure,
$\Heat$ and $\Cool$ are explicit heating and cooling terms,
$K$ is the radiative (thermal) conductivity, 
$\Strain$ is the traceless rate-of-strain tensor and $\zeta$ describes
some bulk viscosity.

The heat conduction term can be written in the form
\begin{eqnarray}
\lefteqn{\frac{\Div(K\grad T)}
              {\varrho c_p T}} \\
%
  &=&
  \chi\Bigl[
        \Laplace\ln T
        + \grad\ln T \cdot \grad(\ln T{+}\ln\chi{+}\ln\varrho)
      \Bigr] \\
%
  &=&
  \chi \left[ \gamma\Laplace s/c_p + (\gamma{-}1)\Laplace\ln\varrho \right] \nonumber\\
  & &   + \chi \left[ \gamma\grad s/c_p
                      + (\gamma{-}1)\grad\ln\varrho \right]
          \cdot\left[ \gamma\left(\grad s/c_p + \grad\ln\varrho\right)
                      + \grad\ln\chi \right] \; ,
\end{eqnarray}
where $\chi = K/(\varrho c_p)$ is the thermal diffusivity.
The latter equation shows that the diffusivity for $s$ is $\gamma\chi$,
which is what we have used in Eq.~(\ref{Dmax}).

\bigskip

Note that by setting $\gamma=1$ and initially $s=0$, one obtains an
isothermal equation of state (albeit at some unnecessary expense of
memory).
Similarly, by switching off the evolution terms of entropy, one immediately
gets polytropic behavior (if $s$ was initially constant) or generalized
polytropic behavior
(where $s$ is not uniform, but $\partial s/\partial t = 0$).

A better way to achieve isothermality is to use the \name{noentropy}
module.

\subsubsection{Viscous heating}

We can write the viscosity as the divergence of a tensor $\tau_{ij,j}$,
\begin{equation}
  \rho \frac{\partial u_i}{\partial t} = \tau_{ij,j} \, ,
\end{equation}
where $\tau_{ij}=2\nu\rho{\sf S}_{ij}$ is the stress tensor. The viscous power
density $P$ is
\begin{eqnarray}
  P &=& u_i\tau_{ij,j}\\
    &=& {\partial\over\partial x_j}\left(u_i\tau_{ij}\right) - u_{i,j}\tau_{ij}
\end{eqnarray}
The term under the divergence is the viscous energy flux and the other
term is the kinetic energy loss due to heating.
The heating term $+u_{i,j}\tau_{ij}$ is positive definite, because $\tau_{ij}$ is a symmetric tensor
and the term only gives a contribution from the symmetric part of $u_{i,j}$,    which
is $\frac{1}{2}(u_{i,j}+u_{j,i})$, so
\begin{equation}
  u_{i,j}\tau_{ij}=\frac{1}{2}\nu\rho(u_{i,j}+u_{j,i})(2{\sf S}_{ij}) \, .
\end{equation}
But, because ${\sf S}_{ij}$ is traceless, we can add anything
proportional to $\delta_{ij}$ and, in particular, we can write
\begin{eqnarray}
u_{i,j}\tau_{ij}&=&\frac{1}{2}(u_{i,j}+u_{j,i})(2\nu\rho{\sf S}_{ij})\\
&=&\frac{1}{2}(u_{i,j}+u_{j,i}-\frac{1}{3}\delta_{ij}\nab\cdot\vec{u})(2\nu\rho{\sf         S}_{ij})\\
&=&2\nu\rho\mathbf{S}^2,
\end{eqnarray}
which is positive definite.


% ---------------------------------------------------------------------- %

\subsection{Transport equation for a passive scalar}

In conservative form, the equation for a passive scalar is
\begin{equation}
{\partial\over\partial t}(\varrho c)+
\Div\left[\varrho c\uv-\varrho{\cal D}\nabla c\right]=0.
\end{equation}
Here $c$ denotes the concentration (per unit mass) of the passive scalar and
${\cal D}$ its diffusion constant (assumed constant).
In the code this equation is solved in terms of $\ln c$,
\begin{equation}
  \frac{D\ln c}{Dt}
  = {\cal D} \left[ \Laplace\ln c + (\grad\ln\varrho+\grad\ln c)\cdot\grad\ln c
               \right] \; .
\end{equation}
Using $\ln c$ instead of $c$ has the advantage that it enforces $c>0$ for all
times.

% ---------------------------------------------------------------------- %

\subsection{Bulk viscosity}
\label{Bulkviscosity}
\index{Viscosity}

For a monatomic gas it can be shown that the bulk viscosity vanishes.
We therefore don't use it in most of our runs.
However, for supersonic flows, or even otherwise, one might want to add a
shock viscosity which, in its simplest formulation, take the form of a
bulk viscosity.

\subsubsection{Shock Viscosity}
\index{Shock viscosity}

Shock viscosity, as it is used here and also in the Stagger Code of {\AA}ke Nordlund,
is proportional to positive flow convergence, maximum over
five zones, and smoothed to second order,
\begin{equation}
\zeta_{\rm shock}=c_{\rm shock}\left<\max_5[(-\Div\uv)_+]\right>(\min(\delta x,\delta y,\delta z))^2,
\end{equation}
where $c_{\rm shock}$ is a constant defining the strength of the shock viscosity.
In the code this dimensionless coefficient is called \code{nu_shock}, and it
is usually chosen to be around unity.
Assume that the shock viscosity only enters as a bulk viscosity,
so the whole stress tensor is then
\begin{equation}
\vekt{\tau}_{ij}=2\varrho\nu{\sf S}_{ij}+\varrho\zeta_{\rm shock}\delta_{ij}\nab\cdot\uv.
\end{equation}
Assume $\nu=\mbox{const}$, but $\zeta\neq\mbox{const}$, so
\begin{equation}
\varrho^{-1}\vekt{F}_{\rm visc}=
\nu\left(\Laplace\uv+\frac{1}{3}\grad\Div\uv+2\Strain\cdot\grad\ln\varrho\right)
+\zeta_{\rm shock}\left[\grad\Div\uv+\left(\grad\ln\varrho+\grad\ln\zeta_{\rm shock}\right)\Div\uv\right].
\end{equation}
and
\begin{equation}
\varrho^{-1}\Gamma_{\rm visc}=2\nu\Strain^2+\zeta_{\rm shock}(\Div\uv)^2.
\end{equation}

% ---------------------------------------------------------------------- %

\subsection{Equation of state}
\index{Equation of state}
\label{S-eos}

In its present configuration only hydrogen ionization is explicitly included.
Other constituents (currently He and H$_2$) can have fixed values.
The pressure is proportional to the total number of particles, i.e.\
\begin{equation}
p=(n_{\rm HI}+n_{\rm HII}+n_{\rm H_2}+n_{\rm e}+n_{\rm He}+...)k_{\rm B}T.
\end{equation}
It is convenient to normalize to the total number of H both in atomic
and in molecular hydrogen, $n_{\rm Htot}\equiv n_{\rm H}+2n_{\rm H_2}$,
where $n_{\rm HI}+n_{\rm HII}=n_{\rm H}$, and define
$x_{\rm e}\equiv n_{\rm e}/n_{\rm Htot}$,
$x_{\rm He}\equiv n_{\rm He}/n_{\rm Htot}$, and
$x_{\rm H_2}\equiv n_{\rm H_2}/n_{\rm Htot}$.
Substituting $n_{\rm H}=n_{\rm Htot}-2n_{\rm H_2}$, we have
\begin{equation}
p=(1-x_{\rm H_2}+x_{\rm e}+x_{\rm He}+...)n_{\rm Htot}k_{\rm B}T.
\end{equation}
This can be written in the more familiar form
\begin{equation}
p={{\cal R}\over\mu}\rho T,
\end{equation}
where ${\cal R}=k_{\rm B}/m_{\rm u}$ and
$m_{\rm u}$ is the atomic mass unit (which is for all practical
purposes the same as $m_{\rm Htot}$) and
\begin{equation}
\mu={n_{\rm H}+2n_{\rm H_2}+n_{\rm e}+4n_{\rm He}\over
n_{\rm H}+n_{\rm H_2}+n_{\rm He}}
={1+4x_{\rm He}\over1-x_{\rm H_2}+x_{\rm e}+x_{\rm He}}
\end{equation}
is the mean molecular weight (which is here dimensionless; see
Kippenhahn \& Weigert 1990, p.\ 102).
The factor 4 is really to be substituted for 3.97153.
%AB: need to check more carefully:
Some of the familiar relations take still the usual form, in particular
$e=c_vT$ and $h=c_pT$ with $c_v={3\over2}{\cal R}/\mu$ and
$c_p={5\over2}{\cal R}/\mu$.

The number ratio, $x_{\rm He}$, is more commonly expressed as the mass
ratio, $Y=m_{\rm He}n_{\rm He}/(m_{\rm H}n_{\rm Htot}+m_{\rm He}n_{\rm e}n_{\rm He})$,
or $Y=4x_{\rm He}/(1+4x_{\rm He})$, or $4x_{\rm He}=(1/Y-1)^{-1}$.
For example, $Y=0.27$ corresponds to $x_{\rm He}=0.9$.
Note also that for 100\% H$_2$ abundance, $x_{\rm H_2}=1/2$.

In the following, the ionization fraction is given as $y=n_{\rm e}/n_{\rm H}$,
which can be different from $x_{\rm e}$ if there is H$_2$.
Substituting for $n_{\rm H}$ in terms of $n_{\rm Htot}$ yields
$y=x_{\rm e}/(1-2x_{\rm H_2})$.

% ---------------------------------------------------------------------- %

\subsection{Ionization}
\index{Ionization}
\label{S-ionization}

This part of the code can be invoked by setting
\code{IONIZATION=ionization} in the \file{Makefile.local} file.
The equation of state described below works for variable ionization,
and the entropy offset is different from that used in
Eq.~(\ref{EOSsimple}), which is now no longer valid.
As a replacement, one can use \code{IONIZATION=ionization\_fixed},
where the degree of ionization can be given by hand.
Here the normalization of the entropy is the same as for
\code{IONIZATION=ionization}.
This case is described in more detail below.\footnote{We omit here
the contribution of H$_2$.}

We treat the gas as being composed of partially ionized hydrogen and neutral
helium. These are four different particle species, each of which regarded as
a perfect gas.

The ionization fraction $y$, which gives the ratio of ionized hydrogen to the
total amount of hydrogen $n_{\rm H}$, is obtained from the Saha equation
which, in this case, may be written as
\begin{equation}
\frac{y^2}{1-y}=\frac{1}{n_{\rm H}}
\left(\frac{m_{\rm e}k_{\rm B}T}{2\pi\hbar^2}\right)^{3/2}
\exp\left(-\frac{\chi_{\rm H}}{k_{\rm B}T}\right)\ .
\end{equation}

The temperature $T$ cannot be obtained directly from the Pencil Code's
independent variables $\ln\rho$ and $s$, but is itself dependent on $y$.
Hence, the calculation of $y$ essentially becomes a root finding problem.

The entropy of a perfect gas consisting of particles of type $i$ is known from
the Sackur-Tetrode equation
\begin{equation}
S_i=k_{\rm B}N_i\left(\ln\left[\frac{1}{n_{\rm tot}}
                               \left(\frac{m_ik_{\rm B}T}
                                     {2\pi\hbar^2}\right)^{3/2}\right]
                      +\frac{5}{2}\right)\ .
\end{equation}

Here $N_i$ is the number of particles of a single species and $n_{\rm tot}$
is the total number density of all particle species.

In addition to the individual entropies we also have to take the entropy of
mixing, \mbox{$S_{\rm mix}=-N_{\rm tot}k_{\rm B}\sum_ip_i\ln p_i$}, into
account. Summing up everything, we can get a closed expression for the
specific entropy $s$ in terms of $y$, $\ln\rho$ and $T$, which may be solved
for $T$.

% ---------------------------------------------------------------------- %
\begin{figure}[htb]
  \centering
  \includegraphics%
    [width=.6\textwidth,keepaspectratio]%
    {pTTss}
  \caption{Dependence of temperature on entropy for different values of
     the density.}
  \label{pTTss}
\end{figure}
% ---------------------------------------------------------------------- %

For given $\ln\rho$ and $s$ we are then able to calculate the ionization
fraction $y$ by finding the root of

\begin{equation}
f(y)=\ln\left[\frac{1-y}{y^2}\frac{1}{n_{\rm H}}
              \left(\frac{m_{\rm e}k_{\rm B}T(y)}
                    {2\pi\hbar^2}\right)^{3/2}\right]
     -\frac{\chi_{\rm H}}{k_{\rm B}T(y)}\ .
\end{equation}

In the ionized case, several thermodynamic quantities of the gas become
dependent on the ionization fraction $y$ such as its pressure,
$P\!=(1\!+y+x_{\rm He})n_{\rm H}k_{\rm B}T$, and its internal energy,
$E=\frac{3}{2}(1+y+x_{\rm He})n_{\rm H}k_{\rm B}T+y\chi_{\rm H}$, where
$x_{\rm He}$ gives the ratio of neutral helium to the total amount of hydrogen.
The dependence of temperature on entropy is shown in Fig.~\ref{pTTss}
for different values of the density.

For further details regarding the procedure of solving for the entropy
see Sect.~\ref{S-Ioni} in the appendix.

% ---------------------------------------------------------------------- %

\subsection{Radiative transfer}

This module (\file{radiation-exp.f90} or \file{radiation-ray.f90}) is
still quite experimental (as of June 2003).

The basic equation for radiative transfer is
\begin{equation} \label{radiative-transfer}
  \frac{dI}{d\tau} = -I +S \; ,
\end{equation}
where
\begin{equation}
  \tau \equiv \int\limits_0^s \chi(s') \, ds'
\end{equation}
is the optical depth ($s$ is the geometrical coordinate along the ray).

% ---------------------------------------------------------------------- %

\subsection{Self-gravity}

The Pencil Code can consider the self-gravity of the fluid in the simulation
box by adding the term
\begin{equation}
  \frac{\partial \vec{u}}{\partial t}
      = \ldots - \nab \varPhi_{\rm self}
\end{equation}
to the equation of motion. The self-potential $\varPhi_{\rm self}$ (or just
$\varPhi$ for simplicity) satisfies Poisson's equation
\begin{equation}
  \nabla^2 \varPhi = 4 \pi G \rho \, .
\end{equation}
The solution for a single Fourier component at scale $\vec{k}$ is
\begin{equation}
  \varPhi_\vec{k} = -\frac{4 \pi G \rho_\vec{k}}{k^2} \, .
\end{equation}
Here we have assumed periodic boundary conditions. The potential is obtained by
Fourier-transforming the density, then finding the corresponding potential at
that scale, and finally Fourier-transforming back to real space.

The $x$-direction in the shearing sheet is not strictly periodic, but is rather
shear periodic with two connected points at the inner and outer boundary
separated by the distance $\Delta y(t)={\rm mod}[(3/2) \varOmega_0 L_x t,L_y]$
in the $y$-direction. We follow here the method from \cite{Gammie2001} to allow
for shear-periodic boundaries in the Fourier method for self-gravity. First we
take the Fourier transform along the periodic $y$-direction. We then shift
the entire $y$-direction by the amount $\delta y(x)=\Delta y(t) x/L_x$ to make
the $x$-direction periodic. Then we proceed with Fourier transforms along $x$
and then $z$. After solving the Poisson equation in Fourier space, we transform
back to real space in the opposite order. We differ here from the method by
\cite{Gammie2001} in that we shift in Fourier space rather than in real
space\footnote{We were kindly made aware of the possibility of interpolating in
Fourier space by C.\ McNally on his website.}. The Fourier interpolation formula
has the advantage over polynomial interpolation in that it is continuous and
smooth in all its derivatives.

% ---------------------------------------------------------------------- %

\subsection{Dust equations}

The code treats gas and dust as two separate fluids\footnote{See master's
thesis of A. Johansen (can be downloaded from\\
\url{http://www.mpia.de/homes/johansen/research_en.php})}. The dust and the gas
interact through a drag force. This force can most generally be written as an
additional term to the equation of motion as
\begin{equation}
  \frac{\De \vec{u}_{\rm d}}{\De t} = \ldots - \frac{1}{\tau_{\rm s}} 
  \left( \vec{u}_{\rm d} - \vec{u} \right)  \, .
\end{equation}
Here $\tau_{\rm s}$ is the so-called stopping time of the considered dust
species. This measures the coupling strength between dust and gas. In the
Epstein drag regime
\begin{equation}
  \tau_{\rm s} = \frac{a_{\rm d} \rho_{\rm s}}{c_{\rm s} \rho} \, ,
\end{equation}
where $a_{\rm d}$ is the radius of the dust grain and $\rho_{\rm s}$ is the
solid density of the dust grain.
\\ \\
Two other important effects work on the dust. The first is coagulation
controlled by the discrete coagulation equation
\begin{equation}
  \frac{\de n_k}{\de t} = \frac{1}{2} \sum_{i+j=k} A_{ij} n_i n_j
  - n_k \sum_{i=1}^\infty A_{ik} n_i \, .
\end{equation}
In the code $N$ discrete dust species are considered. Also the bins are
logarithmically spaced in order to give better mass resolution. It is also
possible to keep track of both number density and mass density of each bin,
corresponding to having a variable grain mass in each bin.
\\ \\
Dust condensation is controlled by the equation
\begin{equation}
  \frac{\de N}{\de t} = \frac{1}{\tau_{\rm cond}} N^{\frac{d-1}{d}} \, .
\end{equation}
Here $N$ is the number of monomers in the dust grain (such as water molecules)
and $d$ is the physical dimension of the dust grain. The condensation time
$\tau_{\rm cond}$ is calculated from
\begin{equation}
  \frac{1}{\tau_{\rm cond}} = A_1 v_{\rm th} \alpha n_{\rm mon}
  \left\{ 1-\frac{1}{S_{\rm mon}} \right\} \, , 
\end{equation}
where $A_1$ is the surface area of a monomer, $\alpha$ is the condensation
efficiency, $n_{\rm mon}$ is the number density of monomers in the gas and
$S_{\rm mon}$ is the saturation level of the monomer given by
\begin{equation}
  S_{\rm mon} = \frac{P_{\rm mon}}{P_{\rm sat}} \, .
\end{equation}
Here $P_{\rm sat}$ is the saturated vapor pressure of the monomer. Currently
only water ice has been implemented in the code.
\\ \\
All dust species fulfill the continuity equation
\begin{equation}
  {\partial\rho_{\rm d}\over\partial t}+\nab\cdot(\rho_{\rm d}\uv_{\rm d})=0.
\end{equation} 
%\begin{equation}
%\rho\left[{\partial\uv\over\partial t}+\uv\cdot\nab\uv-\fv(\uv)\right]
%=-\nab p+\Fv-\beta(\uv-\uv_{\rm d}),
%\end{equation}
%\begin{equation}
%\rho_{\rm d}\left[{\partial\uv_{\rm d}\over\partial t}+\uv_{\rm d}\cdot\nab\uv_{\rm d}
%-\fv(\uv_{\rm d})\right]
%=\Fv_{\rm d}-\beta(\uv_{\rm d}-\uv),
%\end{equation}
%\begin{equation}
%{\partial\rho\over\partial t}+\nab\cdot(\rho\uv)=0,
%\end{equation}
%\begin{equation}
%{\partial\rho_{\rm d}\over\partial t}+\nab\cdot(\rho_{\rm d}\uv_{\rm d})=0,
%\end{equation} 
%where $\fv(\uv)$ denotes the \name{Coriolis force} and $\Fv$ and $\Fv_{\rm d}$
%other forces.

% ---------------------------------------------------------------------- %

\subsection{Cosmic ray pressure in diffusion approximation}
\index{Cosmic rays}

Cosmic rays are treated in the diffusion approximation.
The equation of state is $p_{\rm c}=(\gamma_{\rm c})e_{\rm c}$
where the value of $\gamma_{\rm c}$ is usually somewhere between
$14/9$ and $4/3$.
In the momentum equation (\ref{DuDt}) the cosmic ray pressure force,
$-\varrho^{-1}\nab p_{\rm c}$ is added on the right hand side, and
$e_{\rm c}$ satisfies the evolution equation
\begin{equation}
{\partial e_{\rm c}\over\partial t}+\nab\cdot(e_{\rm c}\uv)
+p_{\rm c}\nab\cdot\uv=\partial_i(K_{ij}\partial_j e_{\rm c})+Q_{\rm c},
\label{CReqn}
\end{equation}
where $Q_{\rm c}$ is a source term and
\begin{equation}
K_{ij}=K_\perp\delta_{ij}+(K_\parallel-K_\perp)\Bhat_i\Bhat_j
\label{Difftensor}
\end{equation}
is an anisotropic diffusivity tensor.

In the non-conservative formulation of this code it is advantageous
to expand the diffusion term using the product rule, i.e.\
\begin{equation}
\partial_i(K_{ij}\partial_j e_{\rm c})
=-\Uv_{\rm c}\cdot\nab e_{\rm c}+K_{ij}\partial_i\partial_j e_{\rm c}.
\end{equation}
where $U_{{\rm c}\,i}=-\partial K_{ij}/\partial x_j$ acts like an extra
velocity trying to straighten magnetic field lines.
We can write this term also as
$\Uv_{\rm c}=-(K_\parallel-K_\perp)\nab\cdot(\BBhat\BBhat)$,
where the last term is a divergence of the dyadic product of unit
vectors.\footnote{In practice, we calculate $\partial_j(\Bhat_i\Bhat_j)
=(\delta_{ij}-2\Bhat_i\Bhat_k)\Bhat_j B_{k,j}/|\Bv|$, where
derivatives of $\Bv$ are calculated as $B_{i,j}=\epsilon_{ikl}A_{l,jk}$.}
However, near magnetic nulls, this term can becomes infinite.
In order to avoid this problem we are forced to limit
$\nab\cdot(\BBhat\BBhat)$, and hence $|\Uv_{\rm c}|$,
to the maximum possible value that can be resolved at a given
resolution.

A physically appealing way of limiting the maximum propagation
speed is to restore an explicit time dependence in the equation for the
cosmic ray flux, and to replace the diffusion term in \Eq{CReqn} by
a divergence of a flux that in turn obeys the equation
\begin{equation}
{\partial{\cal F}_{{\rm c}i}\over\partial t}=-\tilde{K}_{ij}\nabla_je_{\rm c}
-{{\cal F}_{{\rm c}i}\over\tau}
\quad\mbox{(non-Fickian diffusion)},
\label{nonFickian}
\end{equation}
where $K_{ij}=\tau\tilde{K}_{ij}$ would be the original diffusion tensor
of \Eq{Difftensor}, if the time derivative were negligible.
Further details are described in Snodin et al.\ (2005).


\subsection{Particles}
\label{S-particles-equations}
\index{Particles}

Particles are entities that each have a space coordinate and a velocity vector,
where a fluid only has a velocity vector field (the continuity equation of a
fluid in some way corresponds to the space coordinate of particles). In the
code particles are present either as tracer particles or as dust particles

\subsubsection{Tracer particles}

Tracer particles always have the local velocity of the gas. The dynamical
equations are thus
\begin{equation}
  \frac{\partial \vec{x}_i}{\partial t} = \vec{u} \, ,
\end{equation}
where the index $i$ runs over all particles. Here $\vec{u}$ is the gas velocity
at the position of the particle. One can choose between a first order (default)
and a second order spline interpolation scheme (set
\code{lquadratic_interpolation=T} in \code{\&particles_init_pars}) to
calculate the gas velocity at the position of a tracer particle.

The sample run \file{samples/dust-vortex} contains the latest setup for
tracer particles.

\subsubsection{Dust particles}

Dust particles are allowed to have a velocity that is not similar to the gas,
\begin{equation}
  \frac{\dd \vec{x}_i}{\dd t} = \vec{v}_i \, .
\end{equation}
The particle velocity follows an equation of motion similar to a fluid, only
there is no advection term. Dust particles also experience a drag force from
the gas (proportional to the velocity difference between a particle and the
gas).
\begin{equation}
  \frac{\dd \vec{v}_i}{\dd t} = \ldots -\frac{1}{\tau_{\rm s}}
  (\vec{v}_i-\vec{u}) \, .
\end{equation}
Here $\tau_{\rm s}$ is the stopping time of the dust particle. The
interpolation of the gas velocity to the position of a particle is done using
one of three possible particle-mesh schemes,
\begin{itemize}
  \item NGP (Nearest Grid Point, default)\\
    The gas velocity at the nearest grid point is used.
  \item CIC (Cloud in Cell, set \code{lparticlemesh_cic=T})\\
    A first order interpolation is used to obtain the gas velocity field at
    the position of a particle. Affects 8 grid points.
  \item TSC (Triangular Shaped Cloud, set \code{lparticlemesh_tsc=T})\\
    A second order spline interpolation is used to obtain the gas velocity
    field at the position of a particle. Affects 27 grid points.
\end{itemize}
The particle description is the proper description of
dust grains, since they do not feel any pressure forces (too low number
density). Thus there is no guarantee that the grains present within a given
volume will be equilibrated with each other, although drag force may work for
small grains to achieve that. Larger grains (meter-sized in protoplanetary
discs) must be treated as individual particles.

To conserve momentum the dust particles must affect the gas with a friction
force as well. The strength of this force depends on the dust-to-gas ratio
$\epsilon_{\rm d}$, and it can be safely ignored when there is much more gas
than there is dust, e.g.\ when $\epsilon_{\rm d}=0.01$. The friction force on
the gas appears in the equation of motion as
\begin{equation}
  \frac{\partial \vec{u}}{\partial t} = \ldots - \frac{\rho_{\rm p}^{(i)}}{\rho}
  \left( \frac{\partial \vec{v}^{(i)}}{\partial t} \right)_{\rm drag}
\end{equation}
Here $\rho_{\rm p}^{(i)}$ is the dust density that particle $i$ represents.
This can be set through the parameter \code{eps_todt} in
\code{\&particle_init_pars}. The drag force is assigned from the particles onto
the mesh using either NGP, CIC or TSC assignment. The same scheme is used both
for interpolation and for assignment to avoid any risk of a particle
accelerating itself (see Hockney \& Eastwood 1981).

% ====================================================================== %

\section{Troubleshooting / Frequently Asked Questions}
\label{FAQ}
\index{FAQ}\index{Frequently Asked Questions}

\subsection{Setup}

\subsubsection[Shell gives error message when sourcing
               {\file[sourceme]{sourceme.X}}]%
              {When sourcing the \file{sourceme.sh}/\file{sourceme.csh}
  file or running \cmd{setup-src}, I get error messages from the shell,
  like `if: Expression Syntax.'
  or `set: Variable name must begin with a letter.'}

\medskip

{\em

  This sounds like a buggy shell setup, either by yourself or your system
  administrator --- or a shell that is even more idiosyncratic than the ones
  we have been working with.

  To better diagnose the problem, collect the following information before
  filing a bug report to us:

  \begin{enumerate}

  \item \code{uname -a}

  \item \code{/bin/csh -v}

  \item \code{echo \$version}

  \item \code{echo \$SHELL}

  \item \code{ps -p \$\$}

  \item If you have problems while sourcing the \file{sourceme} script,
    \begin{enumerate}
    \item unset the \env{PENCIL_HOME} variable:
      \begin{description}
      \item[for \name{csh} and similar:]
        \code{unsetenv PENCIL_HOME}
      \item[for \name{bash} and similar:]
        \code{unexport PENCIL_HOME; unset PENCIL_HOME}
      \end{description}
    \item switch your shell in verbose mode,
      \begin{description}
      \item[for \name{csh} and similar:]
        \code{set verbose; set echo}
      \item[for \name{bash} and similar:]
        \code{set -v; set -x}
      \item then source again.
      \end{description}
    \end{enumerate}

  \item If you have problems with \cmd{setup-src},
    run it with \cmd{csh} in verbose mode:
    \begin{alltt}
      \code{/bin/csh -v -x \$PENCIL_HOME/bin/setup-src}
    \end{alltt}
  \end{enumerate}
}


% ---------------------------------------------------------------------- %

\subsection{Compilation}

%%%
\subsubsection[Compiling with \name{ifc} under Linux]%
              {How do I compile the Pencil Code with  the Intel
  (\name{ifc}) compiler under \name{Linux}?}

\medskip

{\em
  The Pencil Code should compile successfully with \name{ifc}~6.x,
  \name{ifc}~7.0, sufficiently recent versions of \name{ifc} 7.1 (you
  should get the latest version; if your's is too old, you will typically
  get an `internal compiler error' during compilation of
  \file[hydro.f90]{src/hydro.f90}), as well as with recent versions of
  \name{ifort} 8.1 (8.0 may also work).

  You can find the \name{ifort} compiler at
  \url{ftp://download.intel.com/software/products/compilers/downloads}.

  On many current (as of November 2003) Linux systems, there is a mismatch
  between the \name{glibc} versions used by the compiler and the linker.
  To work around this, use the following flag for compiling
  \begin{Verbatim}
    FC=ifc -i_dynamic
  \end{Verbatim}
  and set the environment variable
  \begin{Verbatim}
    LD_ASSUME_KERNEL=2.4.1; export LD_ASSUME_KERNEL
  \end{Verbatim}
  or
  \begin{Verbatim}
    setenv LD_ASSUME_KERNEL 2.4.1
  \end{Verbatim}
  This has solved the problems e.g.~on a system with \emph{glibc-2.3.2}
  and kernel \emph{2.4.22}.

  Thanks to Leonardo J. Milano (\url{http://udel.edu/~lmilano/}) for part of
  this info.
}

%%%
\subsubsection[Segmentation fault with \name{ifort} 8.0 under Linux]%
              {I keep getting segmentation faults with \file{start.x} when
                compiling with \name{ifort} 8.0}

\medskip

{\em
  There was/is a number of issues with \name{ifort} 8.0.
  Make sure you have the latest patches applied to the compiler.
  A number of things to consider or try are:
  \begin{enumerate}
  \item Compile with the the \option[-nothreads]{-static -nothreads} flags.
  \item Set your stacksize to a large value (but a far too large value may
    be problematic, too), e.\,g.
    \begin{Verbatim}
  limit stacksize 256m
  ulimit -s 256000
    \end{Verbatim}
  \item Set the environment variable KMP_STACKSIZE to a large value (like
    \code{100M})
  \end{enumerate}

See also \url{http://softwareforums.intel.com/ids/board/message?board.id=11&message.id=1375}

}

%%%
\subsubsection[The underscore problem: linking with \name{MPI}]%
              {When compiling with MPI on a Linux system, the linker
               complains:}
\label{Sec-underscore-problem}
\begin{Verbatim}
  mpicomm.o: In function `mpicomm_mpicomm_init_':
  mpicomm.o(.text+0x36): undefined reference to `mpi_init_'
  mpicomm.o(.text+0x55): undefined reference to `mpi_comm_size_'
  mpicomm.o(.text+0x6f): undefined reference to `mpi_comm_rank_'
  [...]
\end{Verbatim}

\medskip

{\em
  This is the infamous \name{underscore problem}.
  Your \name{MPI} libraries have been compiled with \name{G77} without the
  option \option{-fno-second-underscore}, which makes the \name{MPI}
  symbol names incompatible with other Fortran compilers.

  As a workaround, use
  \begin{Verbatim}
  MPICOMM = mpicomm_
  \end{Verbatim}
  in \file{Makefile.local}.
  Or, even better, you can set this globally (for the given computer) by
  inserting  that line into the file
  \file[.adapt-mkfile.inc]{\~{}/.adapt-mkfile.inc}
  (see \code{perldoc adapt-mkfile} for more details).
  \index{adapt-mkfile}
}


%%%
\subsubsection{Compilation stops with the cryptic error message:}
\begin{Verbatim}
  f95  -O3 -u -c .f90.f90
  Error : Could not open sourcefile .f90.f90
  compilation aborted for .f90.f90 (code 1)
  make[1]: *** [.f90.o] Error 1
\end{Verbatim}
What is the problem?

\medskip

{\em
  There are two possibilities:
  \begin{enumerate}
  \item One of the variables for \name{make} has not been set, so
    \name{make} expands it to the empty string.
    Most probably you forgot to specify a module in
    \file[Makefile.local]{src/Makefile.local}.
    One possibility is that you have upgraded
    from an older version of the code that did not have some of the modules
    the new version has.
  
    Compare your \file[Makefile.local]{src/Makefile.local} to one of the
    examples that work.

  \item One of the variables for \name{make} has a space appended to it,
    e.\,g.~if you use the line
    \begin{quote}
      \upshape\ttfamily
      MPICOMM = mpicomm_\textvisiblespace
    \end{quote}
    (see \S~\ref{Sec-underscore-problem}) with a trailing blank, you will
    encounter this error message.
    Remove the blank.
    This problem can also occur if you added a new
    module (and have an empty space after the module
    name in \file[Makefile.src]{src/Makefile.src}, i.e.\
    \cmd{CHIRAL=nochiral\textvisiblespace}), in which case the compiler
    will talk about ``circular dependence'' for the file \file{nochiral}.

  \end{enumerate}
}


%%%
\subsubsection{The code doesn't compile,}
\ldots there is a problem with \var{mvar}:
\begin{Verbatim}
  make start.x run.x
  f95 -O4 -u   -c cdata.f90
  Error: cdata.f90, line 71: Implicit type for MVAR
         detected at MVAR@)
  [f95 terminated - errors found by pass 1]
  make[1]: *** [cdata.o] Error 2
\end{Verbatim}

\medskip

{\em
  Check and make sure that \file{mkcparam} (directory
  \file[bin/]{\$PENCIL_HOME/bin}) is in your path.
  If this doesn't help, there may be an {\it empty} \file{cparam.inc}
  file in your \file[src/]{src} directory.
  Remove \file{cparam.inc} and try again
  (Note that \file{cparam.inc} is automatically generated from the
  \file{Makefile}).
}

%%%
\subsubsection{Some samples don't even compile,}
as you can see on the web,
\url{http://www.nordita.dk/software/pencil-code/tests.html}.
\begin{Verbatim}
samples/helical-MHDturb:
    Compiling..           not ok:
  make start.x run.x read_videofiles.x
make[1]: Entering directory `/home/dobler/f90/pencil-code/samples/helical-MHDturb/src'
/usr/lib/lam/bin/mpif95  -O3   -c initcond.f90
/usr/lib/lam/bin/mpif95  -O3   -c density.f90
      use Gravity, only: gravz, nu_epicycle
                                ^
Error 208 at (467:density.f90) : No such entity in the module
Error 355 : In procedure INIT_LNRHO variable NU_EPICYCLE has not been given a type
Error 355 : In procedure POLYTROPIC_LNRHO_DISC variable NU_EPICYCLE has not been given a type
3 Errors
compilation aborted for density.f90 (code 1)
make[1]: *** [density.o] Error 1
make[1]: Leaving directory `/home/dobler/f90/pencil-code/samples/helical-MHDturb/src'
make: *** [code] Error 2
\end{Verbatim}

\medskip

{\em
  Somebody may have checked in something without having
  run auto-test beforehand. The problem here is that something has been added in
  one module, but not in the corresponding no-module. You can of course check
  with \name{CVS} who it was\ldots
}

%%%
\subsubsection{Internal compiler error with Compaq/Dec F90}
The Dec Fortran optimizer has occasional problems with
\file{nompicomm.f90}:
\begin{Verbatim}
  make start.x run.x read_videofiles.x
  f90  -fast -O5 -tune ev6 -arch ev6  -c cparam.f90
  [...]
  f90  -fast -O5 -tune ev6 -arch ev6  -c nompicomm.f90
  otal vm  2755568      otal vm  2765296        otal vm  2775024        
  otal vm  2784752      otal...
  Assertion failure:  Compiler internal error - please submit problem r...
    GEM ASSERTION, Compiler internal error - please submit problem report
  Fatal error in: /usr/lib/cmplrs/fort90_540/decfort90 Terminated 
  *** Exit 3
  Stop.
  *** Exit 1
  Stop.
\end{Verbatim}

\medskip

{\em
  The occurrence of this problem depends upon the grid size;
  and the problem never seems to occur with \file{mpicomm.f90},
  except when \code{ncpus=1}.
  The problem can be avoided by switching off the loop transformation 
  optimization (part of the \option{-O5} optimization), via:
  \begin{Verbatim}
    #OPTFLAGS=-fast -O5 -notransform_loops
  \end{Verbatim}
%   This has not been made standard in \file{Makefile}, 
%   as it can cause some slowdown: e.g.
%   an increase from 2.67 to 2.89 ${\mu\rm s}\,/\,{\rm pt}\,/\,{\rm step}$,
%   for \file{runs/forced/hel128f} 
%   with \code{nxgrid=nygrid=nzgrid=128}, \code{ncpus=16}, \code{nt=100},
%   on the Mhd machine.
  This is currently the default compiler setting in \file{Makefile},
  although it has a measurable performance impact (some 8\% slowdown).
}


%%%
\subsubsection{Assertion failure under SunOS}
Under SunOS, I get an error message like
\begin{Verbatim}
  user@sun> f90 -c param_io.f90
  Assertion failed: at_handle_table[at_idx].tag == VAR_TAG,
                    file ../srcfw/FWcvrt.c, line 4018
  f90: Fatal error in f90comp: Abort
\end{Verbatim}

\medskip

{\em
  This is a compiler bug that we find at least with Sun's
  WorkShop Compiler version `5.0 00/05/17 FORTRAN 90 2.0 Patch 107356-05'.
  Upgrade the compiler version (and possibly also the operating system):
  we find that the code compiles and works with version
  `Sun WorkShop 6 update 2 Fortran 95 6.2 Patch 111690-05 2002/01/17' under
  SunOS version `5.8 Generic_108528-11'.
}

% ---------------------------------------------------------------------- %

\subsection{Running}

%%%
\subsubsection[Periodic boundary conditions in \file{start.x}]%
              {Why does \file{start.x} / \file{start.csh} write data with
  periodic boundary conditions?}

\medskip

{\em

  Because you are setting the boundary conditions in \file{run.in}, not in
  \file{start.in}; see Sect.~\ref{S-boundconds-where}.
  There is nothing wrong with the initial data --- the ghost-zone values
  will be re-calculated during the very first time step.
}

%%%
\subsubsection{\file{run.csh} doesn't work:}
\begin{Verbatim}
  Invalid character ''' in NAMELIST input
  Program terminated by fatal I/O error
  Abort
\end{Verbatim}

{\em
  The string array for the boundary condition, e.g.\ \var{bcx} or
  \var{bcz} is too long. Make sure it has exactly as many elements
  as \var{nvar} is big.
}

%%%
\subsubsection{Namelist problem under IRIX}
Under IRIX, I get
\begin{Verbatim}
  lib-4001 : UNRECOVERABLE library error 
  
  Encountered during a namelist READ from unit 1
  Fortran unit 1 is connected to a sequential formatted text file: "run.in"
  IOT Trap
  Abort
\end{Verbatim}

\medskip

{\em
  This is a compiler bug that has been found at least with the MIPSpro F90
  compiler version 7.3.1.3m. The problem seems to have been fixed in  version
  7.4.20m. 

%  It has been reported to SGI in mid-2002; you should complain to SGI if a
%  patch is still not available.

  The error comes and goes, depending on the configuration (and possibly
  even the input parameters) you are using.
  Until SGI fix their compiler, you can experiment with adding new variables
  to the module \name{Param_IO}; this has solved the problem once for us.
  If this trick does not help,
  you will need to turn your namelist input (at least
  \file{run.in} into Fortran statements, include them into a replacement
  version of \file{param_io.f90}, and recompile each time you make changes.
}

%%%
\subsubsection{Code crashes after restarting}

\begin{Verbatim}
>  > removing mu_r from the namelist just `like that' makes the code
>  > backwards incompatible.
>
> That means that we can never get rid of a parameter in start.in once we
> have introduced it, right?
\end{Verbatim}

\medskip

{\em

  In the current implementation, without a corresponding cleaning procedure,
  unfortunately yes.

  Of course, this does not affect users' private changes outside the
  central CVS tree.

%   My experience of some 5 days ago was quite enormous. It started
%   with noticing that remesh.f90 doesn't work correctly. I thought:- ok,
%   there must be a problem with remesh.f90, because nobody remembered having
%   tested this for nonuniform mesh-aspect ratios. (So I told Nils that his
%   remesh routine needs reworking.)

%   Later that day I noticed that several of my old jobs started crashing,
%   so, of course, I thought there must be yet another mistake somewhere.
%   Most incredible was that I lost places in 64 and 128 proc queues,
%   even though I have a fairly save backup procedure in place that if a job
%   crashes, it goes on another job. Because the job transfer times can be
%   on the order of 2h, because of slow writing, and because we had detected
%   some days earlier yet another timeout problem with job transfers of 64
%   and 128 proc jobs, we thought the problem was connected with that.

%   Only later that day I noticed that what the real problem was.

%   It was rather bad that this was not predicted by any of the auto-tests,
%   so I wrote the backwards-compatible, which detects those kind of errors.

%   This was of course another wild experience. When I came back from
%   swimming, "nothing" was working, and since I had done some minor changes,
%   I thought I was to blame. Then I found the problem with init conditions,
%   but this solved only one out of 3 problems. Then I found the mu\_r problem,
%   but this solved only the 2nd problem. Finally, I found and solved the
%   third problem.

%   So, those people who checked in changes where already working with buggy
%   codes and checked on even further bugs. As I said, I was worried that
%   it was all my fault, but buggy codes are potentially bad for others too.
}

\subsubsection{auto-test gone mad...?}

{\bf Q}: Have you ever seen this before:
\begin{Verbatim}
  giga01:/home/pg/n7026413/cvs-src/pencil-code/samples/conv-slab> auto-test
  .

  /home/pg/n7026413/cvs-src/pencil-code/samples/conv-slab:
      Compiling..           ok
          No data directory; generating data -> /var/tmp/pencil-tmp-25318
      Starting..            ok
      Running..             ok
      Validating results..Malformed UTF-8 character (unexpected continuation
  byte 0x80, with no preceding start byte) in split at
  /home/pg/n7026413/cvs-src/pencil-code/bin/auto-test line 263.
  Malformed UTF-8 character (unexpected continuation byte 0x80, with no
  preceding start byte) in split at
  /home/pg/n7026413/cvs-src/pencil-code/bin/auto-test line 263.
\end{Verbatim}

{\bf A}:
{\em
  You are running on a RedHat 8 or 9 system, right?

  Set \cmd{LANG=POSIX} in your shell's startup script and life will be
  much better.
}

% ---------------------------------------------------------------------- %

\subsection{Visualization}

%%%
\subsubsection{\file{start.pro} doesn't work:}
\begin{Verbatim}
  Reading grid.dat..
  Reading param.nml..
  \% Expression must be a structure in this context: PAR.
  \% Execution halted at:  \$MAIN\$            104
  /home/brandenb/pencil-code/runs/forced/hel1/../../../idl/start.pro
\end{Verbatim}

\medskip

{\em
  You don't have the subdirectory \file[data/]{data} in your IDL variable
  \var{!path}.
  Make sure you source \file{sourceme.csh}/\file{sourceme.sh}
  or set a sufficient IDL path otherwise.
}

%%%
\subsubsection{\file{start.pro} doesn't work:}

Isn't there some clever (or even trivial) way that one can avoid the 
annoying error messages that one gets, when running e.g. ".r rall" after
a new variable has been introduced in "idl/varcontent.pro". Ever so
often there's a new variable that can't be found in my param2.nml --
this time is was IECR, IGG, and ILNTT that I had to circumvent\ldots

\medskip
\index{NOERASE file}

{\em
  The simplest solution is to invoke \file{NOERASE}, i.e.\ say
  \begin{Verbatim}
  touch NOERASE
  start.csh
  \end{Verbatim}
  or, alternatively, \code{start_run.csh}.
  What it does is that it reruns \code{src/start.x} with a new version of
  the code; this then produces all the necessary auxiliary files, but it
  doesn't overwrite or erase the \file{var.dat} and other \file{VAR} and
  \file{slice} files.
}

%%%
\subsubsection{Something about tag name undefined:}

{\bf Q}:
In one of my older run directories I can't read the data
with idl anymore. What should I do? Is says something like
\begin{Verbatim}
Reading param.nml..
% Tag name LEQUIDIST is undefined for structure <Anonymous>.
% Execution halted at: $MAIN$            182
  /people/disk2/brandenb/pencil-code/idl/start.pro
\end{Verbatim}

\medskip

{\bf A}:
{\em
  Go into \file{data/param.nml}
  and add \code{, LEQUIDIST=T} anywhere in the file (but before the
  last slash).
}

%%%
\subsubsection{Something INC in start.pro}

{\bf Q}:
start doesn't even work:
\begin{Verbatim}
% Compiled module: $MAIN$.
nname=      11
Reading grid.dat..
Reading param.nml..
Can't locate Namelist.pm in INC (INC contains: /etc/perl /usr/local/lib/perl/5.8.4 /usr/local/share/perl/5.8.4 /usr/lib/perl5 /usr/share/perl5 /usr/lib/perl/5.8 /usr/share/perl/5.8 /usr/local/lib/site_perl .) at /home/brandenb/pencil-code/bin/nl2idl line 49.
BEGIN failed--compilation aborted at /home/brandenb/pencil-code/bin/nl2idl line 49.
\end{Verbatim}

\medskip

{\bf A}:
{\em
  Go into \file{\$PENCIL_HOME} and say \cmd{cvs up source.csh}
  and/or \cmd{cvs up source.sh}. (They were just out of date.)
}


\subsubsection{nl2idl problem when reading param2.nml}

{\bf Q}:
Does anybody encounter a backward problem with nl2idl? The file
param*.nml files are checked in under
\file[pencil-code/]{pencil-code/axel/couette/SStrat128a_mu0.20_g2}
and the problem is below.
\begin{Verbatim}
 at /people/disk2/brandenb/pencil-code/bin/nl2idl line 120
HCOND0= 0.0,HCOND1= 1.000000,HCOND2= 1.000000,WIDTHSS= 1.192093E-06,MPOLY0=
^------  HERE
 at /people/disk2/brandenb/pencil-code/bin/nl2idl line 120
\end{Verbatim}

\medskip

{\bf A}:
The problem is the stupid ifc compiler writing the following into the
namelist file:
\begin{Verbatim}
  COOLING_PROFILE='gaussian                 ',COOLTYPE='Temp
  'COOL= 0.0,CS2COOL= 0.0,RCOOL= 1.000000,WCOOL= 0.1000000,FBOT= 0.0,CHI_T= 0.0
\end{Verbatim}
{\em
If you add a comma after the closing quote:
}
\begin{Verbatim}
  COOLING_PROFILE='gaussian                 ',COOLTYPE='Temp
  ',COOL= 0.0,CS2COOL= 0.0,RCOOL= 1.000000,WCOOL= 0.1000000,FBOT= 0.0,CHI_T= 0.0
\end{Verbatim}
{\em
things will work.
}

Note that ifc cannot even itself read what it is writing here, so if this
happened to occur in param.nml, the code would require manual intervention
after each start.csh.


% ---------------------------------------------------------------------- %

\subsection{General questions}

%%%
\subsubsection{``Installation'' procedure}
Why don't you use GNU \name{autoconf/automake} for installation of the
Pencil Code?

\medskip

{\em
  What do you mean by ``installation''?
  Unlike the applications that normally use \name{autoconf}, the
  \name{Pencil Code} is neither a binary executable, nor a library that
  you compile once and then dump somewhere in the system tree.
  \name[autoconf]{Autoconf} is the right tool for these applications, but
  not for numerical codes, where the typical compilation and usage pattern
  is very different:

  You have different directories with different \file{Makefile.local}
  settings, recompile after introducing that shiny new term in your
  equations, etc.
  Moreover, you want to sometimes switch to a different compiler (but just
  for that run directory) or another \name{MPI} implementation.
  Our \cmd{adapt-mkfile} approach gives you this flexibility in a
  reasonably convenient way, while doing the same thing with
  \name{autoconf} would be using that system against most of its design
  principles.

  Besides, it would really get on my (WD's) nerves if I had to wait two
  minutes for \name{autoconf} to finish before I can start compiling
  (or maybe 5--10 minutes if I worked on a NEC machine\ldots).

  Finally, if you have ever tried to figure out what a \file{configure}
  script does, you will appreciate a comprehensible configuration system.
}

%%%
\subsubsection{Small numbers in the code}
 What is actually the difference between epsi, tini and tiny?

\medskip

\begin{Verbatim}
F90 has two functions epsilon() and tiny(), with

  epsilon(x) = 1.1920929e-07
  tiny(x)    = 1.1754944e-38
(and then there is huge(x) = 3.4028235e+38)
for a single-precision number x.

epsilon(x) is the smallest number that satisfies
  1+epsilon(1.) /= 1 ,
while tiny(x) is the smallest number that can be represented without
precision loss.

In the code we have variants hereof,
   epsi=5*epsilon(1.0)
   tini=5*tiny(1.0)
   huge1=0.2*huge(1.0)
that have added safety margins, so we don't have to think about doing
things like 1/tini.

So in sub.f90,
  -      evr = evr / spread(r_mn+epsi,2,3)
did (minimally) affect the result for r_mn=O(1), while the correct version
  +      evr = evr / spread(r_mn+tini,2,3)
only avoids overflow.
\end{Verbatim}

%%%
\subsubsection{Pencil code discussion forum}
  Do I just need to send an email somewhere to subscribe or what?

\medskip

The answer is yes; just go to:
\begin{Verbatim}
   http://lists.nordita.dk/mailman/listinfo/pencilcode-discuss
\end{Verbatim}

\cleardoublepage
% ====================================================================== %

\part{Programming the Pencil Code}
\index{Programming style}
\index{Style}

The {\sc Pencil Code} has expanded approximately linearly in the number of
lines of code and the number of subroutines (Fig.~\ref{pfile-size}).
The increase in the functionality of the code is documented by the
rise in the number of sample problems (Fig.~\ref{pauto-tests}).
In order to keep the development of the code going, it it important
that the users are able to understand and modify (program!) the code.
In this section we explain first how to orient yourself in the code
and to understand what is in it, and then to modify it according to
your needs.

% ---------------------------------------------------------------------- %
\begin{figure}[htb]
  \centering
  \includegraphics%
    [width=.6\textwidth,keepaspectratio]%
    {pfile-size}
  \caption{Number of lines of code and the number of subroutines
  since the end of 2001. The jump in the Summer of 2005 was the
  moment when the developments on the side branch (eos branch) were
  merged with the main trunk of the code. Note the approximately
  linear scaling with time.}
  \label{pfile-size}
\end{figure}
% ---------------------------------------------------------------------- %

% ---------------------------------------------------------------------- %
\begin{figure}[htb]
  \centering
  \includegraphics%
    [width=.6\textwidth,keepaspectratio]%
    {pauto-tests}
  \caption{Number of tests in the sample directory that are
  used in the nightly auto tests. Note again the approximately
  linear scaling with time.}
  \label{pauto-tests}
\end{figure}
% ---------------------------------------------------------------------- %

\section{Understanding the code}

Understanding the code means looking through the code.
This is not normally done by just printing out the entire code,
but by searching your way through the code in order to address
your questions.
The general concept will be illustrated here with an example.

\subsection{Example: how is the continuity equation being solved?}

All the physics modules are solved in the routine \code{pde}, which
is located in the file and module \file{Equ}.
Somewhere in the \code{pde} subroutine you find the line
\begin{verbatim}
        call dlnrho_dt(f,df,p)
\end{verbatim}
This means that here the part belonging to $\partial\ln\rho/\partial t$
is being assembled.
Using the \cmd{grep} command you will find that this routine is located
in the module \code{density}, so look in there and try to understand
the pieces in this routine.
We quickly arrive at the following crucial part of code,
\begin{verbatim}
!
!  continuity equation
!
      if (lcontinuity_gas) then
        if (ldensity_nolog) then
          df(l1:l2,m,n,ilnrho) = df(l1:l2,m,n,ilnrho) - p%ugrho - p%rho*p%divu
        else
          df(l1:l2,m,n,ilnrho) = df(l1:l2,m,n,ilnrho) - p%uglnrho - p%divu
        endif
      endif
\end{verbatim}
where, depending on some logicals that tell you whether the continuity
equation should indeed be solved and whether we do want to
solve for the logarithmic density and not the actual density,
the correct right hand side is being assembled.
Note that all these routines always only \emph{add} to the existing
\code{df(l1:l2,m,n,ilnrho)} array and never reset it.
Resetting \code{df} is only done by the timestepping routine.
Next, the pieces \code{p\%uglnrho} and \code{p\%divu} are being
subtracted.
These are \emph{pencils} that are organized in the \emph{structure}
with the name \code{p}.
The meaning of their names is obvious: \code{uglnrho} refers to
$\uv\cdot\nabla\ln\rho$ and \code{divu} refers to $\nabla\cdot\uv$.
In the subroutine \code{pencil_criteria_density} you find under which
conditions these pencils are requested.
Using \cmd{grep}, you also find where they are calculated.
For example \code{p\%uglnrho} is calculated in \file{density.f90}; see
\begin{verbatim}
          call u_dot_grad(f,ilnrho,p%glnrho,p%uu,p%uglnrho,UPWIND=lupw_lnrho)
\end{verbatim}
So this is a call to a subroutine that calculates the $\uv\cdot\nabla$ operator,
where there is the possibility of upwinding, but this is \emph{not}
the default.
The piece \code{divu} is calculated in \file{hydro.f90} in the line
\begin{verbatim}
!
!  calculate uij and divu, if requested
!
      if (lpencil(i_uij)) call gij(f,iuu,p%uij,1)
      if (lpencil(i_divu)) call div_mn(p%uij,p%divu,p%uu)
\end{verbatim}
Note that the divergence calculation uses the velocity gradient
matrix as input, so no new derivatives are recalculated.
Again, using \cmd{grep}, you will find that this calculation and
many other ones happen in the module and file \file{sub.f90}.
The various derivatives that enter here have been calculated using
the $\code{gij}$ routine, which calls the \code{der} routine, e.g., like so
\begin{verbatim}
      k1=k-1
      do i=1,3
        do j=1,3
          if (nder == 1) then
            call der(f,k1+i,tmp,j)
\end{verbatim}
For all further details you just have to follow the trail.
So if you want to know how the derivatives are calculated,
you have to look in \code{deriv.f90}, and only here is it
where the indices of the \code{f} array are being addressed.

If you are interested in magnetic fields, you have to look
in the file \file{magnetic.f90}. The right hand side of
the equation is assembled in the routine
\begin{verbatim}
!***********************************************************************
    subroutine daa_dt(f,df,p)
!
!  magnetic field evolution
!
!  calculate dA/dt=uxB+3/2 Omega_0 A_y x_dir -eta mu_0 J
!  for mean field calculations one can also add dA/dt=...+alpha*bb+delta*WXJ
!  add jxb/rho to momentum equation
!  add eta mu_0 j2/rho to entropy equation
!
\end{verbatim}
where the header tells you already a little bit of what comes below.
It is also here where ohmic heating effects and other possible effects
on other equations are included, e.g.\
\begin{verbatim}
!
!  add eta mu_0 j2/rho to entropy or temperature equation
!
      if (lentropy .and. lohmic_heat) then
        df(l1:l2,m,n,iss) = df(l1:l2,m,n,iss) &
                          + etatotal*mu0*p%j2*p%rho1*p%TT1
      endif
\end{verbatim}

We leave it at this and encourage the user to do similar
inspection work on a number of other examples.
If you think you find an error, file a ticket with the
\cmd{trac} bug tracker. You can of course also repair it!

\section{Adapting the code}
\subsection{The Pencil Code coding standard}
As with any code longer than a few lines the appearance and layout
of the source code is of the utmost importance.  Well layed out
code is more easy to read and understand and as such is less prone 
to errors.

A consistent coding style has evolved in the Pencil Code and we
ask that those contributing try to be consistent for everybody's 
benefit.  In particular, it would be appreciated if those committing
changes to the code via CVS submit code

There are not terribly many rules and using existing code as a template
is usually the easiest way to proceed.  In short the most important rules are:
\begin{itemize}
\item tab characters do not occur anywhere in the code (in fact the use of
tab character is an extension to the Fortran standard)
\item code in any delimited block, e.g. if statements, do loops, subroutines 
etc., is indented be precisely 2 spaces.
\item in general all comments are placed on their own lines with the '!' 
appearing in the first column. 
\item all subroutine/functions begin with a standard comment block describing
what they do, when and by whom they were created and when and by whom any
non-trivial modifications were made.
\item lines longer that 78 characters should be explicitly wrapped, unless
there is a block of longer lines that can only be read easily when they are
not wrapped.
\end{itemize}

These and other issues are discussed in more depth and with examples in 
Appendix~\ref{coding-standard}.

\subsection{Adding new output diagnostics}

With the implementation of new physics and the development of new procedures
it will become necessary to monitor new diagnostic quantities that
have not yet been implemented in the code.
In the following, we describe the steps necessary to set up a new
diagnostic variable.

This is nontrivial as, in order to keep latency effects low on
multi-processor machines, the code minimizes the number of global
reduction operations by assembling all quantities that need the maximum
taken in \var{fmax}, and those that need to be summed up over all
processors (mostly for calculating mean quantities) in \var{fsum} (see
subroutine \code{diagnostic} in file \file[equ.f90]{src/equ.f90}).

As a sample variable, let us consider \var{jbm} (the volume average
$\bigl<\jv\cdot\Bv\bigr>$).
Only the module \name{magnetic} will be affected, as you can see (the
diagnostic quantity \var{jbm} is already implemented) with
\begin{alltt}
  \prompt{unix> } grep -i jbm src/*.f90
\end{alltt}

If we pretend for the sake of the exercise that no trace of \var{jbm} was
in the code, and we were only now adding it, we would need to do the following
\begin{enumerate}
\item add the variable \var{idiag_jbm} to the \emph{module variables} of
  \name{Magnetic} in both \file{magnetic.f90} and \file{nomagnetic.f90}:
  \begin{alltt}
  integer :: idiag_jbm=0
  \end{alltt}
  The variable \var{idiag_jbm} is needed for matching the position of \var{jbm}
  with the list of diagnostic variables specified in \file{print.in}.
\item in the subroutine \code{daa_dt} in \file{magnetic.f90}, declare and
  calculate the quantity \var{jb} (the average of which will be
  \var{jbm}), and call \code{sum_mn_name}
  \begin{alltt}
  real, dimension (nx) :: jb  {\sl! jj\(\,\cdot\,\)BB}
  {\sl[\ldots]}
  if (ldiagnos) then          {\sl! only calculate if diagnostics is required}
    if (idiag_jbm/=0) then        {\sl! anybody asked for jbm?}
      call dot_mn(jj,bb,jb)   {\sl! assuming jj and bb are known}
      call sum_mn_name(jb,i_jbm)
    endif
  endif
  \end{alltt}
\item in the subroutine \code{rprint_magnetic} in both
  \file{magnetic.f90}, add the following:
  \begin{alltt}
  !
  !  reset everything in case of RELOAD
  !  (this needs to be consistent with what is defined above!)
  !
  if (lreset) then  {\sl! need to reset list of diagnostic variables?}
    {\sl[\ldots]}
    idiag_jbm=0
    {\sl[\ldots]}
  endif
  !
  !  check for those quantities that we want to evaluate online
  !
  do iname=1,nname
    {\sl[\ldots]}
    call parse_name(iname,cname(iname),cform(iname),'jbm',idiag_jbm)
    {\sl[\ldots]}
  enddo
  {\sl[\ldots]}
  !
  !  write column, i_XYZ, where our variable XYZ is stored
  !
  {\sl[\ldots]}
  write(3,*) 'i_jbm=',idiag_jbm
  {\sl[\ldots]}
  \end{alltt}
\item in the subroutine \code{rprint_magnetic} in \file{nomagnetic.f90}, add
  the following (newer versions of the code may not require this any more):
  \begin{alltt}
  !
  !  write column, i_jbm, where our variable jbm is stored
  !  idl needs this even if everything is zero
  !
  {\sl[\ldots]}
  write(3,*) 'i_jbm=',idiag_jbm
  {\sl[\ldots]}
  \end{alltt}
\item and don't forget to add your new variable to \file{print.in}:
  \begin{alltt}
  jbm(f10.5)
  \end{alltt}
\end{enumerate}

If, instead of a mean value, you want a new maximum quantity, you need to
replace \code{sum\_mn\_name()} by \code{max\_mn\_name()}.


% ---------------------------------------------------------------------- %

\subsubsection{Outputting new horizontal averages}
\label{S-new-hor-averages}

Currently, only the horizontally $xy$-averaged $x$ and $y$ components
of the magnetic and velocity fields can be written.
This is determined by the existence
and content of the file \file{xyaver.in}; a new line of averages is
written every time \file{var.dat} is updated (i.e.\ every \var{it1}th time
steps).
New such variables can be added by using the existing averaging
procedures as examples.

% ---------------------------------------------------------------------- %

\subsubsection{Other averages}
\label{S-other-averages}

There is the possibility to output also $xz$ and $yz$
averages. This is done by averaging first in the $z$-direction only, so
the result (which is available on the root processor) depends on $x$
and $y$. From these planes we calculate then (on the root processor)
the $xz$ and $yz$ averages.
The energies of the so defined mean magnetic fields
are referred to as \var{bmy} and \var{bmx}, respectively,
and the energies of the so defined mean velocity fields
are referred to as \var{umy}, \var{umx}, and \var{umz}, respectively,
(The last letter indicates the direction on which the averaged
quantity still depends.)

{\it Disadvantage}: one needs to set the file \file{zaver.in} with
the entries \var{bxmxy}, \var{bymxy}, \var{bzmxy}, which produces a
rather big file \file[zaverages.dat]{data/zaverages.dat}.


% ---------------------------------------------------------------------- %
\subsection{The f array}
\label{f-array}
The `f' array is the largest array in the Pencil Code and its primary role
is to store the current state of the timestepped PDE variables.  The f array
and its slightly smaller counter part (the df array; see below)
are the only full size 3D arrays in
the code.  The f array is of type real but PDEs for a complex variable may 
be solved by using two slots in the f array.  The actual size of the f array
is $\rm{mx}\times\rm{my}\times\rm{mz}\times\rm{mfarray}$. Here, 
$\rm{mfarray}=\rm{mvar}+\rm{maux}+\rm{mglobal}+\rm{mscratch}$
where $\rm{mvar}$ refers to the number of real PDE variables.
%% WORK IN PROGRESS 

\subsection{The df array}
\label{df-array}
The `df' array is the second largest chunk of data in the Pencil Code.
By using a 2N storage scheme (see \ref{S-2N-scheme}) after 
Williamson \cite{2Nstorage} the code only needs one more storage
area for each timestepped variable on top of the current state stored in
the f-array.  As such, and in contrast to the f array, the df array is of size
$\rm{mx}\times\rm{my}\times\rm{mz}\times\rm{mvar}$.  Like the df array it is
of type real.  In fact the ghost zones
of df are not required or calculated but having f and df arrays of the same
size make the coding more transparent.  For $\rm{mx}$, $\rm{my}$ and
$\rm{mz}$ large the wasted storage becomes negligible.

\subsection{The pencil case}
\label{pencil-case}

Variables that are derived from the basic physical variables of the code are
stored in one-dimensional \name{pencils} of length \var{nx}. All the pencils
that are defined for a given set of physics modules are in turn bundled up in a
Fortran structure called \code{p} (or, more illustrative, the \name{pencil
case}). Access to individual pencils happens through the variable
\code{p\%name}, where \code{name} is the name of a pencil, e.g. \code{rho} that
is a derived variable of the logarithmic density \code{lnrho}.

The pencils provided by a given physics module are declared in the header of
the file, e.g. in the Density module:
\begin{alltt}
! PENCILS PROVIDED lnrho,rho,rho1,glnrho,grho,uglnrho,ugrho
\end{alltt}
Before compiling the code, the script \file{mkcparam} collects the names of all
pencils that are provided by the chosen physics modules. It then defines the
structure \code{p} with slots for every single of these pencils. The definition
of the pencil case \code{p} is written in the include file
\file{cparam_pencils.inc}. When the code is run, the actual pencils that are
needed for the run are chosen based on the input parameters. This is done in
the subroutines \code{pencil_criteria_modulename} that are present in each
physics module. They are all called once before entering the time loop. In the
\code{pencil_criteria} subroutines the logical arrays \code{lpenc_requested},
\code{lpenc_diagnos}, \code{lpenc_diagnos2d}, and \code{lpenc_video} are set
according to the pencils that are needed for the given run. Some pencils depend
on each other, e.g. \code{uglnrho} depends on \code{uu} and \code{glnrho}. Such
interdependencies are sorted out in the subroutines
\code{pencil_interdep_modulename} that are called after
\code{pencil_criteria_modulename}.

In each time-step the values of the pencil logicals \code{lpenc_requested},
\code{lpenc_diagnos}, \code{lpenc_diagnos2d}, and \code{lpenc_video} are
combined to one single pencil array \code{lpencil} which is different from
time-step to time-step depending on e.g.\ whether diagnostics or video output
are done in that time-step.  The pencils are then calculated in the subroutines
\code{calc_pencils_modulename}. This is done before calculating the time
evolution of the physical variables, as this depends very often on derived
variables in pencils.

The centralized pencil calculation scheme is a guarantee that
\begin{itemize}
  \item All pencils are only calculated once
  \item Pencils are always calculated by the proper physics module
\end{itemize}
Since the Pencil Code is a multipurpose code that has many different physics
modules, it can lead to big problems if a module tries to calculate a derived
variable that actually belongs to another module, because different input
parameters can influence how the derived variables are calculated. One example
is that the Density module can consider both logarithmic and non-logarithmic
density, so if the Magnetic module calculates
\begin{alltt}
  rho = exp(f(l1:l2,m,n,ilnrho)
\end{alltt}
it is wrong if the Density module works with non-logarithmic density! The
proper way for the Magnetic module to get to know the density is to request the
pencil \code{rho} in \code{pencil_criteria_magnetic}.

\subsubsection{Pencil check}

To check that the correct pencils have been requested for a given run, one can
run a \name{pencil consistency check} in the beginning of a run by setting the
logical \code{lpencil_check} in \code{\&run_pars}. The check is meant to see if
\begin{itemize}
  \item All needed pencils have been requested
  \item All requested pencils are needed
\end{itemize}
The consistency check first calculates the value of \code{df} with all the
requested pencils. Then the pencil requests are flipped one at a time --
requested to not requested, not requested to requested. The following
combination of events can occur:
\begin{itemize}
  \item not requested $\rightarrow$ requested, \code{df} not changed\\
    The pencil is not requested and is not needed.
  \item not requested $\rightarrow$ requested, \code{df} changed\\
    The pencil is not requested, but is needed. The code stops.
  \item requested $\rightarrow$ not requested, \code{df} not changed\\
    The pencil is requested, but is not needed. The code gives a warning.
  \item requested $\rightarrow$ not requested, \code{df} changed\\
    The pencil is requested and is needed.
\end{itemize}

\subsubsection{Adding new pencils}

Adding a new pencil to the pencil case is trivial but requires a few steps.
\begin{itemize}
  \item Declare the name of the pencil in the header of the proper physics
      module
  \item Define the new pencil in \file{\$PENCIL_HOME/src/sripts/mkcparam}. Here
      is also set whether the pencil is a scalar, a vector or a matrix.
  \item Set interdependency of the new pencil (i.e.\ what other pencils does it
      depend on) in the subroutine \code{pencil_interdep_modulename}
  \item Make rule for calculating the pencil in \code{calc_pencils_modulename}
  \item Request the new pencil based on the input parameters in any relevant
      physics module
\end{itemize}
Remember that the centralized pencilation scheme is partially there to force
the users of the code to think in general terms when implementing new physics.
Any derived variable can be useful for a number of different physics problems,
and it is important that a pencil is accessible in a transparent way to all
modules.





% ---------------------------------------------------------------------- %

\subsection{Adding new physics modules}

If you want to add new physics to the code, you will in many cases want to
add a new Special module.  Doing so is relatively straight forward and
there is even a special directory for such additions.

To create your own special module, copy \file{nospecial.f90} from the src/
directory to a new name in the src/special/ directory.  It is currently only
possible to have one special modules at a time and so several new bits of 
physics are often put in to one special module. For this reasons a name 
should be chosen that relates to the problem to be solved rather than
the specific physics being implemented.

The first thing to do in your new module is to change the lspecial=.false.
header to say lspecial=.true.

The file is heavily commented though all such comments can be removed as you
go.  You may implement any of the subroutines/function that exist in
\name{nospecial.f90} and those routines must have the names and parameters
as in \name{nospecial.f90}.  You do not however need to implement all
routines, and you may either leave the dummy routines copied from
\name{nospecial.f90} or delete them all together (provided the "include
'special_dummy.inc'" is kept intact at the end of the file.  Beyond that,
and data and subroutines can be added to a special module as required,
though only for use within that module.

There are routines in the special interface to allow you to add new
equations, modify the existing equation, add diagnostics, add slices,
and many more things.  If you feel there is something missing extra hooks
can easily be added - please contact the Pencil Code team for assistance.

You are encouraged to submit/commit your special modules to the Pencil-Code
source.

\subsection{Adding switchable modules}

In some cases where a piece of physics is thought to be more fundamental,
useful in many situations or simply more flexibility is required it may be
necessary to add a new module \name{newphysics} together with the
corresponding \name{nonewphysics} module.  The special modules follow
the same structure as the rest of the switchable modules and so using
a special module to prototype new ideas can make writing a new 
switchable module much easier.

For an example of module involving a new
variable (and PDE), the \name{pscalar} module is a good prototype.
The grep command
\begin{alltt}
  \prompt{unix> } grep -i pscalar src/*
\end{alltt}
gives you a good overview of which files you need to edit or add.

% ====================================================================== %

\section{Useful internals}

%%%
\subsection{Global variables}

The following variables are defined in \file{cdata.f90} and are available
in any routine that \cmd{use}s the module \name{Cdata}.

 \begin{longtable}{lp{0.6\textwidth}}
\toprule
  \multicolumn{1}{c}{\emph{Variable}}
               & \multicolumn{1}{c}{\emph{Meaning}} \\
\midrule
  \multicolumn{2}{c}{real}\\
\midrule
  \var{t}
               & simulated time $t$.\\
\midrule
  \multicolumn{2}{c}{integer}\\
\midrule
  \var[nxgrid]{n[xyz]grid}
               & global number of grid points (excluding ghost cells) in
                 $x$, $y$ and $z$ direction. \\
  \var{nx}, \var{ny}, \var{nz}
               & number of grid points (excluding ghost cells) as seen by
                 the current processor, i.\,e.~\code{ny=nygrid/nprocy},
                 etc. \\
  \var{mx}, \var{my}, \var{mz}
               & number of grid points seen by the current processor,
                 but \emph{including ghost cells}.
                 Thus, the total box for the \var[]{ivar}th variable
                 (on the given processor) is given by
                 \code{f(1:mx,1:my,1:mz,ivar)}. \\
  \var{l1}, \var{l2}
               & smallest and largest $x$-index for the physical domain
               (i.\,e.~excluding ghost cells) on the given processor. \\
  \var{m1}, \var{m2}
               & smallest and largest $y$-index for physical domain. \\
  \var{n1}, \var{n2}
               & smallest and largest $z$-index for physical domain,
                 i.\,e.~the physical part of the \var[]{ivar}th variable
                 is given by \code{f(l1:l2,m1:m2,n1:n2,ivar)} \\
  \var{m}, \var{n}
               & pencil indexing variables:
                 During each time-substep the box is traversed in
                 $x$-pencils of length \var{mx} such that the current
                 pencil of the \var[]{ivar}th variable is
                 \code{f(l1:l2,m,n,ivar)}. \\
\midrule
  \multicolumn{2}{c}{logical}\\
\midrule
  \var{lroot}
               & true only for MPI root processor. \\
  \var{lfirst}
               & true only during first time-substep of each time step. \\
  \var{headt}
               & true only for very first full time step (comprising 3
                 substeps for the 3rd-order Runge--Kutta scheme) on root
                 processor. \\
  \var{headtt}
               & \code{= (lfirst .and. lroot)}:
                 true only during very first time-substep on root processor. \\
  \var{lfirstpoint}
               & true only when the very first pencil for a given
                 time-substep is processed, i.\,e.~for the first set of
                 $(\var{m},\var{n})$, which is probably $(3,3)$ . \\ 
  \var{lout}
               & true when diagnostic output is about to be written. \\
\bottomrule
\end{longtable}


%%%
\subsection{Subroutines and functions}

\begin{description}
\item[\code{output(file,a,nv)}]
  (module \name{IO}):
  Write (in each \file[procN/]{proc$N$} directory) the content of the
  global array \emph{a} to a file called \emph{file}, where \emph{a} has
  dimensions \var{mx}$\times$\var{my}$\times$\var{mz}$\times$\var{nv},
  or \var{mx}$\times$\var{my}$\times$\var{mz} if \code{nv=1}.
\item[\code{output_pencil(file,a,nv)}]
  (module \name{IO}):
  Same as \code{output()}, but for a pencil variable, i.\,e.~an auxiliary
  variable that only ever exists on a pencil (e.\,g.~the magnetic field
  strength \var{bb} in \file{magnetic.f90}, or the squared sound speed
  \var{cs2} in \file{entropy.f90}).
  The file has the same structure as those written by \code{output()},
  because the values of \emph{a} on the different pencils are accumulated
  in the file.
  This involves a quite nontrivial access pattern to the file and has thus
  been coded in \name{C} (\file[debug_c.c]{src/debug_c.c}).
\item[\code{cross(a,b,c)}]
  (module \name{Sub}):
  Calculate the cross product of two vectors \emph{a} and \emph{b} and
  store in \emph{c}.
  The vectors must either all be of size
  \var{mx}$\times$\var{my}$\times$\var{mz}$\times$3 (global arrays), or of
  size \var{nx}$\times$3 (pencil arrays).
\item[\code{dot(a,b,c)}]
  (module \name{Sub}):
  Calculate the dot product of two vectors \emph{a} and \emph{b} and
  store in \emph{c}.
  The vectors must either be of size
  \var{mx}$\times$\var{my}$\times$\var{mz}$\times$3 (\emph{a} and
  \emph{b}) and \var{mx}$\times$\var{my}$\times$\var{mz} (\emph{c}),
  or of size \var{nx}$\times$3 (\emph{a} and \emph{b}) and
  \var{nx} (\emph{c}).
\item[\code{dot2(a,c)}]
  (module \name{Sub}):
  Same as \code{dot(a,a,c)}.
\end{description}

\cleardoublepage
% ====================================================================== %
\typeout{=======LAST_BODY_PAGE=======}
\appendix
% ====================================================================== %

\part{Appendix}

\section*{APPENDIX
  \hfill\normalsize\rm$ $Date: 2007-08-19 08:55:28 $ $,~ $ $Revision: 1.366 $ $}

\section{Timings}

In the following table we list the results of timings of the code on
different machines.
Shown is (among other quantities) the wall clock time per mesh point
(excluding the ghost zones) and per full 3-stage time step, a quantity
that is printed by the code at the end of a run.\footnote{
  Note that when using \file{nompicomm.f90}, the timer currently used will
  overflow on some machines, so you should not blindly trust the timings
  given by the code.
}

As these results were assembled during the development phase of the code
(that hasn't really finished yet,\ldots), you may not get the same numbers,
but they should give some orientation of what to expect for your specific
application on your specific hardware.

The code will output the timing (in microseconds per grid point per time-step)
at the end of a run. You can also specify \code{walltime} in \code{print.in}
to have the code continuously output the physical time it took to reach the
time-steps where diagnostics is done. The time-dependent code speed can then be
calculated by differentiating, e.g.\ in \code{IDL} with\\
\prompt{IDL> } \code{pc\_read\_ts, obj=ts}\\
\prompt{IDL> } \code{plot, ts.it, 1/nw*deriv(ts.it,ts.walltime/1.0e-6), psym=2}\\
where \code{nw=nx*ny*nz}.


% ---------------------------------------------------------------------- %
\small %AB: Wolfgang, this \small doesn't seem to have an effect!?
%\begin{table}[htb]
\begin{center}
%    \caption{
%      Wall clock time per mesh point (excluding the ghost zones)
%      and per full 3-stage time step.
%    }
%    \label{Ttimescale}
    \begin{small}
%    \begin{tabular}{rllrlrll}
    \begin{longtable}{rllrlrrr}
    \toprule
   \mcc{proc}
     & \mcc{machine}
             & \mcc{$\displaystyle\frac{{\mu\rm s}}{\rm pt\;\;step}$}
                    & \mcc{resol.} 
                              & \mcc{what}
                                             & \mcc{mem/proc}
                                                      & \mcc{when}&\mcc{who}\\
    \midrule
   1 & Nl3   &  19  &  $64^3$ & kinematic    &  10 MB & 20-may-02 & AB \\
   1 & Nl3   &  30  &  $64^3$ & magn/noentro &  20 MB & 20-may-02 & AB \\
   1 & Nq1   &  10  &  $64^3$ & magn/noentro &        & 30-may-02 & AB \\
   1 & Ukaff & 9.2  &  $64^3$ & magn/noentro &        & 20-may-02 & AB \\
   1 & Nl6   & 6.8  &  $64^3$ & magn/noentro &        & 10-mar-03 & AB \\
   1 & Nl6   & 36.3 &  $64\!\times\!128\!\times\!64$
                              & nomag/entro/dust &    & 19-sep-03 & AB \\
   1 & Nl6   & 42.7 & $16^2{\times}256$                                  
                     & nomag/entro/rad6/ion &        & 22-oct-03 & AB \\
   1 & Nl6   & 37.6 & $16^2{\times}256$                                  
                     & nomag/entro/rad2/ion &        & 22-oct-03 & AB \\
   1 & Nl6   & 19.6 & $16^2{\times}256$                                  
                      & nomag/entro/ion &      & 22-oct-03 & AB \\
   1 & Nl6   &  8.7 & $16^2{\times}256$ & nomag/entro &      & 22-oct-03 & AB \\
   1 & Nl6n  &  9.8 & $32^3$                                  
                    & magn/noentro/pscalar   &        & 17-mar-06 & AB \\
   1 & Mhd   & 7.8  &  $64^3$ & magn/noentro &        & 20-may-02 & AB \\
   1 & Nq4   &14.4  & $128^3$ & magn/noentro &        &  8-oct-02 & AB \\
   1 & Nq5   & 6.7  & $128^3$ & magn/noentro &        &  8-oct-02 & AB \\
   1 & fe1   & 5.1  & $128^3$ & magn/noentro &        &  9-oct-02 & AB \\
   1 & Kabul & 4.4  & $128^3$ & magn/noentro & 130 MB & 20-jun-02 & WD \\
   1 & Hwwsx5& 3.4  & $256^3$ & convstar     & 7.8 GB & 29-jan-03 & WD \\
   1 &Mac/g95& 7.7  & $32^3$  & magn/noentro &        & 14-jan-07 & BD \\
   1 &Mac/ifc& 4.5  & $32^3$  & magn/noentro &        & 14-jan-07 & BD \\
   2 & Kabul & 2.5  & $128^3$ & magn/noentro &  80 MB & 20-jun-02 & WD \\
   2 & Nq3+4 & 7.4  & $128^3$ & magn/noentro &        &  8-oct-02 & AB \\
   2 & Nq4+4 & 8.9  & $128^3$ & magn/noentro &        &  8-oct-02 & AB \\
   2 & Nq4+5 & 7.3  & $128^3$ & magn/noentro &        &  8-oct-02 & AB \\
   2 & Nq5+5 & 3.7  & $128^3$ & magn/noentro &        &  8-oct-02 & AB \\
   2 & fe1   & 3.45 & $128^3$ & magn/noentro &        &  9-oct-02 & AB \\
   2 & Nq2   & 9.3  &  $64^3$ & magn/noentro &        & 11-sep-02 & AB \\
   2 & Nq1+2 & 8.3  &  $64^3$ & magn/noentro &        & 11-sep-02 & AB \\
   2 & Hwwsx5& 1.8  & $256^3$ & convstar     & 7.9 GB & 29-jan-03 & WD \\
   4 & Nq1+2 & 5.4  &  $64^3$ & magn/noentro &        & 11-sep-02 & AB \\
   4 & Nq1235& 4.1  & $128^3$ & magn/noentro &        & 11-sep-02 & AB \\
   4 & Nq0-3 & 6.8  & $256^3$ & magn/noentro & 294 MB & 10-jun-02 & AB \\
   4 & Mhd   & 2.76 &  $64^3$ & magn/noentro &        & 30-may-02 & AB \\
   4 & fe1   & 3.39 &  $32^3$ & magn/noentro &        & 16-aug-02 & AB \\
   4 & Rasm. & 2.02 &  $64^3$ & magn/noentro &  2x2   &  8-sep-02 & AB \\
   4 & Mhd   & 8.2  &  $64^2\!\times\!16$
                              & nomag/entro &        & 23-jul-02 & AB \\
   4 & fe1   & 6.35 &  $64\!\times\!128\!\times\!64$
                              & nomag/entro/dust &    & 19-sep-03 & AB \\
   4 & fe1   & 2.09 & $128^3$ & magn/noentro &        &  9-oct-02 & AB \\
   4 & fe1   & 1.45 & $128^3$ & magn/noentro & giga   &  9-oct-02 & AB \\
   4 & fe1   & 7.55 & $16^2{\times}512$                                  
                     & nomag/entro/rad2/ion &  4x1   &  1-nov-03 & AB \\
   4 & fe1   & 5.48 & $16^2{\times}512$                                  
                     & nomag/entro/rad2/ion &  1x4   &  1-nov-03 & AB \\
   4 & Luci  & 1.77 &  $64^3$ & magn/noentro &        & 27-feb-07 & AB \\
   4 & Lenn  & 0.65 &  $64^3$ &nomag/noentro &        & 13-jan-07 & AB \\
   4 & Lenn  & 1.21 &  $64^3$ & magn/noentro &        &  7-nov-06 & AB \\
   4 & Kabul & 1.5  & $128^3$ & magn/noentro &  47 MB & 20-jun-02 & WD \\
   4 & Hwwsx5& 1.8  & $256^3$ & convstar     & 8.2 GB & 29-jan-03 & WD \\
   8 & Nqall & 3.0  & $128^3$ & magn/noentro &        &  8-oct-02 & AB \\
   8 & fe1   & 3.15 &  $64^3$ & magn/noentro &  1x8   &  8-sep-02 & AB \\
   8 & fe1   & 2.36 &  $64^3$ & magn/noentro &  2x4   &  8-sep-02 & AB \\
   8 & Ukaff & 1.24 &  $64^3$ & magn/noentro &        & 20-may-02 & AB \\
   8 & Kabul & 1.25 & $64^2{\times}128$                                  
                              & nomag/entro &        & 11-jul-02 & WD \\
   8 & fe1   & 1.68 & $128^3$ & magn/noentro &  1x8   &  8-sep-02 & AB \\
   8 & fe1   & 1.50 & $128^3$ & magn/noentro &  2x4   &  8-sep-02 & AB \\
   8 & fe1   & 1.44 & $128^3$ & magn/noentro &  4x2   &  8-sep-02 & AB \\
   8 & Kabul & 0.83 & $128^3$ & magn/noentro &  28 MB & 20-jun-02 & WD \\
   8 & Gridur& 1.46 & $128^3$ & magn/noentro &        & 19-aug-02 & NE \\
   8 & Kabul & 0.87 & $256^3$ & magn/noentro & 160 MB & 20-jun-02 & WD \\
   8 & fe1   & 0.99 & $256^3$ & magn/noentro &  2x4   &  8-sep-02 & AB \\
   8 & fe1   & 0.98 & $256^3$ & magn/noentro &  4x2   &  8-sep-02 & AB \\
   8 & Mhd   & 1.46 & $160^2\times40$
                              & nomag/entro &  46 MB &  7-oct-02 & AB \\
   8 & Hwwsx5& 0.50 & $256^3$ & convstar     & 8.6 GB & 29-jan-03 & WD \\
  16 & fe1   & 1.77 &  $64^3$ & convstar     &        &  9-feb-03 & AB \\
  16 & copson& 0.596& $128^3$ & geodynamo/ks95 &      & 21-nov-03 & DM \\
  16 & fe1   & 0.94 & $128^3$ & magn/noentro &  4x4   &  8-sep-02 & AB \\
  16 & fe1   & 0.75 & $128^3$ & magn/noentro &4x4/ifc6&  9-may-03 & AB \\
  16 & workq & 0.88 & $128^3$ & magn/noentro &4x4/ifc6& 21-aug-04 & AB \\
  16 & giga  & 0.76 & $128^3$ & magn/noentro &4x4/ifc6& 21-aug-04 & AB \\
  16 & giga2 & 0.39 & $128^3$ & magn/noentro &4x4/ifc6& 20-aug-04 & AB \\
  16 & giga  & 0.47 & $128^3$ & chiral       &4x4/ifc6& 29-may-04 & AB \\
  16 & giga  & 0.43 & $128^3$ & nomag/noentro&4x4/ifc6& 28-apr-03 & AB \\
  16 & Mhd   & 2.03 & $128^3$ & magn/noentro &        & 26-nov-02 & AB \\
  16 & Mhd   & 0.64 & $256^3$ & magn/noentro &  60 MB & 22-may-02 & AB \\
  16 & fe1   & 0.56 & $256^3$ & magn/noentro &  4x4   & 16-aug-02 & AB \\
  16 & fe1   & 6.30 & $128\!\times\!256\!\times\!128$
                              & nomag/entro/dust &    & 19-sep-03 & AB \\
  16 & fe1   & 1.31 & $128^2{\times}512$                                  
                     & nomag/entro/rad2/ion &  4x4   &  1-nov-03 & AB \\
  16 & Ukaff & 0.61 & $128^3$ & magn/noentro &        & 22-may-02 & AB \\
  16 & Ukaff & 0.64 & $256^3$ & magn/noentro &        & 20-may-02 & AB \\
  16 & Kabul & 0.80 & $128^3$ & magn/noentro &  16 MB & 20-jun-02 & WD \\
  16 & Kabul & 0.51 & $256^3$ & magn/noentro &   9 MB & 20-jun-02 & WD \\
  16 & Gridur& 0.81 & $128^3$ & magn/noentro &        & 19-aug-02 & NE \\
  16 & Gridur& 0.66 & $256^3$ & magn/noentro &        & 19-aug-02 & NE \\
  16 & Sander& 0.53 & $256^3$ & magn/noentro &        &  8-sep-02 & AB \\
  16 & Luci  & 0.375& $128^3$ & magn/noentro &        & 28-oct-06 & AB \\
  16 & Lenn  & 0.284& $128^3$ & magn/noentro &        &  8-nov-06 & AB \\
  32 & giga? & 0.32 & $256^3$ & magn/noentro &        & 13-sep-03 & AB \\
  32 & Ukaff & 0.34 & $256^3$ & magn/noentro &        & 20-may-02 & AB \\
  32 & Ukaff & 0.32 & $512^3$ & magn/noentro &        & 20-may-02 & AB \\
  32 & fe1   & 0.168& $512^3$ &nomag/noentro&        &  9-oct-02 & AB \\
  32 & fe1   & 1.26 & $64^2\times256$ &nomag/entro/rad/ion& & 7-sep-03 & AB \\
  32 & Luci  & 0.182& $256^3$ &magn/noentro &        & 26-feb-07 & AB \\
  32 & Lenn  & 0.147& $256^3$ &nomag/entro/cool/fo& 4x8 &  8-nov-06 & AB \\
  32 & Steno & 0.076& $256^3$ &nomag/entro/cool/fo& 4x8 & 20-jun-06 & AB \\
  32 & Steno & 0.081& $256^3$ &nomag/entro/cool& 4x8 & 20-jun-06 & AB \\
  32 & Steno & 0.085& $256^3$ &nomag/entro/cool/sh& 4x8 & 20-jun-06 & AB \\
  32 & Steno & 0.235& $512^2\times256$ &mag/entro& 4x8 &  9-jul-06 & AB \\
  32 & Sanss & 0.273& $128\times256^2$ &nomag& 4x8 &  3-jul-07 & AB \\
  64 & fe1   & 0.24 & $256^3$ & magn/noentro &  8x8   &  2-sep-02 & AB \\
  64 & giga  & 0.11 & $256^3$ &nomag/noentro&  4x16  & 29-apr-03 & AB \\
  64 & giga  & 0.23 & $256^3$ &nomag/noentro/hyp&4x16& 8-dec-03& AB \\
  64 & fe1   & 0.164& $512^3$ &nomag/noentro/hyp&4x16&17-dec-03& AB \\
  64 & giga  & 0.091& $512^3$ &nomag/noentro/hyp&4x16&17-dec-03& AB \\
  64 & giga  & 0.150& $256^3$ & magn/noentro &  4x16  &  1-jul-03 & AB \\
  64 & giga  & 0.166& $512^3$ & magn/noentro &64*173MB& 10-jul-03 & AB \\
  64 & Gridur& 0.25 & $256^3$ & magn/noentro &        & 19-aug-02 & NE \\
  64 & Ukaff & 0.17 & $512^3$ & magn/noentro &        & 21-may-02 & AB \\
  64 & Steno & 0.075& $512^3$ & magn/noentro &  8x16  & 19-oct-06 & AB \\
 128 & fe1   & 0.44 & $256^3$ & nomag/entro/rad8/ion &  4x32  & 10-mar-04 & TH \\
 128 & fe1   & 2.8  & $512^3$ & magn/noentro & 16x8   &  5-sep-02 & AB \\
 128 & fe1   & 0.51 & $512^3$ & magn/noentro & 8x16   &  5-sep-02 & AB \\
 128 & fe1   & 0.27 & $512^3$ & magn/noentro & 4x32   &  5-sep-02 & AB \\
 128 & fe1   & 0.108& $512^3$ & magn/noentro &4x32/ifc6& 5-jan-02 & AB \\
64+64& giga2 & 0.0600&$512^3$ & magn/noentro &4x32/ifc6&21-aug-04 & AB \\
 128l& giga2 & 0.0605&$512^3$ & magn/noentro &4x32/ifc6&21-aug-04 & AB \\
 128 & fe1   & 0.35 & $512^3$ & magn/noentro & 2x64   &  9-sep-02 & AB \\
 128 & fe1   & 0.094& $786^3$ & magn/noentro &4x32/ifc6& 9-sep-02 & AB \\
 256 & giga2 & 0.028& $1024^3$& magn/noentro &4x64/ifc6&20-aug-04 & AB \\
    \bottomrule
  \end{longtable}
  %\end{tabular}
    \end{small}
  \end{center}
%\end{table}
\normalsize
% ---------------------------------------------------------------------- %

The machines we have used can be characterized as follows:
\begin{description}
\item[Nl3:] 500\,MHz Pentium III single CPU;
  RedHat Linux 6.2;
  256\,MB memory
\item[Nq0:] 931\,MHz Pentium III single CPU;
  RedHat Linux 7.3;
  0.5\,GB memory
\item[Nq{[1-4]}:] 869\,MHz Pentium III dual-CPU cluster;
  RedHat Linux 7.3;
  0.77\,GB memory per (dual) node
\item[Nq{[5-6]}:] 1.2\,GHz Athlon dual-CPU cluster;
  RedHat Linux 7.3;
  1\,GB memory per (dual) node
\item[Kabul:] 1.9\,GHz Athlon dual-CPU cluster;
  1\,GB memory per (dual) node;
  256\,kB cache per CPU;
  Gigabit ethernet;
  SuSE Linux 8.0;
  LAM-MPI
\item[Cincinnatus:] 1.7\,GHz Pentium 4 single CPU;
  1\,GB memory;
  256\,kB cache per CPU;
  SuSE Linux 7.3
\item[Horseshoe (fe1, giga, and giga2):] consists of different subclusters.
  The old one (queue name: workq, referred to as fe1)
  2.0\,GHz Pentium 512 single CPU;
  25x 24-port fast ethernet switches with gigabit ethernet uplink;
  1 30-port gigabit ethernet switch; 1\,GB memory.
  The next generation has gigabit switches directly between nodes,
  and 2.6 GHz processors.
  The third generation (giga2) has 3.2 GHz processors (most of which have 1 GB,
  some 2 GB), is organized in 2 blocks interconnected with 2 Gb links,
  with 10 Gb uplinks within each block.
\item[Ukaff:] SGI Origin 3000;
  400\,MHz IP35 CPUs;
  IRIX 6.5;
  native MPI
\item[Mhd:] EV6 Compaq cluster with 4 CPUs per node;
  4\,GB memory per node (i.\,e.~1\,GB per CPU)
  OSF1 4.0;
  native MPI
\item[Sander and Rasmussen:] Origin 3000
\item[Steno] 118 node IBM cluster with dual node AMD Opteron processors
with 10 Gb infiniband network, compiled with pgf90 -fastsse -tp k8-64e (Copenhagen).
\item[Gridur:] Origin 3000
\item[Luci:] (full name Lucidor) is an HP Itanium cluster, each of the 90
nodes has two 900 MHz Itanium 2 "McKinley" processors and 6 GB of main memory.
The interconnect is myrinet.
\item[Lenn:] (full name Lenngren) is a Dell Xeon cluster with 442 nodes.
Each node has two 3.4GHz ``Nocona'' Xeon processors and 8GB of main memory. 
A high performance Infiniband network from Mellanox is used for MPI traffic.
\end{description}

Table~\ref{Ttimescale-convsample} shows a similar list, but for a few
well-defined sample problems.
The \name{CVS} \index{CVS} check-in patterns are displayed graphically
in Fig~\ref{cvsstat}.

% ---------------------------------------------------------------------- %
\begin{figure}[htb]
  \centering
  \includegraphics%
    [width=.6\textwidth,keepaspectratio]%
    {ptimings}
  \caption{Scaling results on three different machines.
  The thin straight line denotes perfectly linear scaling.}
  \label{ptimings}
\end{figure}
% ---------------------------------------------------------------------- %

% ---------------------------------------------------------------------- %
\begin{table}[htb]
  \begin{center}
    \caption{
      Like previous table, but for the versions from the
      \file[samples/]{samples} directory.
    }
    \label{Ttimescale-convsample}
    \begin{small}
    \begin{tabular}{rllrlrll}
    \toprule
    \mcc{proc(s)}
       & \mcc{machine}
             & \mcc{$\displaystyle\frac{{\mu\rm s}}{\rm pt\;\;step}$}
                    & \mcc{resol.} 
                                    & \mcc{mem./proc}
                                                    & \mcc{when}&\mcc{who}\\
    \midrule
      \multicolumn{7}{c}{\emph{conv-slab}}\\
    \midrule
        1 & Mhd         & 6.45 &  $32^3$ &    4 MB & 23-jul-02 & wd \\
        1 & Cincinnatus & 4.82 &  $32^3$ &    3 MB & 23-jul-02 & wd \\
        1 & Cincinnatus & 11.6 &  $64^3$ &   14 MB & 23-jul-02 & wd \\
        1 & Cincinnatus & 20.8 & $128^3$ &   93 MB & 23-jul-02 & wd \\
        1 & Kabul       & 3.91 &  $32^3$ &         & 23-jul-02 & wd \\
        1 & Kabul       & 3.88 &  $64^3$ &         & 23-jul-02 & wd \\
        1 & Kabul       & 4.16 & $128^3$ &   93 MB & 23-jul-02 & wd \\
    \midrule
      \multicolumn{7}{c}{\emph{conv-slab-flat}}\\
    \midrule
        1 & Kabul       & 3.02 &  $128^2{\!\times\!}32$
                                         &   29 MB & 23-jul-02 & wd \\
        2 & Kabul       & 1.81 &  $128^2{\!\times\!}32$
                                         &   18 MB & 23-jul-02 & wd \\
        4 & Kabul       & 1.03 &  $128^2{\!\times\!}32$
                                         &   11 MB & 23-jul-02 & wd \\
        8 & Kabul       & 0.87 &  $128^2{\!\times\!}32$
                                         &    9 MB & 23-jul-02 & wd \\
    \bottomrule
    \end{tabular}
    \end{small}
  \end{center}
\end{table}
% ---------------------------------------------------------------------- %
% ====================================================================== %
\section{Coding Standard}
\label{coding-standard}

The numerous elements that make up the Pencil Code are written
in a consistent style that has evolved since it was first created.
Many people have contributed their knowledge and experience with
in this and the result is what we believe is and extremely
readable and manageable code.

As well as improving the readability of the code, by having some
naming conventions for example aids greatly in understanding what
the code does. 

There is a standard for all aspect of the code be it Fortran source,
shell scripts, Perl scripts, LaTeX source, Makefiles or otherwise.
Where nothing has been explicitly stated it is recommended that 
similar existing examples found in the code are used as a template.

\subsection{File naming conventions}
All files with the exception of the \file{Makefile}s are given
lowercase filenames. 

Fortran source files all have the `.f90' extension.  Files that
contain `non-executable code' i.e. declarations that are included into 
other files are given the extension `.h' and those that are generated 
dynamically at compile time have a `.inc' extension.

Fortran source code defining a module is placed in files whose names
begin with the Fortran module name in all lowercase.  Where there
exist multiple implementations of a specific module the filenames
are extended using and with an underscore ad a brief name relating
to what they do.

Text files containing parameters to be read by the code at run time are
placed in files with the extension `.in'


\subsection{Fortran Code}
Fortran is not case-sensitive but in almost all instances we prescribe
some form of capitalization for readability.

In general all Fortran code including keywords, variable names etc. are
written in lowercase. 

\subsubsection{Indenting and Whitespace}
Whitespace should be removed from the end of lines.

Blank lines are kept to a minimum, and when occurring in subroutines
or functions are replaced by a single `!' in the first column.

Tab characters are not used anywhere in the code.  Tab characters are
not in fact allowed by the Fortran standard and compilers that accept
them do so as an extension.

All lines are kept to be not more than 78 characters long. Where
lines are longer they must be explicitly wrapped using the 
Fortran continuation character `\&'

Code in syntactic blocks such as if--endif, do--enddo, 
subroutine--endsubroutine etc. is always indented by precisely two spaces.
The exception to this is that nested loops where only the innermost loop
contains executable code should be written with an the do--enddo pairs at 
the same level of indentation.

\begin{itemize}
\item in general all comments are placed on their own lines with the '!' 
appearing in the first column. 
\item all subroutine/functions begin with a standard comment block describing
what they do, when and by whom they were created and when and by whom any
non-trivial modifications were made.
\item lines longer that 78 characters must be explicitly wrapped. 
\end{itemize}

\subsubsection{Comments}
Descriptive comments are written on their own lines unless there is a 
strong reason to do otherwise.  Comments are not normally indented
and the `!' should appear in the first column followed by two spaces
and then the text of the comment.

Comments also must not exceed the 78 character line length and should
be wrapped onto more lines as needed.

Typically comments should appear with a blank commented line above and
below the wrapped text of the comment.

Comments should be written in sentences using the usual capitalization 
and punctuation of English where comment text continues either side of
actual code much as text is continued around equations in a mathematical
document. Punctuation at the end of a comment is mostly omitted. 

For example:
\begin{verbatim}
      some fortran code
      some more fortran code
!
!  A descriptive comment explaining what the following few lines
!  of code do.  And so it begins
!
      the fortran code being described
      the fortran code being described
      ...
!
!  then just time for a final detail
!
      the final fortran code 
      the final fortran code
      ...
\end{verbatim}

Subroutines and functions are started with a comment block describing
what they do, when and by whom they were created and when and by whom any
non-trivial modifications were made.  The layout of this comment block
is a standard, for example:
\begin{verbatim}
!***********************************************************************
    subroutine initialize_density(f,lstarting)
!
!  Perform any post-parameter-read initialization i.e. calculate derived
!  parameters.
!
!  For compatibility with other applications, we keep the possibility
!  of giving diffrho units of dxmin*cs0, but cs0 is not well defined general
!
!  24-nov-02/tony: coded 
!  31-aug-03/axel: normally, diffrho should be given in absolute units
!
\end{verbatim}
where dates are written in dd-mmm-yy format as shown and names appearing
after the `/' are either the users cvs login name or, where such exists
amongst the Pencil Code community, the accepted short form ($\approx 4$ 
characters) of the authors name.

\subsubsection{Module Names}
The names of modules are written with initial letter capitalization 
of each word and the multiple words written consecutively without
any separator.

\subsubsection{Variable Names}
Variable are given short but meaning full names and written
in all lowercase.  Single character names are avoided except
for commonly used loop indices and the two code data structures
of the Pencil Code: `f' the main state array (see \ref{f-array}) and `p' the pencil case
structure (see \ref{pencil-case}).

Quantities commonly represented by a particular single character
in mathematics are typically given names formed by repeating the
character (usually in lowercase), e.g. the velocity $u$ becomes `uu', 
specific entropy $s$ becomes `ss' etc. 

Temperature in variable names is denoted with a capital T so as not to
be confused with time as represented by a lowercase t.  Note however the
since Fortran is not case sensitive the variables for example `TT' and
`tt' are the same so distinct names must be used. For this reason time is
usually represented by a single t contrary to the above guideline.

The natural log of a quantity is represented by using adding `ln' to its 
name, for example log of temperature would be `lnTT'.

There are some standard prefixes used to help identify the type and nature
of variables they are as follows:
\begin{itemize}
\item i -- Denotes integer variables typically used as array indices.
\item i_ -- Denotes pencil case array indices.
\item idiag_ -- Denotes diagnostic indices.
\item l -- Denotes logical/boolean flags
\item cdt -- Denotes timestep constraint parameters.
\item unit_ -- Denotes conversion code/physics unit conversion parameters.
\end{itemize}

\subsubsection{Emacs settings}
Here are some settings from wd's \file[.emacs]{\~{}/.emacs} file:
\begin{Verbatim}
  ;;; ~/.f90.emacs
  ;;; Set up indentation and similar things for coding the Pencil Code.
  ;;; Most of this can probably be set through Emacs' Customize interface
  ;;; as well.
  ;;; To automatically load this file, put the lines
  ;;;   (if (file-readable-p "~/.f90.emacs")
  ;;;       (load-file "~/.f90.emacs"))
  ;;; into your ~/.emacs file.

  ;; F90-mode indentation widths
  (setq f90-beginning-ampersand nil) ; no 2nd ampersand at continuation line
  (setq f90-do-indent           2)
  (setq f90-if-indent           2)
  (setq f90-type-indent         2)
  (setq f90-continuation-indent 4)

  ;; Don't use any tabs for indentation (with TAB key).
  ;; This is actually already set for F90-mode.
  (setq-default indent-tabs-mode nil)

  ;; Ensure Emacs uses F90-mode (and not Fortran-mode) for F90 files: 
  (setq auto-mode-alist
        (append
         '(
           ("\\.[fF]90$"   . f90-mode)
           ("\\.inc$"      . f90-mode)
          )
         auto-mode-alist))

  ;; Make M-Backspace behave in Xemacs as it does in GNU Emacs. The default
  ;; behavior is apparently a long-known bug the fix for which wasn't
  ;; propagated from fortran.el to f90.el.
  ;; (http://list-archive.xemacs.org/xemacs-patches/200109/msg00026.html):
  (add-hook 'f90-mode-hook
            (function (lambda ()
               (define-key f90-mode-map [(meta backspace)] 'backward-kill-word)
  )))
\end{Verbatim}
\subsection{Other best practices}
When implementing IF or SELECT blocks always write
code for all cases -- including the default or else case. 
This should be done even when that code is only a call to 
raise an error that the case should not have been reached.
If you see a missing case anywhere then do add it.  These
failsafes are essential in a large multi-purpose multi-user 
code like the Pencil Code.

If a case is supposed to do nothing and it may be unclear that the
coder has recognized this fact then make it explicit by adding the 
default case with a comment like  ! Do Nothing  .   
The compiler will clean away any such empty blocks.


% ====================================================================== %

\section{Some specific initial conditions}

\subsection{Random velocity or magnetic fields}

Obtained with \code{inituu='gaussian-noise'}
(or \code{initaa='gaussian-noise'}).
The vector $\uv$ (or $\Av$) is set to normally distributed, uncorrelated random
numbers in all meshpoints for all three components.
The power spectrum of
$\uv$ ($\Av$) increases then quadratically with wavenumber $k$ (without cutoff)
and the power spectrum of $\omv$ (or $\Bv$) increases like $k^4$.

Note that a random initial condition contains significant power at the
Nyquist frequency ($k_{\rm Ny}=\pi/N$, where $N$ is the number of mesh
points). In a decay calculation, because of the discretization error,
such power decays slower than it ought to; see Fig.~\ref{ppowercomp2},
where we show the evolution for a random initial velocity field for $64^3$
meshpoints, $\nu=5\times10^{-2}$ (fairly large!), and \code{nfilter=30}.

It is clearly a good idea to filter the initial condition to prevent
excess power at $k_{\rm Ny}$. On the other hand, such excess power is
weak by comparison with the power at the energy carrying scale, so one
does not see it in visualizations in real space. Furthermore, as seen
from Fig.~\ref{ppowercomp2}, for $k<k_{\rm Ny}/2$ the power spectra for
filtered and unfiltered initial conditions is almost the same.

% ---------------------------------------------------------------------- %
\begin{figure}[hbtp]
  \centering
  \includegraphics%
    [width=0.8\textwidth,keepaspectratio]%
    {ppowercomp2}
  \caption{Velocity power spectra at three different times
    with and without filtering of the initial condition.
  }
  \label{ppowercomp2}
\end{figure}
% ---------------------------------------------------------------------- %


% ---------------------------------------------------------------------- %

\subsection{Beltrami fields}
Obtained with \code{inituu='Beltrami-z'} or \code{initaa='Beltrami-z'}.

\begin{equation}
  \Av=(\cos z,\;\sin z,\;0),\quad\mbox{or}\quad
  \uv=(\cos z,\;\sin z,\;0)
  \label{Beltrami}
\end{equation}


% ---------------------------------------------------------------------- %

\subsection{Magnetic flux rings: \code{initaa='fluxrings'}}
\label{fluxrings}

This initial condition sets up two interlocked thin magnetic tori
(i.\,e.~thin, torus-shaped magnetic flux tubes).
One torus of radius $R$ lying in the plane $z=0$ can be described in
cylindrical coordinates by the
vector potential
\begin{equation} \label{Av-flux-ring-cyl}
  \Av = 
  \Phi_{\rm m}
  \begin{pmatrix}
    0\\ 0\\ -\Heavi(r{-}R) \delta(z)
  \end{pmatrix} \; ,
\end{equation}
resulting in a magnetic field
\begin{equation}
  \Bv = 
  \Phi_{\rm m}
  \begin{pmatrix}
    0\\ \delta(r{-}R) \delta(z)\\ 0
  \end{pmatrix} \; .
\end{equation}
Here $\Phi_{\rm m}$ is the magnetic flux through the tube,
$\Heavi(x)$ denotes the Heaviside function, and
\begin{equation} \label{Heavi-Dirac}
 \delta(x) = \Heavi'(x)
\end{equation}
is Dirac's delta function.

Any smoothed versions of $\Heavi(x)$ and $\delta(x)$ will do, as long as
the consistency condition (\ref{Heavi-Dirac}) is satisfied.
E.\,g.~the pairs
\begin{equation} \label{delta-Gaussian}
  \delta_\varepsilon(x)
  = \frac{1}{\sqrt{2\pi\varepsilon^2}} e^{-\frac{x^2}{2\varepsilon^2}} \; ,
  \quad
  \Heavi_\varepsilon(x)
  = \frac{1}{2} \left( 1 + \erf\frac{x}{\sqrt{2}\varepsilon} \right)
\end{equation}
or
\begin{equation}
  \delta_\varepsilon(x)
  = \frac{1}{2\varepsilon}\frac{1}{\cosh^2\frac{x}{\varepsilon}} \; ,
  \quad
  \Heavi_\varepsilon(x)
  = \frac{1}{2} \left( 1 + \tanh\frac{x}{\varepsilon} \right)
\end{equation}
are quite popular.
Note, however, that the Gaussian profile (\ref{delta-Gaussian}) is the
only one that yields a radially symmetric (with respect to the distance
from the central line of the torus) magnetic field profile
$B_varphi = B_\varphi(\sqrt{(r{-}R)^2{+}z^2})$
if $\varepsilon$ is sufficiently small.

In Cartesian coordinates, the vector potential (\ref{Av-flux-ring-cyl})
takes the form
\begin{equation} \label{Av-flux-ring-cart}
  \Av =
  \Phi_{\rm m}
  \begin{pmatrix}
    0\\ 0\\ -\Heavi \left( \sqrt{x^2{+}y^2}{-}R \right) \delta(z)
  \end{pmatrix} \; .
\end{equation}


% ---------------------------------------------------------------------- %

\subsection{Vertical stratification}
\label{VerticalStratification}
\index{Stratification}

Gravity, $\bf{g}=-\nab\Phi$, is specified in terms of a potential
$\Phi$. In slab geometry, $\Phi=\Phi(z)$, we have ${\bf g}=(0,0,g_z)$ and
$g_z=-\dd\Phi/\dd z$.

Use \var{grav_profile}='const' (which is already the default)
together with \var{gravz}$=-1$ to get
\begin{equation} \label{constgrav-gravz-init}
  \Phi=(z-z_\infty)(-g_z),\quad (-g_z)>0.
\end{equation}
Use \var{grav_profile}='linear' to get
\begin{equation} \label{disc-gravz-init}
  \Phi={\textstyle\frac{1}{2}}(z^2-z_\infty^2)\nu_{\rm g}^2,
\quad g_z=-\nu_{\rm g}^2z
\end{equation}
where $\nu_{\rm g}$ is the vertical epicyclic frequency. For a Keplerian
accretion disc, $\nu_{\rm g}=\varOmega$. For galactic discs,
$\nu_{\rm g}=0.5\varOmega$ is representative of the solar neighborhood.

The value of $z_\infty$ is determined such that $\varrho=\varrho_0$ and
$c_{\rm s}^2=c_{\rm s0}^2$ at $z=z_{\rm ref}$. This depends on the
values of $\gamma$ and the polytropic index $m$ (see below).

\subsubsection{Isothermal atmosphere}

Here we want $c_{\rm s}=c_{\rm s0}=\mbox{const}$.
Using \code{initlnrho='isothermal'} means
\begin{equation} \label{constgrav-lnrho-init}
  \ln\frac{\varrho}{\varrho_0}
  = -\gamma \frac{\Phi}{c_{\rm s0}^2} \; .
\end{equation}
The entropy is then initialized to
\begin{equation} \label{constgrav-ss-init}
  \frac{s}{c_p}
  = (\gamma{-}1) \frac{\Phi}{c_{\rm s0}^2} \; .
\end{equation}
In order that $\varrho=\varrho_0$ and $c_{\rm s}^2=c_{\rm s0}^2$ at
$z=z_{\rm ref}$, we have to choose $z_\infty=z_{\rm ref}$.

\subsubsection{Polytropic atmosphere}

For a polytropic equation of state, $p=K\varrho^\Gamma$, where generally
$\Gamma\neq\gamma$, we can write
\begin{equation}
  -\grad h+T\grad s
  = -\frac{1}{\varrho}\grad p
  = -\grad\left(\frac{\Gamma K}{\Gamma{-}1}\varrho^{\Gamma{-}1}\right)
\equiv-\grad\tilde{h},
\end{equation}
where we have introduced a pseudo enthalpy $\tilde{h}$ as
\begin{equation}
  \tilde{h} = \frac{\Gamma K}{\Gamma{-}1}\varrho^{\Gamma{-}1}
  =\left[\left(1-\frac{1}{\gamma}\right)
    \left/\left(1-\frac{1}{\Gamma}\right)\right.\right]\,h\; .
\end{equation}
Obviously, for $\Gamma=\gamma$, the pseudo enthalpy $\tilde{h}$ is
identical to $h$ itself.
Instead of specifying $\Gamma$, one usually defines the polytropic
index $m=1/(\Gamma{-}1)$. Thus, $\Gamma=1+1/m$, and
\begin{equation}\label{hhtilde}
  \tilde{h}=(m{+}1)\left(1-\frac{1}{\gamma}\right)\,h
\end{equation}

This is consistent with a fixed entropy dependence,
where $s$ only depends on $\varrho$ like
\begin{equation}
  \frac{s}{c_p} = \left( \frac{\Gamma}{\gamma} - 1 \right)
                        \ln\frac{\varrho}{\varrho_0} \; ,
\end{equation}
and implies that
\begin{equation}
  \ln \frac{c_{\rm s}^2}{c_{\rm s0}^2}
  = (\Gamma{-}1) \ln\frac{\varrho}{\varrho_0} \; .
\end{equation}

For hydrostatic equilibrium we require
$\tilde{h}+\Phi=\tilde{h}_0=\const$.
For gravity potentials that vanish at infinity, we can have
$\tilde{h}_0\neq0$, i.e.\ a finite pseudo enthalpy at infinity.
For $g_z=-1$ or $g_z=-z$, this is not the case, so we put
$\tilde{h}_0=0$, and therefore $\tilde{h}=-\Phi$.
Using $c_{\rm s}^2=(\gamma{-}1)h$ together with (\ref{hhtilde}) we find
\begin{equation}
  c_{\rm s}^2 = -\frac{\gamma}{m{+}1}\,\Phi.
\end{equation}
In order that $\varrho=\varrho_0$ and $c_{\rm s}^2=c_{\rm s0}^2$ at
$z=z_{\rm ref}$, we have to choose (remember that $g_z$ is normally negative!)
\begin{equation}\label{zinfty}
  z_\infty = z_{\rm ref} + (m{+}1) \frac{c_{\rm s0}^2}{\gamma(-g_z)}
  \quad\mbox{for \var{grav_profile}='const'},
\end{equation}
and
\begin{equation}\label{zinfty2}
  z_\infty^2
  = z_{\rm ref}^2
    + (m{+}1) \frac{c_{\rm s0}^2}{{\textstyle\frac{1}{2}}\gamma\nu_{\rm g}^2}
  \quad\mbox{for \var{grav_profile}='linear'}.
\end{equation}
Thus, when using \code{initlnrho='polytropic\_simple'} we calculate
\begin{equation}
  \ln \frac{c_{\rm s}^2}{c_{\rm s0}^2}
  =\ln\left[-\frac{\gamma\Phi}{(m{+}1)c_{\rm s0}^2}\right]
\end{equation}
and so the stratification is given by
\begin{equation}
  \ln\frac{\varrho}{\varrho_0}
  = m\,\ln \frac{c_{\rm s}^2}{c_{\rm s0}^2} \; ,\quad
  %
  \frac{s}{c_p}
  = \left(\frac{\Gamma}{\gamma}-1\right)
    m\,\ln \frac{c_{\rm s}^2}{c_{\rm s0}^2} \; .
\end{equation}

\subsubsection{Changing the stratification}

Natural: measure length in units of $c_{\rm s0}^2/g_z$.
Can increase stratification by moving $z_{\rm top}$ close to $z_\infty$
or, better still, keeping $z_{\rm top}=0$ and moving $z_{\rm bot}\to-\infty$.
Disadvantage: in the limit of weak stratification, the box size will
be very small (in nondimensional units).

Box units: measure length in units of $d$.
Can increase stratification by increasing $g_z$ to
$g_{\max}$, which can be obtained by putting $z_{\rm top}=z_\infty$
in (\ref{zinfty}), so
\begin{equation}\label{gmax}
  g_{\max}=\frac{m{+}1}{\gamma}\,\frac{c_{\rm s0}^2}{z_{\rm top}-z_{\rm ref}}.
\end{equation}
For $m=1$, $\gamma=5/3$, $z_{\rm top}=1$, and $z_{\rm ref}=0$, for
example, we have $g_{\max}=6/5=1.2$.

Gravitational box units: measure speed in units of $\sqrt{g_z d}$.
The limit of vanishing stratification corresponds to
$c_{\rm s0}\rightarrow\infty$. This seems optimal if we want
to approach the Boussinesq case.

In Hurlburt et al.\ (1984), $z$ increased downward and the atmosphere
always terminated at $z=0$. In order to reproduce their case most
directly, we put $z_\infty=0$ and consider only negative values of $z$.
To reproduce their chase with a density stratification of 1:1.5, we place
the top of the model at $z=-2$ and the bottom at $z=-3$. In addition,
the reference height, $z_{\rm ref}$, is chosen to be at the top of the
box, i.e.\ $z_{\rm ref}=-2$. From Eq.~(\ref{zinfty}) we have
$c_{\rm s0}^2=\gamma(-g_z)(-z_{\rm ref})/(m+1)$. Using $(-g_z)=1$
and $m=1$ we find $c_{\rm s0}^2=\gamma$, so $c_{\rm s0}=1.291$
(for $\gamma=5/3$). Values for other combinations are listed in
Table~\ref{Tdensitycontrast}.

\begin{table}[htb]
  \begin{center}
    \caption{
      Correspondence between density contrast, top and bottom values
      of $z$, and $c_{\rm s0}$ for $(-g_z)=1$, $m=1$, and $\gamma=5/3$.
    }
    \label{Tdensitycontrast}
    \begin{tabular}{cccc}
    \toprule
$\varrho_{\rm bot}/\varrho_{\rm top}$ &
$z_{\rm bot}$ & $z_{\rm top}$ & $c_{\rm s0}$ \\
    \midrule
 1.5  & 3    & 2    & 1.291 \\
 3    & 1.5  & 0.5  & 0.645 \\
 6    & 1.2  & 0.2  & 0.408 \\
 11   & 1.1  & 0.1  & 0.289 \\
 21   & 1.05 & 0.05 & 0.204 \\
    \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\subsubsection{The Rayleigh number}

In Ref.~\cite{BJNRST96} the Rayleigh number is defined as
\begin{equation}\label{Radef}
  \Ra = {gd^4\over\overline{\nu}\;\overline{\chi}}
\left(-{\mbox{d}s/c_p\over\mbox{d}z}\right)_{\rm hydrostat},
\end{equation}
where the (negative) entropy gradient was evaluated in the middle
of the box for the associated hydrostatic reference solution, and
$\overline\chi=K/(\overline\rho c_p)$ and either $\overline\nu=\nu$
(if $\nu$ was assumed constant) or $\overline\nu=\mu/\overline\rho$
(if $\mu$ was assumed constant). Note that $\overline\rho$ is the average
mass in the box per volume, which is conserved. For a polytrope we have
\begin{equation}\label{sgradient}
\left(-{\mbox{d}s/c_p\over\mbox{d}z}\right)_{\rm hydrostat}
=\left[1-(m+1)\left(1-{1\over\gamma}\right)\right]
{1\over z_{\infty}-z_{\rm m}},
\end{equation}
where $z_{\rm m}=(z_1+z_2)/2$. This factor was also present in
the definition of Hurlburt et al.\ \cite{HTM84}, but their definition
differs slightly from Eq.~(\ref{Radef}), because they normalized the
density not with respect to the average value (which is constant for
all times), but with respect to the value at the top of the initial
hydrostatic solution. Since the Rayleigh number is
proportional to $\rho^2$, their definition included the extra factor
$[(z_{\infty}-z_{\rm m})/d]^2$. Therefore
\begin{equation}\label{Rdef}
  \Ra_{\rm HTM}=\left({z_{\infty}-z_{\rm m}\over d}\right)^{2m}
  \left({\rho_{\rm top}\over\overline\rho}\right)^2\Ra
\end{equation}
In the first model of Hurlburt et al.\ (1984), the Rayleigh
number, $\Ra_{\rm HTM}$, was chosen to be 310 times supercritical,
and the critical Rayleigh number was around 400, so
$\Ra_{\rm HTM}=1.25\times10^5$. In their model the density contrast
was 1:1.5 and $m=1$. This turns out to correspond to $\Ra=4.9\times10^4$,
$F_{\rm bot}=0.0025$, and $K=0.002$.

Another model that was considered by Hurlburt \& Toomre (1988) had
$\Ra_{\rm HTM}=10^5$, a density contrast of 11, and had a vertical imposed
magnetic field (Chandrasekhar number $Q=72$). This corresponds to
$\Ra=3.6\times10^8$, $K=0.0011$, $F_{\rm bot}=0.0014$.

\subsubsection{Entropy boundary condition}

This discussion only applies to the case of convection in a slab.
A commonly used lower boundary condition is to prescribe the
radiative flux at the bottom, i.e.\ $F_{\rm bot}=-K\dd T/\dd z$.
Assuming that the density in the ghost zones has already been
updated, we can calculate the entropy gradient from
\begin{equation}\label{Fbot}
F_{\rm bot}=-{K\over c_p}{c_{\rm s}^2\over\gamma-1}\left(
(\gamma-1){\dd\ln\varrho\over\dd z}+\gamma{\dd s/c_p\over\dd z}\right),
\end{equation}
which gives
\begin{equation}\label{Fbot2}
{\dd s/c_p\over\dd z}=-{\gamma-1\over\gamma}\left(
c_p{F_{\rm bot}\over K c_{\rm s}^2}+{\dd\ln\varrho\over\dd z}\right)
\end{equation}
for the derivative of the entropy at the bottom.
This is implemented as the \option{c1} boundary condition at the bottom.

\subsubsection{Temperature boundary condition at the top}

In earlier papers the temperature at the top was set in terms of the
quantity $\xi_0$, which is the ratio of the pressure scale height relative
to the depth of the unstable layer. Expressed in terms of the sound speed
at the top we have
\begin{equation}\label{cs2top}
c_{\rm s,top}^2=\gamma\xi_0 gd.
\end{equation}
\begin{equation}\label{cs2bot}
c_{\rm s,bot}^2=\left(\xi_0+{1\over m+1}\right)\gamma gd.
\end{equation}

\begin{table}[htb]
  \begin{center}
    \caption{
      Correspondence between $\xi_0$ and $c_{\rm s,bot}^2$ in single
      layer polytropes.
    }
    \label{Txi0}
    \begin{tabular}{rr}
    \toprule
$\xi_0$ & $c_{\rm s,bot}^2$ \\
    \midrule
10.00 & 17.500 \\
 0.20 &  1.167 \\
 0.10 &  1.000 \\
 0.05 &  0.917 \\
 0.02 &  0.867 \\
    \bottomrule
    \end{tabular}
  \end{center}
\end{table}

% ---------------------------------------------------------------------- %

\subsection{Planet solution in the shearing box}
\index{Planet solution}
\label{S-planet}

In order to test the setup for accretion discs and the sliding periodic
shearing sheet boundary condition, a useful initial condition is the
so-called planet solution of Goodman, Narayan, \& Goldreich \cite{GNG87}.

Assume $s=0$ (isentropy), so the equations in 2-D are
\begin{equation}\label{ux-eqn}
  u_x u_{x,x} + (u_y^{(0)}+u_y) u_{x,y} = 2\varOmega u_y - h_{,x}
\end{equation}
\begin{equation}\label{uy-eqn}
  u_x u_{y,x} + (u_y^{(0)}+u_y) u_{y,y} = -(2-q)\varOmega u_x - h_{,y}
\end{equation}
where $u_y^{(0)}=-q\varOmega x$.
Express ${\bf u}$ in terms of a stream function, so
${\bf u}=\curl(\psi\hat{\bf z})$, or
\begin{equation}\label{u-streamfunction}
  u_x = \psi_{,y},\quad u_y = -\psi_{,x}.
\end{equation}
Ansatz for enthalpy
\begin{equation}\label{u-ansatz}
  h={\textstyle{1\over2}}\delta^2\varOmega^2(R^2- x^2 - \epsilon^2 y^2
  - z^2/\delta^2)
\end{equation}
\begin{equation}\label{u-ansatz2}
  \psi=-{\textstyle{1\over2}}\sigma\varOmega(R^2- x^2 - \epsilon^2 y^2)
       -{\textstyle{1\over2}}q\varOmega x^2
\end{equation}
This implies
\begin{equation}\label{u-streamfunction2}
  u_x = \sigma\varOmega\epsilon^2 y,\quad u_y = (q-\sigma)\varOmega x
\end{equation}
and $u_{x,x}=u_{y,y}=0$.
Inserting into Eqs~(\ref{ux-eqn}) and (\ref{uy-eqn}) yields
\begin{equation}\label{ux-eqn-insert1}
  (-q+q-\sigma)\sigma\epsilon^2 = 2(q-\sigma)+\delta^2
\end{equation}
\begin{equation}\label{uy-eqn-insert1}
  \sigma(q-\sigma) = -(2-q)\sigma+\delta^2
\end{equation}
where we have already canceled common $\varOmega^2$ factors in both equations
and common $\epsilon^2$ factors in the last equation.
Simplifying both equations yields
\begin{equation}\label{ux-eqn-insert2}
  -\sigma^2\epsilon^2 = 2(q-\sigma)+\delta^2
\end{equation}
\begin{equation}\label{uy-eqn-insert2}
  -\sigma^2 = -2\sigma+\delta^2
\end{equation}
The second equation yields
\begin{equation}\label{ux-eqn-elim1}
  \delta^2=(2-\sigma)\sigma
\end{equation}
and subtracting the two yields
\begin{equation}\label{ux-eqn-elim2}
  \sigma^2=2q/(1-\epsilon^2)
\end{equation}

\begin{table}[htb]
  \begin{center}
    \caption{
      Dependence of $\epsilon$ and $\delta$ on $\epsilon$.
    }
    \label{Tplanetsoltn}
    \begin{tabular}{lcc}
    \toprule
$\epsilon$ & $\sigma$ & $\delta$ \\
    \midrule
 0.1 & 1.74 & 0.67 \\
 0.2 & 1.77 & 0.64 \\
 0.3 & 1.82 & 0.58 \\
 0.4 & 1.89 & 0.46 \\
 0.48& 1.97 & 0.22 \\
 0.5 &  2   &  0   \\
    \bottomrule
    \end{tabular}
  \end{center}
\end{table}


% ====================================================================== %

\section{Special techniques}
\label{S-techinques}

% --------------------------------------------------------------------- %

\subsection{Remeshing (regridding)}
\index{Remeshing}
\index{Regridding}
\label{S-remesh}
[This should be written up in a better way and put somewhere else.
But currently, remeshing is only available for the Pencil developers
anyway.]

Suppose you have a directory run_64 with a $64^3$ run (running on
$N_0=$\var{ny}$\times$\var{nz}=$2\times1$ CPUs) that you want to
continue from \file{VAR1} at $128^3$ (on
\var{ny}$\times$\var{nz}=$4\times4$ CPUs).
\begin{enumerate}
\item Get the remeshing stuff from repository:
  \begin{alltt}
  \prompt{unix> } cd $PENCIL_HOME; cvs co -d remesh pencil-remesh
  \prompt{unix> } setenv PATH $\{PATH\}:$PENCIL_HOME/remesh/bin
  \end{alltt}
  % a $ for font-lock

\item Create another run directory with current \file{VAR1} as
  \file{var.dat} (remesh.csh so far only works with \file{var.dat}):
  \begin{alltt}
  \prompt{unix> } cd run_64
  \prompt{run_64> } reprodir ../tmp_64 {\sl or } new tmp_64
  \prompt{run_64> } mkdir -p ../tmp_64/data {\sl or } (cd ../tmp_64/; crtmp)
  \prompt{run_64> } (cd ../tmp_64/data ; mkproc-tree \(N_0\))
  \prompt{run_64> } restart-new-dir-VAR ../tmp_64 1
  \end{alltt}

\item Create the new run directory:
  \begin{alltt}
  \prompt{run_64> } cd ../tmp_64
  \prompt{tmp_64> } reprodir -l ../run_128 {\sl or } new run_128
  \prompt{tmp_64> } vi ../run_128/src/cparam.local
  {\sl{}# set nxgrid=128, ncpus=16, nprocy=4}
  \prompt{tmp_64> } (cd ../run_128; crtmp; setup-src; make)
\end{alltt}

\item Setup and do remeshing
  \begin{alltt}
  \prompt{tmp_64> } setup-remesh
  \prompt{tmp_64> } vi remesh/common.local
  {\sl{}# set muly=2, mulz=4, remesh\_par=2}
  \prompt{tmp_64> } (cd remesh; make)
  \prompt{tmp_64> } vi remesh.in
  {\sl{}# Replace line by ../run\_128}
  \prompt{tmp_64> } remesh.csh
  {\sl{}# Answer `yes'}  
  \end{alltt}
\end{enumerate}


% ====================================================================== %

\section{Runs and reference data}
\label{S-ref-data}

For reference purposes we document here some results obtained with various
samples of the code.

% ---------------------------------------------------------------------- %

\subsection{Shock tests}

\subsubsection{Sod shock tube problem}

\begin{table}[htb]
  \begin{center}
    \caption{
        Combinations of $\rho$, $p$, and $s/c_p$ that are relevant
        for the Sod shock tube problem with constant temperature
        and different pressure ratios on the left and right hand
        sides of the shock.
    }
    \label{Tshock1}
    \begin{tabular}{lll}
    \toprule
$\rho$  &  $p$  &  $s$  \\
    \midrule
   1.0  &  1.0  &  0.3065  \\
   0.1  &  0.1  &  1.2275  \\
   0.01 &  0.01 &  2.1486  \\
    \bottomrule
    \end{tabular}
  \end{center}
\end{table}

\subsubsection{Temperature jump}

\begin{table}[htb]
  \begin{center}
    \caption{
        Combinations of $c_{\rm s}^2$, $p$, and $s/c_p$ that are
        relevant for the temperature shock problem with constant density,
        $\varrho=1$, and different temperature ratios on the left and
        right hand sides of the shock.
    }
    \label{Tshock2}
    \begin{tabular}{lll}
    \toprule
$c_{\rm s}^2$  &  $s$  \\
    \midrule
   1.0    &$ 0.0$ \\
   0.1    &$-2.3$ \\
   0.01   &$-4.6$ \\
$10^{-4}$ &$-9.2$ \\
    \bottomrule
    \end{tabular}
  \end{center}
\end{table}

% ---------------------------------------------------------------------- %

\subsection{Random forcing function}
\label{SRandomForcingFunction}

A solenoidal random forcing function $f$ can be invoked by putting
\code{iforce='helical'} in the \name{forcing_run_pars} namelist.
This produces the forcing function $\fv$ of the form
\begin{equation}
\fv(\xv,t)=\mbox{Re}\{N\fv_{\kv(t)}\exp[i\kv(t)\cdot\xv+i\phi(t)]\},
\end{equation}
where $\kv(t)=(k_x,k_y,k_z)$ is a time dependent wave vector,
$\xv=(x,y,z)$ is position, and $\phi(t)$ with $|\phi|<\pi$ is
a random phase. On dimensional grounds the normalization factor
is chosen to be
$N=f_0 c_{\rm s}(kc_{\rm s}/\delta t)^{1/2}$, where $f_0$ is a
nondimensional factor, $k=|\kv|$, and $\delta t$ is the length of
the timestep.
The $\delta t^{-1/2}$ dependence ensures that the forcing,
which is delta-correlated in time, is properly normalized such
that the correlator of the forcing function is independent of the
length of the time step, $\delta t$.
We focus on the case where $|\kv|$ is around 5, and
select at each timestep randomly one of the 228 possible vectors in
$4.5<|\kv|<5.5$. We force the system with eigenfunctions of the curl
operator,
\begin{equation}
\fv_{\kv}={i\kv\times(\kv\times\ev)-\sigma|\kv|(\kv\times\ev)
\over\sqrt{1+\sigma^2}\;\kv^2\sqrt{1-(\kv\cdot\ev)^2/\kv^2}},
\end{equation}
where $\ev$ is an arbitrary unit vector needed in order
to generate a vector $\kv\times\ev$ that is perpendicular
to $\kv$. Note that $|\fv_{\kv}|^2=1$ and, for $\sigma=1$,
$i\kv\times\fv_{\kv}=|\kv|\fv_{\kv}$, so the helicity density of this
forcing function satisfies
\begin{equation}
\fv\cdot\nab\times\fv=|\kv|\fv^2>0\quad(\mbox{for $\sigma=1$})
\end{equation}
at each point in space. We note that since the forcing function is like
a delta-function in $\kv$-space, this means that all points of $\fv$
are correlated at any instant in time, but are different at the next
timestep. Thus, the forcing function is delta-correlated in time (but
the velocity is not). This is the forcing function used in Brandenburg
(2001), Brandenburg \& Dobler (2001), and other papers in that series.

For $\sigma=0$, the forcing function is completely nonhelical
and reduces to the simpler form
\begin{equation}
\fv_{\kv}=\left(\kv\times\ev\right)/\sqrt{\kv^2-(\kv\cdot\ev)^2}.
\end{equation}
For $0<|\sigma|<1$, the forcing function has fractional helicity, where
$\sigma\approx\left<\vekt{\omega}\cdot\uv\right>/(k_{\rm f}
\left<\uv^2\right>)$; see Sect.~4.5 of Ref.~\cite{BDS02}.
In the code and the \name{forcing_run_pars} namelist,
$\sigma$ is called \var{relhel}.

% ---------------------------------------------------------------------- %

\subsection{Three-layered convection model}

% ---------------------------------------------------------------------- %
\begin{figure}[hbtp]
  \centering
  \includegraphics%
    [width=0.8\textwidth,keepaspectratio]%
    {pvert2}
  \caption{Like in Fig.~\ref{Fig-pvert1}, but at time $t=50$.
  }
  \label{Fig-pvert2}
\end{figure}
% ---------------------------------------------------------------------- %

In Sect.~\ref{S-getting-started} we have shown the early stages of the
convection model located in \file[conv-slab/]{samples/conv-slab}.
To arrive at fully developed convection, you will need to run the code for
many more time steps.
Figure~\ref{Fig-pvert2} shows the vertical profiles of four basic
quantities at time $t=50$.
Figure~\ref{Fig-evol} shows the time evolution of rms and maximum
velocity for the model for $0<t<50$.

% ---------------------------------------------------------------------- %
\begin{figure}[hbtp]
  \centering
  \includegraphics%
    [width=0.6\textwidth,keepaspectratio]%
    {evol}
  \caption{Time evolution of rms and maximum velocity for the model
    \file[conv-slab/]{samples/conv-slab}.
    Similar plots can be produced by running the IDL script
    \file{ts.pro}.
  }
  \label{Fig-evol}
\end{figure}
% ---------------------------------------------------------------------- %

Figures~\ref{Fig-hsection2} and \ref{Fig-vsection2} show vertical and
horizontal sections for time $t=50$.

% ---------------------------------------------------------------------- %
\begin{figure}[hbtp]
  \centering
  \includegraphics%
    [width=0.75\textwidth,height=0.65\textheight,keepaspectratio]%
    {hsect2}
  \caption{Horizontal sections for $t=50$.
    Top: velocity field.
    Bottom: entropy (color coded) and density (isocontours).
    Plots of this type can be produced by running the IDL script
    \file{hsections.pro})
  }
  \label{Fig-hsection2}
\end{figure}
% ---------------------------------------------------------------------- %

% ---------------------------------------------------------------------- %
\begin{figure}[hbtp]
  \centering
  \includegraphics%
    [width=0.75\textwidth,height=0.65\textheight,keepaspectratio]%
    {vsect2}
  \caption{Vertical section $y=0.516$ at $t=50$.
    Top: velocity field.
    Bottom: entropy (color coded) and density (isocontours).
    Plots of this type can be produced by running the IDL scripts
    \file{vsections.pro}) or \file{vsections2.pro}).
  }
  \label{Fig-vsection2}
\end{figure}
% ---------------------------------------------------------------------- %


% ====================================================================== %

\section{Numerical methods}

\subsection{Sixth-order spatial derivatives}
\index{6th-order derivatives}
\index{Sixth-order derivatives}
\label{S-6th-order}

Spectral methods are commonly used in almost all studies of ordinary
(usually incompressible) turbulence. The use of this method is justified
mainly by the high numerical accuracy of spectral schemes. Alternatively,
one may use high order finite differences that are faster to compute
and that can possess almost spectral accuracy.  Nordlund \& Stein \cite{NS90}
and Brandenburg et al.\ \cite{BNST95} use high order finite difference
methods, for example fourth and sixth order compact schemes
\cite{Lele92}.\footnote{The fourth order compact scheme is really identical to
calculating derivatives from a cubic spline, as was done in Ref.~\cite{NS90}.
In the book by Collatz \cite{Collatz66} the compact methods are also
referred to as {\it Hermitian methods} or as {\it Mehrstellen-Verfahren},
because the derivative in one point is calculated using the derivatives
in neighboring points.}

The sixth order first and second derivative schemes are given by
\begin{equation}
f'_i=(-f_{i-3}+9f_{i-2}-45f_{i-1}
+45f_{i+1}-9f_{i+2}+f_{i+3})/(60\delta x),
\end{equation}
\begin{equation}
f''_i=(2f_{i-3}-27f_{i-2}+270f_{i-1}-490f_i
+270f_{i+1}-27f_{i+2}+2f_{i+3})/(180\delta x^2),
\end{equation}

In Fig.~\ref{Fpkeff} we plot effective wavenumbers for different schemes.
Apart from the different {\it explicit} finite difference schemes
given above, we also consider a {\it compact} scheme of 6th order,
which can be written in the form
\begin{equation}
{\textstyle{1\over3}}f'_{i-1}+f'_i+{\textstyle{1\over3}}f'_{i+1}
=(f_{i-2}-28f_{i-1}+28f_{i+1}-f_{i+2})/(36\delta x),
\end{equation}
for the first derivative, and
\begin{equation}
{\textstyle{2\over11}}f''_{i-1}+f''_i+{\textstyle{2\over11}}f''_{i+1}
=(3f_{i-2}+48f_{i-1}-102f_i+48f_{i+1}+3f_{i+2})/(44\delta x^2).
\end{equation}
for the second derivative. As we have already mentioned in the introduction, this
scheme involves obviously solving tridiagonal matrix equations and is
therefore effectively nonlocal.

\begin{figure}[h!]\begin{center}\includegraphics[width=.99\textwidth]
{pkeff}\end{center}\caption[]{
Effective wave numbers for first and second derivatives using different
schemes. Note that for the second derivatives the sixth order compact
scheme is almost equivalent to the tenth order explicit scheme. For the
first derivative the sixth order compact scheme is still superior to the
tenth order explicit scheme.
}\label{Fpkeff}\end{figure}

In the second panel of Fig.~\ref{Fpkeff} we have plotted effective
wavenumbers for second derivatives, which were calculated as
\begin{equation}
(\cos kx)''_{\rm num}=-k_{\rm eff}^2\cos kx.
\end{equation}
Of particular interest is the behavior of the second derivative at the
Nyquist frequency, because that is relevant for damping zig-zag modes.
For a second-order finite difference scheme $k_{\rm eff}^2$ is only 4,
which is less than half the theoretical value of $\pi^2=9.87$. For fourth,
sixth, and tenth order schemes this value is respectively 5.33, 6.04,
6.83. The last value is almost the same as for the 6th order compact
scheme, which is 6.86. Significantly stronger damping at the Nyquist
frequency can be obtained by using hyperviscosity, which Nordlund \&
Galsgaard (1995) treat as a quenching factor that diminishes the value
of the second derivative for wavenumbers that are small compared with
the Nyquist frequency. Accurate high order second derivatives (with no
quenching factors) are important when calculating the current $\Jv$ in
the Lorentz force $\Jv\times\Bv$ from a vector potential $\Av$ using
$-\mu_0\Jv=\nabla^2\Av-\nab\nab\cdot\Av$. This will be important in
the MHD calculations presented below.

\subsection{Upwind derivatives to avoid `wiggles'}
\index{Upwinding}
\label{S-upwind}
\newcommand{\Order}[1]{O\left(#1\right)}

High-order centered-difference convection simulations often show
``wiggles'' (Nyquist zigzag pattern) in $\ln\varrho$, which are apparently
caused by a velocity profile where the velocity approaches zero on the
boundary or inside the box.\footnote{%
  A simple one-dimensional test profile would be $u(x) = 1-x^2$ on $x \in
  [-1,1]$, which will accumulate more and more mass near the right
  boundary $x=1$.

  In two- or three-dimensional settings, the presence of stagnation points
  of X-type leads to the same configuration, this time with the
  possibility of a steady state (i.e.~without accumulation of mass).
  Such stagnation points occur e.g.~at the top of an upwelling, or at
  the bottom of a downdraft in convection simulations, where locally
  $u_z \propto z_{\rm X}-z$.
}
This causes the density profile to be squeezed into a boundary layer where
eventually numerical resolution is insufficient and, for centered
differences, a spurious Nyquist signal is generated that almost
instantaneously propagates into much of the whole box.

Even if the stagnation point is on the boundary (and enforced by the
boundary conditions), this behavior is hardly influenced by the boundary
conditions on $\ln\varrho$ at all.
A solution, however, is to apply upwinded derivative operators.
The simplest upwind derivative is a finite-difference derivative operator
where the point furthest downwind is excluded from the stencil.
For $u>0$, that means that instead of
\begin{equation}
  f'_0
  = \frac{-f_{-3} + 9 f_{-2} - 45 f_{-1} \qquad
           + 45 f_{1} - 9 f_{2} + f_{3}}
         {60\,\delta x}
    - \frac{\delta x^6\,f^{(7)}}{140}
  = D^{\rm(cent,6)} + \Order{\delta x^6} \; ,
\end{equation}
one takes
\begin{equation}
  f'_0
  = \frac{-2 f_{-3} + 15 f_{-2} - 60 f_{-1} + 20 f_{0} + 30 f_{1} - 3 f_{2}}
         {60\,\delta x}
    + \frac{\delta x^5\,f^{(6)}}{60}
  = D^{\rm(up,5)} + \Order{\delta x^5} \; .
\end{equation}
A fourth-order upwind scheme (excluding two downwind points) would be
\begin{equation}
  f'_0
  = \frac{-f_{-3} + 6 f_{-2} - 18 f_{-1} + 10 f_{0} + 3 f_{1}}
         {12\,\delta x}
    - \frac{\delta x^4\,f^{(5)}}{20}
  = D^{\rm(up,4)} + \Order{\delta x^4} \; .
\end{equation}
The effect of upwinding is mostly to stop the Nyquist perturbations from
spreading away from the boundary or stagnation point.
With the fourth-order formula they actually hardly ever develop.

The difference between central and fifth-order upwind derivative is
\begin{equation}
  [D^{\rm(up,5)} - D^{\rm(cent,6)}] f_0
  = \frac{-f_{-3} + 6 f_{-2} - 15 f_{-1} + 20 f_{0}
          - 15 f_{1} + 6 f_{2} - f_{3}}
         {60\,\delta x}
  = -\frac{\delta x^5}{60} f^{(6)}_{0} \; .
\end{equation}
In other words, 5th-order upwinding can be represented for any sign of $u$
as hyper-diffusion:
\begin{equation} \label{Eq-upwind-hyperdiff}
  -u f'_{\rm(up,5th)}
  = -u f'_{\rm(centr,6th)}  + \frac{|u|\,\delta x^5}{60} f^{(6)} \; .
\end{equation}
The advantage over adopting constant hyperdiffusion is that in the
upwinding scheme hyperdiffusion is only applied where it is needed
(i.e.~where advection is happening, hence the factor $|u|$).

The form (\ref{Eq-upwind-hyperdiff}) also suggests an easy way to get
`stronger' upwinding: Rather than excluding more points in the downwind
direction, we can simply treat the weight of the hyperdiffusion term as a
free parameter $\alpha$:
\begin{equation}
  -u f'_{\rm(up,5th,\alpha)}
  = -u f'_{\rm(centr,6th)}  + \alpha\,|u|\,\delta x^5 f^{(6)} \; .
\end{equation}
If $\alpha$ is large, this may affect the time step, but for
$\alpha=1/60$, the stability requirement for the hyperdiffusive term
should always be weaker than the advective Courant criterion.

\subsection{The bidiagonal scheme for cross-derivatives}
\index{Bidiagonal scheme}
\label{Bidiagonal}

[Wolfgang to describe the theory.]

\begin{verbatim}
      df=fac*( &
                  270.*( f(l1+1:l2+1,m+1,n,k)-f(l1-1:l2-1,m+1,n,k)  &
                        +f(l1-1:l2-1,m-1,n,k)-f(l1+1:l2+1,m-1,n,k)) &
                 - 27.*( f(l1+2:l2+2,m+2,n,k)-f(l1-2:l2-2,m+2,n,k)  &
                        +f(l1-2:l2-2,m-2,n,k)-f(l1+2:l2+2,m-2,n,k)) &
                 +  2.*( f(l1+3:l2+3,m+3,n,k)-f(l1-3:l2-3,m+3,n,k)  &
                        +f(l1-3:l2-3,m-3,n,k)-f(l1+3:l2+3,m-3,n,k)) &
             )
\end{verbatim}

Off-diagonal terms enter not only the diffusion terms through
$\nab\nab\cdot\uv$ and $\nab\nab\cdot\Av$ terms, but also through
the $\Jv=\nab\times\nab\times\Av$ operator.
The general formula is $J_i=A_{j,ij}-A_{i,jj}$, so in 2-D in the
$xy$-plane we have
\begin{equation}
J_x=A_{x,xx}+A_{y,xy}-A_{x,xx}-A_{x,yy}=A_{y,xy}-A_{x,yy}
\end{equation}
\begin{equation}
J_y=A_{x,yx}+A_{y,yy}-A_{y,xx}-A_{y,yy}=A_{x,yx}-A_{y,xx}
\end{equation}

\begin{figure}[h!]\begin{center}
\includegraphics[width=\columnwidth]{pcompbidiagonal}
\end{center}\caption[]{
Alfv\'en wave for $\Bv_0=(1,2,0)$ and $\kv=(1,2,0)$ after $t=2\pi$.
The wave travels in the direction of $\kv$.
The red symbols are for the bidiagonal scheme.
Already for $16^2$ mesh points there are no clear differences.
For $8^2$ mesh points both schemes are equally imprecise regarding
the phase error, but the amplitude error is still quite small
(but this is mainly a property of the time stepping scheme).
}\label{pcompbidiagonal}\end{figure}

\begin{equation}
\dot{u}_z=J_xB_{0y}-J_yB_{0x}
\end{equation}
\begin{equation}
\dot{A}_x=-u_zB_{0y}
\end{equation}
\begin{equation}
\dot{A}_y=+u_zB_{0x}
\end{equation}
Initial condition (as implemented in \texttt{subroutine alfven\_xy}:
\begin{equation}
u_z\sim\cos(k_xx+k_yy-\omega t)
\end{equation}
\begin{equation}
A_x\sim+B_{0y}\sin(k_xx+k_yy-\omega t)/\omega
\end{equation}
\begin{equation}
A_y\sim-B_{0x}\sin(k_xx+k_yy-\omega t)/\omega
\end{equation}
where $\omega=\kv\cdot\Bv_0$.

\subsection{The 2N-scheme for time-stepping}
\index{2N-scheme}
\label{S-2N-scheme}

For time stepping, higher-order schemes are necessary in order to reduce
the amplitude and phase errors of the scheme and, to some extent, to allow
longer time steps.
Usually such schemes require large amounts of memory.
However, we here use the memory-effective $2N$-schemes that require only
two sets of variables to be held in memory.
Such schemes work for
arbitrarily high order, although not all Runge-Kutta schemes can be
written as $2N$-schemes \cite{2Nstorage,SH88}.
Consider the ordinary differential equation (ODE)
\begin{equation}
  \dot{u} = F(u,t) \; ,
\end{equation}
which can also be used as a prototype for a system of ODEs to be solved,
like the ones obtained by spatial discretization of PDEs.
The $2N$-schemes construct an approximation to the solution
\begin{equation}
  u^{(n)} \equiv u(t_n)
\end{equation}
according to the iterative procedure
\begin{eqnarray}
  w_i &=& \alpha_i w_{i-1}+\delta t\,F(u_{i-1},t_{i-1}) \;, \\
  u_i &=& u_{i-1}+\beta_i w_i \;.
\label{iterform0}
\end{eqnarray}
For a three-step scheme we have $i=1,...,3$.
In order to advance the variable $u$ from $u^{(n)}$ at time $t^{(n)}$
to $u^{(n+1)}$ at time $t^{(n+1)}=t^{(n)}+\delta t$ we set in \Eq{iterform0}
\begin{equation}
  u_0=u^{(n)}
  \quad\mbox{and, after the last step,}\quad
  u^{(n+1)}=u_3,
\end{equation}
with $u_1$ and $u_2$ being intermediate steps. In order to be able to
calculate the first step, $i=1$, for which no $w_{i-1}\equiv w_0$ exists,
we have to require $\alpha_1=0$. Thus, we are left with 5 unknowns,
$\alpha_2$, $\alpha_3$, $\beta_1$, $\beta_2$, and $\beta_3$. Three
conditions follow from the fact that the scheme be third order
for linear equations, so we
have to have two more conditions. One possibility is to choose the
fractional times at which the right hand side is evaluated, for
example $(0,1/3,2/3)$ or even $(0,1/2,1)$.
%wd: Don't see the point of the following: 
% In the latter case the right hand
% side is evaluated twice at the same time. It is therefore some sort of
% predictor-corrector scheme.
Yet another possibility is to require that
inhomogeneous equations of the form $\dot{u}=t^n$ with $n=1$ and 2 are
solved exactly.
The corresponding coefficients are listed in Table~\ref{T-2N-RK3} and compared
with those given by Williamson \cite{2Nstorage}. In practice all of them
are about equally good when it comes to real applications, although
we found the first one in Table~\ref{T-2N-RK3} (`symmetric') marginally better in some
simple test problems where an analytic solution was known.
In Ref.~\cite{Ref-1} the accuracy of some nonlinear equations is tested.

\begin{table}[htb]
  \centering
  \caption{
    Coefficients for different $2N$-type third-order Runge-Kutta
    schemes.
    The coefficients $c_i$ (which are determined by the $\alpha_i$,
    $\beta_i$) give the time for each substep,
    $t_i = t_0 + c_i \delta t$
    }\label{T-2N-RK3}
  \vspace{12pt}
  \begin{tabular}{l@{\quad}ccc@{\quad}cc@{\quad}ccc}
    \toprule
    scheme            & $c_1$
                            & $c_2$
                                    & $c_3$
                                              & $\alpha_2$
                                                         & $\alpha_3$
                                                                    & $\beta_1$
                                                                            &$\beta_2$
                                                                                    & $\beta_3$\\
    \midrule
    symmetric         & $0$ & $1/3$ & $2/3$   &  $-2/3$  &   $-1$   & $1/3$ &  $1$  & $1/2$ \\
    {}[predictor/corrector]
                      & $0$ & $1/2$ & $1$     &  $-1/4$  &  $-4/3$  & $1/2$ & $2/3$ & $1/2$ \\
    inhomogeneous     & $0$ & $15/32$ & $4/9$ & $-17/32$ & $-32/27$ & $1/4$ & $8/9$ & $3/4$ \\
    Williamson (1980) & $0$ & $4/9$ & $15/32$ &  $-5/9$  &$-153/128$& $1/3$ &$15/16$& $8/15$\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Ionization}
\index{Ionization}
\label{S-Ioni}

The specific entropy of each particle species (neutral hydrogen, electrons, protons and neutral helium) may be written as
\begin{equation}
\frac{s_i}{s_0}=x_i\left(\ln\left[\frac{1}{x_{\rm tot}}
\frac{\rho_i}{\rho}\left(\frac{T}{T_0}\right)^{3/2}\right]
+\frac{5}{2}\right)\ ,
\end{equation}
where
\begin{equation}
x_{\rm H}=1-y\ ,\quad x_{\rm e}=x_{\rm p}=y\ ,\quad x_{\rm tot}=1+y+x_{\rm He}
\end{equation}
\begin{equation}
s_0=\frac{k_{\rm B}}{\mu m_{\rm H}}\ ,\quad
T_0=\frac{\chi_{\rm H}}{k_{\rm B}}\ ,
\end{equation}
and
\begin{equation}
\rho_i=\mu m_{\rm H}\left(\frac{m_i\chi_{\rm H}}{2\pi\hbar^2}\right)^{3/2}\ .
\end{equation}
The specific entropy of mixing is
\begin{equation}
\frac{s_{\rm mix}}{s_0}=-\sum_ix_i\ln\frac{x_i}{x_{\rm tot}}\ .
\end{equation}
Summing up everything, we get the total specific entropy 
\begin{align}
\frac{s}{s_0}&=\sum_i\frac{s_i}{s_0}+\frac{s_{\rm mix}}{s_0}
=\sum_ix_i\left(\ln\left[\frac{1}{x_i}\frac{\rho_i}{\rho}
\left(\frac{T}{T_0}\right)^{3/2}\right]+\frac{5}{2}\right)\\
&=\sum_ix_i\ln\frac{\rho_i}{x_i}
+x_{\rm tot}\left(\ln\left[\frac{1}{\rho}
\left(\frac{T}{T_0}\right)^{3/2}\right]+\frac{5}{2}\right)\ .
\end{align}
Solving for $T$ gives
\begin{equation}
\frac{3}{2}\ln\frac{T}{T_0}=\frac{s/s_0
+\sum_ix_i\ln x_i/\rho_i}{x_{\rm tot}}
+\ln\rho-\frac{5}{2}\ .
\end{equation}
Using this expression and the constants defined above, we may obtain the
ionization fraction $y$ for given $\ln\rho$ and $s$ by finding the root of
\begin{equation}
F=\ln\left[\frac{\rho_{\rm e}}{\rho}\left(\frac{T}{T_0}\right)^{3/2}
           \frac{1-y}{y^2}\right]-\frac{T_0}{T}\ .
\end{equation}
The derivative with respect to $y$ for Newton-Raphson is
\begin{equation}
\frac{\partial F}{\partial y}
=\left(\frac{3}{2}+\frac{T_0}{T}\right)\frac{\partial\ln T/T_0}{\partial y}
 -\frac{1}{1-y}-\frac{2}{y}\ ,
\end{equation}
where
\begin{equation}
\frac{\partial\ln T/T_0}{\partial y}
=\frac{\frac{2}{3}\left(\ln\rho_{\rm H}/\rho_{\rm p}-F-T_0/T\right)-1}
      {1+y+x_{\rm He}}\ .
\end{equation}
In order to compute the pressure gradient in the momentum equation,
the derivative of $y$ with respect
to $\ln\rho$ and $s$ needs to be known:
\begin{equation}
\frac{\partial\ln P}{\partial\ln\rho}
=\frac{1}{1+y+x_{\rm He}}\frac{\partial y}{\partial\ln\rho}
+\frac{\partial\ln T}{\partial\ln\rho}
+\frac{\partial\ln T}{\partial y}\frac{\partial y}{\partial\ln\rho}+1,
\end{equation}
\begin{equation}
\frac{\partial\ln P}{\partial s}
=\frac{1}{1+y+x_{\rm He}}\frac{\partial y}{\partial s}
+\frac{\partial\ln T}{\partial s}
+\frac{\partial\ln T}{\partial y}\frac{\partial y}{\partial s}.
\end{equation}
Since $F=0$ for all desired solutions $(y,\ln\rho,s)$ we also have
\begin{equation}
dF=\frac{\partial F}{\partial\ln\rho}{\rm d}\ln\rho
   +\frac{\partial F}{\partial s}{\rm d} s
   +\frac{\partial F}{\partial y}{\rm d} y=0\ ,
\end{equation}
and thus
\begin{equation}
\frac{\partial y}{\partial\ln\rho}
=\left(\frac{{\rm d} y}{{\rm d}\ln\rho}\right)_{{\rm d} s=0}
=-\frac{\partial F/\partial\ln\rho}{\partial F/\partial y}
\end{equation}
and
\begin{equation}
\frac{\partial y}{\partial s}
=\left(\frac{{\rm d} y}{{\rm d} s}\right)_{{\rm d}\ln\rho=0}
=-\frac{\partial F/\partial s}{\partial F/\partial y}.
\end{equation}

% --------------------------------------------------------------------- %

\subsection{Radiative transfer}
\index{Radiative transfer}
\label{S-radi-trans}

\subsubsection{Solving the radiative transfer equation}

A formal solution of Eq.~(\ref{radiative-transfer}) is given by
\begin{equation}
  I(\tau) = I(\tau_0) e^{-(\tau-\tau_0)}
            + \int\limits_{\tau_0}^\tau e^{-(\tau-\tau')} S(\tau')\,d\tau' \; .
\end{equation}
Using a generalization of the trapezoidal rule,
\begin{eqnarray}
  \int\limits_{\tau_0}^\tau e^{-(\tau-\tau')} f(\tau')\,d\tau'
  &\approx& \int\limits_{\tau_0}^\tau e^{-(\tau-\tau')}
              \left[ f(\tau_0) + \frac{f(\tau){-}f(\tau_0)}{\tau-\tau_0}\,
              (\tau'-\tau_0)\right] \; d\tau'\\
  &=&       \left[1{-}e^{-(\tau-\tau_0)}\right] f(\tau)
          - \frac{1 - e^{-(\tau-\tau_0)}(1+\tau{-}\tau_0)}{\tau-\tau_0}
            [f(\tau)-f(\tau_0)]
  \; ,
\end{eqnarray}
which is exact for linear functions $S(\tau)$, we discretize this as
\begin{eqnarray}
  I_{k+1} &=& I_k e^{-\delta\tau}
              + (1{-}e^{-\delta\tau}) S_{k+1}
              - \frac{1 - e^{-\delta\tau}(1+\delta\tau)}{\delta\tau}
                (S_{k+1}-S_k) \; , \\
          &=& I_k e^{-\delta\tau}
              + (1{-}e^{-\delta\tau}) S_{k}
              + \frac{e^{-\delta\tau}-1+\delta\tau}{\delta\tau}
                (S_{k+1}-S_k) \; .
\end{eqnarray}
Here the simplest way to calculate $\delta\tau$ is
\begin{equation}
  \delta\tau = \frac{\chi_k{+}\chi_{k+1}}{2} \, \delta x \; ;
\end{equation}
more accurate alternatives are
\begin{equation}
  \delta\tau = \sqrt{\chi_k\chi_{k+1}} \, \delta x
\end{equation}
or
\begin{equation}
  \delta\tau
  = \frac{\chi_{k+1}-\chi_k}{\ln\frac{\chi_{k+1}}{\ln\chi_k}} \, \delta x
\end{equation}


\subsubsection{Angular integration}

% ---------------------------------------------------------------------- %
\begin{table}[htb]
  \renewcommand{\arraystretch}{2.3}
  \centering
  \caption{Sums $\sqrt{4\pi}Y_l^m(\vartheta_i,\varphi_i)$ for special
    sets of directions.
    For all degrees and orders up to $l=8$ not mentioned in this table,
    the sums are $0$.
    The label `Non-h.~f-d.' stands for `non-horizontal face-diagonals',
    i.e.~the eight face diagonals that are not in the horizontal plane.
  }
  \label{Tab-Ylm-sums}
  \footnotesize
  \begin{tabular}{lr@{\qquad}r@{\qquad}rr@{\qquad}rr@{\qquad}rrr}
  \toprule
    Directions   & $Y_0^0$   & $Y_2^0$      & $Y_4^0$          & $Y_4^{\pm4}$             & $Y_6^0$                    & $Y_6^{\pm4}$                & $Y_8^0$                     & $Y_8^{\pm4}$                 & $Y_8^{\pm8}$                  \\
  \midrule
    Coord.       &  $6$      & $0$          &  $\dfrac{21}{2}$ &  $\dfrac{3}{4}\sqrt{70}$ &  $\dfrac{3}{4}\sqrt{13}$   & $-\dfrac{3}{8}\sqrt{182}$   & $\dfrac{99}{32}\sqrt{17}$   & $\dfrac{3}{32}\sqrt{2618}$   & $\dfrac{3}{64}\sqrt{24310}$   \\
    Face diag.   & $12$      & $0$          & $-\dfrac{21}{4}$ & $-\dfrac{3}{8}\sqrt{70}$ & $-\dfrac{39}{16}\sqrt{13}$ &  $\dfrac{39}{32}\sqrt{182}$ & $\dfrac{891}{256}\sqrt{17}$ & $\dfrac{27}{256}\sqrt{2618}$ & $\dfrac{27}{512}\sqrt{24310}$ \\
    Space diag.  &  $8$      & $0$          & $-\dfrac{28}{3}$ & $-\dfrac{2}{3}\sqrt{70}$ & $\dfrac{16}{9}\sqrt{13}$  & $-\dfrac{8}{9}\sqrt{182}$   & $\dfrac{11}{9}\sqrt{17}$    & $\dfrac{1}{27}\sqrt{2618}$   & $\dfrac{1}{54}\sqrt{24310}$   \\
  \midrule
    Non-h.~f-d.  &  $8$      & $2\sqrt{5}$  & $-\dfrac{39}{4}$ &  $\dfrac{3}{8}\sqrt{70}$ & $-\dfrac{19}{16}\sqrt{13}$ & $\dfrac{27}{32}\sqrt{182}$  & $\dfrac{611}{256}\sqrt{17}$ & $\dfrac{51}{256}\sqrt{2618}$ & $\dfrac{3}{512}\sqrt{24310}$  \\
    Coord.
    $x$, $y$     &  $4$      & $-2\sqrt{5}$ & $\dfrac{9}{2}$   &  $\dfrac{4}{3}\sqrt{70}$ & $-\dfrac{5}{4}\sqrt{13}$   & $-\dfrac{3}{8}\sqrt{182}$   & $\dfrac{35}{32}\sqrt{17}$   & $\dfrac{3}{32}\sqrt{2618}$   & $\dfrac{3}{64}\sqrt{24310}$   \\
    Coord. $z$   &  $2$      & $2\sqrt{5}$  & $6$              &  $0$                     & $2\sqrt{13}$               & $0$                         & $2\sqrt{17}$                & $0$                          & $0$                           \\
  \bottomrule
  \end{tabular}
\end{table}
% ---------------------------------------------------------------------- %

For angular integration over the full solid angle, we make the ansatz
\begin{equation}
  \int\limits_{4\pi} f(\vartheta,\varphi)\,\frac{d\omega}{4\pi}
  = \sum_{i=1}^{N} w_i f(\vartheta_i,\varphi_i) + R_N \; .
\end{equation}
Table \ref{Tab-Ylm-sums} shows the sums
$\sqrt{4\pi}Y_l^m(\vartheta_i,\varphi_i)$ over special sets of directions
$(\vartheta_i,\varphi_i)$.
Using these numbers and requiring that angular integration is exact for
$l\le l_{\rm max}$, we find the following weights $w_i$ for different sets
of directions (see also \cite{Abramowitz-Stegun}, \S 25.4.65):

\begin{enumerate}
\item \emph{Axes}

  \begin{tabular}{@{}lr}
    Coordinate axes: & 1/6
  \end{tabular}

  $l_{\rm max} = 3$

\item \emph{Face diagonals}

  \begin{tabular}{@{}lr}
    Face diagonals: & 1/12
  \end{tabular}

  $l_{\rm max} = 3$

\item \emph{Space diagonals}

  \begin{tabular}{@{}lr}
    Space diagonals: & 1/8
  \end{tabular}

  $l_{\rm max} = 3$

\item \emph{Axes + face diagonals}

  \begin{tabular}{@{}lr}
    Coordinate axes: & 1/30 \\
    Face diagonals:  & 1/15 \\
  \end{tabular}

  $l_{\rm max} = 5$

\item \emph{Axes + space diagonals}

  \begin{tabular}{@{}lr}
    Coordinate axes: & 1/15 \\
    Space diagonals: & 3/40 \\
  \end{tabular}

  $l_{\rm max} = 5$

\item \emph{Face + space diagonals}

  \begin{tabular}{@{}lr}
    Face diagonals:  &  2/15 \\
    Space diagonals: & -3/40 \\
  \end{tabular}

  $l_{\rm max} = 5$

\item \emph{Axes, face + space diagonals}

  \begin{tabular}{@{}lr}
    Coordinate axes: & 1/21 \\
    Face diagonals:  & 4/105 \\
    Space diagonals: & 9/280 \\
  \end{tabular}

  $l_{\rm max} = 7$

\item \emph{Axes, non-horizontal face diagonals}

  \begin{tabular}{@{}lr}
    Coordinate axes $x$, $y$: & 1/10 \\
    Coordinate axes $z$:      & 1/30 \\
    Non-hor.~face diagonals:  & 1/15 \\
  \end{tabular}

  $l_{\rm max} = 3$

\item \emph{Axes, non-horizontal face diagonals, space diagonals}

  \begin{tabular}{@{}lr}
    Coordinate axes $x$, $y$: & 12/215 \\
    Coordinate axes $z$:      & 10/129 \\
    Non-hor.~face diagonals:  & -14/645 \\
    Space diagonals:          & 171/1720 \\
  \end{tabular}

  $l_{\rm max} = 5$

\end{enumerate}


% ====================================================================== %

\section{Switchable modules}
  \label{Tab-modules}
%(AB: this is no longer a table, so I treat it as section)
%(AB: but acroread somehow still doesn't bring me to the right place)

The table below lists the available physics and technical
modules.

% ---------------------------------------------------------------------- %
%\begin{table}
\begin{center}
  \begin{small}
%  \caption{%
%    Switchable modules as of September 2002.
%  }
% \label{Tab-modules}
%  \begin{tabular}{lp{0.6\textwidth}}
  \begin{longtable}{lp{0.6\textwidth}}
    \toprule
    \file{hydro.f90}      & Hydrodynamics: add variable $\uv$ with equation of
                            motion; \\
    \file{nohydro.f90}    & no variable $\uv$: useful for kinematic dynamo
                            runs. \\
    \midrule
    \file{density.f90}    & Add variable $\ln\varrho$ with continuity
                            equation; \\
    \file{nodensity.f90}  & no variable $\ln\varrho$: useful for Burgers'
                            equation. \\
    \midrule
    \file{entropy.f90}    & Add variable $s$ with entropy equation (energy
                            equation); \\
    \file{noentropy.f90}  & no variable $s$: isothermal hydrodynamics. \\
    \midrule
    \file{magnetic.f90}   & Add variable $\Av$ (magnetic vector potential) with
                            induction equation; \\
    \file{nomagnetic.f90} & no variable $\Av$: nonmagnetic hydrodynamics. \\
    \midrule
    \file{pscalar.f90}    & Add variable $\ln c$ (concentration) with
                            advection-diffusion equation for transport of a
                            passive scalar; \\
    \file{nopscalar.f90}  & no passive scalar, no variable $\ln c$. \\
    \midrule
    \file{radiation.f90}  & Radiative transfer in the flux-limited diffusion
                            approximation.
                            \emph{Still experimental and not working correctly}. \\
    \file{noradiation.f90}& No radiative transfer. \\
    \midrule
    \file{visc_const.f90} & Constant fluid viscosity. \\
    \file{visc_shock.f90} & Constant fluid viscosity and variable shock
                            viscosity (conserves energy) based upon 
                            local strength of a shock.
                            The shock profile is calculated from the
                            velocity field.
                     \emph{Still experimental and not fully tested. }. \\
    \midrule
    \midrule
    \file{forcing.f90}    & Add a forcing function to rhs of equation of motion
                            (typically helical forcing) for forced turbulence
                            calculations; \\
    \file{noforcing.f90}  & no forcing. \\
    \midrule
    \file{shear.f90}      & Shearing-box boundary conditions; \\
    \file{noshear.f90}    & no shearing box. \\
    \midrule
    \file{gravity_simple.f90}  & Constant x, y or z gravity in equation of motion; \\
    \file{grav_r.f90}     & radially symmetric gravity; \\
    \file{nogravity.f90}     & no gravity. \\
    \midrule
    \file{fft.f}          & Singleton (1968) FFT routine: needed for magnetic
                            potential-field boundary condition; \\
    \file{nofft.f90}      & no FFT: no need to bother about compiler warnings if
                            you don't need this boundary condition. \\
    \midrule
    \file{io_dist.f90}    & Distributed input/output: each processor writes to
                            its own directory. \\
    \file{io_mpio.f90}    & Parallel input/output through \name{MPI-IO}.
                            \emph{Still experimental}. \\
    \midrule
    \file{mpicomm.f90}    & Use \name{MPI} for communication on multiprocessor
                            machines; \\
    \file{nompicomm.f90}  & no parallelization: convenient for testing and
                            smaller production runs on desktop or notebook
                            computers. \\
    \midrule
    \file{debug_c.c}      & Use I/O routines that allow to write auxiliary
                            variables to a file (nontrivial in a Pencil Code);
                            requires C compiler to be set up correctly and maybe
                            requires appropriate compiler flags
                            (\code{-DFUNDERSCORE}) for correctly interfacing C
                            with Fortran (and may still not work in some
                            cases); \\
    \file{nodebug.f90}    & don't use these debugging routines, avoiding any
                            C-Fortran interoperability problems. \\
    \bottomrule
  \end{longtable}
%  \end{tabular}
  \end{small}
\end{center}
%\end{table}
% ---------------------------------------------------------------------- %



% ====================================================================== %

\section{Startup and Run-Time Parameters}
\label{S-all-parameters}

\subsection[Startup parameters for \File{start.in}]%
{List of startup parameters for \file{start.in}}
\label{S-all-init-params}

The following table lists all (at the time of writing, September 2002)
namelists used in \file{start.in}, with the corresponding parameters
and their default values (in square brackets).
Any variable referred to as a \dfn{flag} can be set to any nonzero value
to switch the corresponding feature on.
Not all parameters are used for a given scenario.
This list is not necessarily up to date; also, in many cases it can only
give an idea of the corresponding initial state; to get more insight and
the latest set of parameters, you need to look at the code.

The value $\varepsilon$ corresponds to 5 times the smallest number larger
than zero.
For single precision, this is typically about
$\varepsilon \approx 5\times1.2{\times}10^{-7} = 6{\times}10^{-7}$; for
double precision, $\varepsilon\approx10^{-15}$.

% ---------------------------------------------------------------------- %
\begin{longtable}{lp{0.6\textwidth}}
%\begin{tabular}{lp{0.6\textwidth}}
\toprule
  \multicolumn{1}{c}{\emph{Variable [default value]}}
               & \multicolumn{1}{c}{\emph{Meaning}} \\
\midrule
  \multicolumn{2}{c}{Namelist \name{init_pars}}\\
\midrule
  \var{cvsid} [\code{''}]
               & the \name{CVS} identification string, which allows you to
                 keep track of the version of \file{start.in}.\\
  \var{ip} [$14$]
               & (anti-)verbosity level: \code{ip=1} produces lots of
                 diagnostic output, \code{ip=14} virtually none. \\
  \var{xyz0} [$(-\pi,-\pi,-\pi)$],\\
  \var{Lxyz} [$(2\pi,2\pi,2\pi)$],\\
  \var{lperi} [(\code{T},\code{T},\code{T})]
               & determine the geometry of the box. All three are vectors
                 of the form ($x$-comp., $y$-comp., $z$-comp.); \var{xyz0}
                 describes the left (lower) corner of the box, \var{Lxyz}
                 the box size.
                 \var{lperi} specifies whether a direction is considered
                 periodic (in which case the last point is omitted) or not.
                 In all cases, three ghost zones will be added. \\
  \var{lprocz_slowest} [\code{T}]
               & if set to \code{F}, the ordering of processor numbers is
                 changed, so the $z$ processors are now in the inner loop.
                 Since \var{nprocy}=4 is optimal (see Sect.~\ref{Bandwidth}),
                 you may want to put \var{lprocz_slowest}=T when
                 \code{nygrid}$>$\code{nzgrid}.\\
  \var{lwrite_ic} [\code{F}]
               & if set \code{T}, the initial data are written into the
                 file \file{VAR0}. This is generally useful, but doing this
                 all the time uses up plenty of disk space.\\
  \var{lnowrite} [\code{F}]
               & if set \code{T}, all initialization files are written,
                 except \file{var.dat}. This option allows you to use old
                 file{var.dat} files, but updates all other initialization
                 files. This could be useful after having changed the code
                 and, in particular, when the \file{var.dat} files will be
                 overwritten by \file{remesh.csh}.\\
  \var{lwrite_aux} [\code{F}]
               & if set \code{T}, auxiliary variables (those calculated at
                 each step, but not evolved mathematically) to 
                 \file{var.dat} after the evolved quantities. \\
  \var{lwrite_2d} [\code{F}]
               & if set \code{T}, only 2D-snapshots are written into VAR
                 files in the case of 2D-runs with $nygrid=1$ or
                 $nzgrid=1$.\\
  \var{lread_oldsnap} [\code{F}]
               & if set \code{T}, the old snapshot will be read in before
                 producing (overwriting) initial conditions. For example, if
                 you just want to add a perturbation to the magnetic field,
                 you'd give no initial condition for density and velocity
                 (so you keep the data from a hopefully relaxed run),
                 and just add whatever you need for the magnetic field.\\
  \var{lread_oldsnap_nomag} [\code{F}]
               & if set \code{T}, the old snapshot from a non-magnetic run
                 will be read in before producing (overwriting) initial
                 conditions. This allows one to let a hydrodynamic
                 run relax before adding a magnetic field. However,
                 for this to work one has to modify {\it manually}
                 \file{data/param.nml} by adding an entry for
                 \var{MAGNETIC_INIT_PARS} or \var{PSCALAR_INIT_PARS}.
                 In addition, for \code{idl} to read correctly after the
                 first restarted run, you must adjust the value of \var{mvar}
                 in \file{data/dim.dat} \\
  \var{lread_oldsnap_nopscalar} [\code{F}]
               & if set \code{T}, the old snapshot from a run without
                 passive scalar will be read in before producing
                 (overwriting) initial conditions. This allows one to
                 let a hydrodynamic run relax before adding a passive
                 scalar.\\
  \var{lshift_origin} [\code{F,F,F}]
               & if set \code{T} for any or some of the three directions,
                 the mesh is shifted by 1/2 meshpoint in that or those
                 directions so that the mesh goes through the origin.\\
  \var{unit_system} [\code{'cgs'}]
               & you can set this character string to \index{SI units}
                 \code{'SI'}, which means that you can give \index{Units}
                 physical dimensions in \index{SI units} SI units.
                 The default is \index{cgs units} cgs units.\\
  \var{unit_length} [\code{1}]
               & allows you to set the unit length. Suppose you want
                 the unit length to be $1\,{\rm kpc}$, then you would
                 say \code{unit_length='3e21'}. (Of course, politically
                 correct would be to say \code{unit_system='SI'} in
                 which case you say \code{unit_length='3e19'}.)\\
  \var{unit_velocity} [\code{1}]
               & Example: if you want km/s you say \code{unit_length='1e5'}.\\
  \var{unit_density} [\code{1}]
               & Example: if you want your unit density to be
                 $10^{-24}\,{\rm g}/{\rm cm}^3$ you say
                 \code{unit_density='1e-24'}.\\
  \var{unit_temperature} [\code{1}]
               & Example: \code{unit_temperature='1e6'} if you want
                 mega-Kelvin.\\
  \var{random_gen} [\code{system}]
               & \label{random-gen-init}
                 choose random number generator;
                 currently valid choices are
                 \begin{description}
                 \item[\code{'system'}] (your compiler's generator),
                 \item[\code{'min_std'}] (the `minimal standard' generator
                   \code{ran0()} from `Numerical Recipes'),
                 \item[\code{'nr_f90'}] (the Parker-Miller-Marsaglia
                   generator \code{ran()} from `Numerical Recipes for
                   F90').
                 \end{description} \\
  \var{bcx} [(\code{'p'}, \code{'p'}, \ldots)], \\
  \var{bcy} [(\code{'p'}, \code{'p'}, \ldots)], \\
  \var{bcz} [(\code{'p'}, \code{'p'}, \ldots)]
               & boundary conditions. See Sect.~\ref{boundconds} for a
                 discussion of where and how to set these. \\

                 \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{hydro_init_pars}} \\
\midrule
  \var{inituu} [\code{'zero'}]
               & initialization of velocity. Currently valid choices are
                 \begin{description}
                 \item[\code{`zero'}] ($\uv=0$ ),
                 \item[\code{`gaussian-noise'}] (random,
                   normally-distributed $u_x$,$u_z$),
                 \item[\code{`gaussian-noise-x'}] (random,
                   normally-distributed $u_x$),
                 \item[\code{`sound-wave'}] (sound wave in $x$ direction),
                 \item[\code{`shock-tube'}] (polytropic standing shock),
                 \item[\code{`bullets'}] (blob-like velocity perturbations),
                 \item[\code{`Alfven-circ-x'}] (circularly polarized
                   Alfven wave in x direction),
                 \item[\code{`const-ux'}] (constant x-velocity),
                 \item[\code{`const-uy'}] (constant y-velocity),
                 \item[\code{`tang-discont-z'}] (tangential discontinuity:
                   velocity is directed along $x$, jump is at $z=0$),
                 \item[\code{`Fourier-trunc'}] (truncated Fourier series),
                 \item[\code{`up-down'}] (flow upward in one spot,
                   downward in another; not solenoidal).
                 \end{description}
                 \\
  \var{ampluu} [$0.$]
               & amplitude for some types of initial velocities. \\
  \var{widthuu} [$0.1$]
               & width for some types of initial velocities. \\
  \var{urand} [$0.$]
               & additional random perturbation of $\uv$. If
                 \verb|urand>0|, the perturbation is additive,
                 $u_i \mapsto u_i + u_{\rm rand}{\cal U}_{[0.5,0.5]}$;
                 if \verb|urand<0|, it is multiplicative,
                 $u_i \mapsto u_i \times u_{\rm rand}{\cal U}_{[0.5,0.5]}$;
                 in both cases, ${\cal U}_{[0.5,0.5]}$ is a uniformly
                 distributed random variable on the interval $[-0.5,0.5]$.\\
  \var{uu_left} [$0.$], \\
  \var{uu_right} [$0.$]
               & needed for \code{inituu='shock-tube'}.\\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{density_init_pars}} \\
\midrule
  \var{initlnrho} [\code{'zero'}]
               & initialization of density. Currently valid choices are
                 \begin{description}
                 \item[\code{`zero'}] ($\ln\varrho=0$),
                 \item[\code{`isothermal'}] (isothermal stratification),
                 \item[\code{`polytropic\_simple'}] (polytropic stratification),
                 \item[\code{`hydrostatic-z-2'}] (hydrostatic vertical
                   stratification for isentropic atmosphere),
                 \item[\code{`xjump'}] (density jump in $x$ of width
                   \var{widthlnrho}),
                 \item[\code{`rho-jump-z'}] (density jump in $z$ of width
                   \var{widthlnrho}),
                 \item[\code{`piecew-poly'}] (piecewise polytropic vertical
                   stratification for solar convection),
                 \item[\code{`polytropic'}] (polytropic vertical
                   stratification),
                 \item[\code{`sound-wave'}] (sound wave),
                 \item[\code{`shock-tube'}] (polytropic standing shock),
                 \item[\code{`gaussian-noise'}] (Gaussian-distributed,
                   uncorrelated noise),
                 \item[\code{`gaussian-noise'}] (Gaussian-distributed,
                   uncorrelated noise in $x$, but uniform in $y$ and $z$),
                 \item[\code{`hydrostatic-r'}] (hydrostatic radial density
                   stratification for isentropic or isothermal sphere),
                 \item[\code{`sin-xy'}] (sine profile in $x$ and $y$),
                 \item[\code{`sin-xy-rho'}] (sine profile in $x$ and $y$, but
                   in $\varrho$, not $\ln\varrho$),
                 \item[\code{`linear'}] (linear profile in $\kv\cdot\xv$),
                 \item[\code{`planet'}] (planet solution; see \S\ref{S-planet}).
                 \end{description}
                 \\
  \var{gamma} [$5./3$]
               & adiabatic index $\gamma=c_p/c_v$. \\
  \var{cs0} [$1.$]
               & can be used to set the dimension of velocity;
                 larger values can be used to decrease stratification \\
  \var{rho0} [$1.$]
               & \label{cs0-rho0-init}%
                 reference values of sound speed and density,
                 i.\,e.~values at height \var{zref}. \\
  \var{ampllnrho} [$0.$], \\
  \var{widthlnrho} [$0.1$]
               & amplitude and width for some types of initial densities. \\
  \var{rho_left} [$1.$], \\
  \var{rho_right} [$1.$]
               & needed for \code{initlnrho='shock-tube'}. \\
  \var{cs2bot} [$1.$], \\
  \var{cs2top} [$1.$]
               & sound speed at bottom and top. Needed for some types of
                 stratification. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{grav_init_pars}} \\
\midrule
  \var{zref} [$0.$]
               & \label{zref-init}%
                 reference height where in the initial stratification
                 $c_{\rm s}^2=c_{\rm s0}^2$ and $\ln\varrho=\ln\varrho_0$.\\
  \var{gravz} [$-1.$]
               & vertical gravity component $g_z$.\\
  \var{grav_profile} \\{}
    [\code{'const'}]
               & constant gravity $g_z = \texttt{gravz}$
                 (\code{grav_profile='const'}) gravity
                 or linear profile $g_z = \texttt{gravz}\cdot z$
                 (\code{grav_profile='linear'}, for accretion discs and
                 similar). \\
  \var{z1} [$0.$], \\
  \var{z2} [$1.$]
               & specific to the solar convection case
                 \code{initlnrho='piecew-poly'}.
                 The stable layer is $z_0 < z < z_1$, the unstable layer
                 $z_1 < z < z_2$, and the top (isothermal) layer is
                 $z_2 < z < z_{\rm top}$. \\
  \var{nu_epicycle} [$1.$]
               & vertical epicyclic frequency; for accretion
                 discs it should be equal to Omega, but not for
                 galactic discs; see Eq.~(\ref{disc-gravz-init}) in
                 Sect.~\ref{VerticalStratification}.\\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{entropy_init_pars}} \\
\midrule
  \var{initss} [\code{'nothing'}]
               & initialization of entropy. Currently valid choices are
                 \begin{description}
                 \item[\code{`nothing'}] (leaves the initialization done
                   in the density module unchanged),
                 \item[\code{`zero'}] (put $s=0$ explicitly; this may
                   overwrite the initialization done in the density module),
                 \item[\code{`isothermal'}] (isothermal stratification,
                   $T=\const$),
                 \item[\code{`isobaric'}] (isobaric, $p=\const$),
                 \item[\code{`isentropic'}] (isentropic with superimposed
                   hot [or cool] bubble),
                 \item[\code{`linprof'}] (linear entropy profile in $z$),
                 \item[\code{`piecew-poly'}] (piecewise polytropic
                   stratification for convection),
                 \item[\code{`polytropic'}] (polytropic stratification,
                   polytropic exponent is \var{mpoly0}),
                 \item[\code{`blob'}] (puts a gaussian blob in entropy for
                   buoyancy experiments; see Ref.~\cite{BH01} for details)
                 \item[\code{`xjump'}] (jump in $x$ direction),
                 \item[\code{`hor-tube'}] (horizontal flux tube in entropy,
                   oriented in the $y$-direction).
                 \end{description}
                 \\
  \var{pertss} [\code{'zero'}]
               & additional perturbation to entropy. Currently valid
                 choices are
                 \begin{description}
                 \item[\code{'zero'}] (no perturbation)
                 \item[\code{'hexagonal'}] (hexagonal perturbation for
                   convection).
                 \end{description}
                 \\
  \var{ampl_ss} [$0.$], \\
  \var{widthss} [$2\varepsilon$]
               & amplitude and width for some types of initial entropy. \\
  \var{grads0} [$0.$]
               & initial entropy gradient for \code{initss=linprof}. \\
  \var{radius_ss} [$0.1$]
               & radius of bubble for \code{initss=isentropic}. \\
  \var{mpoly0} [$1.5$], \\
  \var{mpoly1} [$1.5$], \\
  \var{mpoly2} [$1.5$]
               & \label{mpoly012-init}%
                 specific to the solar convection case
                 \code{initss=piecew-poly}:
                 polytropic indices of unstable (\var{mpoly0}), stable
                 (\var{mpoly1}) and top layer (\var{mpoly2}).
                 If the flag \var{isothtop} is set, the
                 top layer is initialized to be isothermal, otherwise
                 thermal (plus hydrostatic) equilibrium is assumed for all
                 three layers, which results in a piecewise polytropic
                 stratification. \\
  \var{isothtop} [$0$]
               & flag for isothermal top layer for \code{initss=piecew-poly}. \\
  \var{khor_ss} [$1.$]
               & horizontal wave number for \code{pertss=hexagonal} \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{magnetic_init_pars}} \\
\midrule
  \var{initaa} [\code{'zero'}]
               & initialization of magnetic field (vector potential).
                 Currently valid choices are
                 \begin{description}
                 \item[\code{`Alfven-x'}] (Alfv\'en wave traveling in the
                   $x$-direction; this also sets the velocity),
                 \item[\code{`Alfven-z'}] (Alfv\'en wave traveling in the
                   $z$-direction; this also sets the velocity),
                 \item[\code{`Alfvenz-rot'}] (same as \code{`Alfven-z'}, but
                   with rotation),
                 \item[\code{`Alfven-circ-x'}] (circularly polarized
                   Alfven wave in x direction),
                 \item[\code{`Beltrami-x'}] ($x$-dependent Beltrami wave),
                 \item[\code{`Beltrami-y'}] ($y$-dependent Beltrami wave),
                 \item[\code{`Beltrami-z'}] ($z$-dependent Beltrami wave),
                 \item[\code{`Bz(x)'}] ($B_z\propto\cos(k x)$),
                 \item[\code{`crazy'}] (for testing purposes).
                 \item[\code{`diffrot'}] ([needs to be documented]),
                 \item[\code{`fluxrings'}] (two interlocked magnetic fluxrings;
                   see \S~\ref{fluxrings}),
                 \item[\code{`gaussian-noise'}] (white noise),
                 \item[\code{`halfcos-Bx'}] ([needs to be documented]),
                 \item[\code{`hor-tube'}] (horizontal flux tube in $\Bv$,
                   oriented in the $y$-direction).
                 \item[\code{`hor-fluxlayer'}] (horizontal flux layer),
                 \item[\code{`mag-support'}] ([needs to be documented]),
                 \item[\code{`mode'}] ([needs to be documented]),
                 \item[\code{`modeb'}] ([needs to be documented]),
                 \item[\code{`propto-ux'}] ([needs to be documented]),
                 \item[\code{`propto-uy'}] ([needs to be documented]),
                 \item[\code{`propto-uz'}] ([needs to be documented]),
                 \item[\code{`sinxsinz'}] ($\sin x \sin z$),
                 \item[\code{`uniform-Bx'}] (uniform field in $x$ direction),
                 \item[\code{`uniform-By'}] (uniform field in $y$ direction),
                 \item[\code{`uniform-Bz'}] (uniform field in $z$ direction),
                 \item[\code{`zero'}] (zero field),
                 \end{description}
                 \\
  \var{initaa2} [\code{'zero'}]
               & additional perturbation of magnetic field.
                 Currently valid choices are
                 \begin{description}
                 \item[\code{`zero'}] (zero perturbation),
                 \item[\code{`Beltrami-x'}] ($x$-dependent Beltrami wave),
                 \item[\code{`Beltrami-y'}] ($y$-dependent Beltrami wave),
                 \item[\code{`Beltrami-z'}] ($z$-dependent Beltrami wave).
                 \end{description}
                 \\
  \var{amplaa} [$0.$]
               & amplitude for some types of initial magnetic fields. \\
  \var{amplaa2} [$0.$]
               & amplitude for some types of magnetic field perturbation. \\
  \var[fring1,fring2]{fring\{1,2\}} [$0.$], \\
  \var[Iring1,Iring2]{Iring\{1,2\}} [$0.$], \\
  \var[Rring1,Rring2]{Rring\{1,2\}} [$1.$], \\
  \var[wr1,wr2]{wr\{1,2\}} [$0.3$]
               & flux, current, outer and inner radius of flux ring 1/2;
                 see Sect.~\ref{fluxrings}. \\
  \var{radius} [$0.1$]
               & used by some initial fields. \\
  \var{epsilonaa} [$10^{-2}$]
               & used by some initial fields. \\
  \var{widthaa} [$0.5$]
               & used by some initial fields. \\
  \var{z0aa} [$0.$]
               & used by some initial fields. \\
  \var{kx_aa} [$1.$], \\
  \var{ky_aa} [$1.$], \\
  \var{kz_aa} [$1.$]
               & wavenumbers used by some initial fields. \\
  \var{lpress_equil} [F]
               & flag for pressure equilibrium (can be used in connection
                 with all initial fields) \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{pscalar_init_pars}} \\
\midrule
  \var{initlncc} [\code{'zero'}]
               & initialization of passive scalar
                 (concentration per unit mass, $c$).
                 Currently valid choices (for $\ln c$) are
                 \begin{description}
                 \item[\code{`zero'}] ($\ln c=0.$),
                 \item[\code{`gaussian-noise'}] (white noise),
                 \item[\code{`wave-x'}] (wave in $x$ direction),
                 \item[\code{`wave-y'}] (wave in $y$ direction),
                 \item[\code{`wave-z'}] (wave in $z$ direction),
                 \item[\code{`tang-discont-z'}] (Kelvin-Helmholtz instability),
                 \item[\code{`hor-tube'}] (horizontal tube in concentration;
                  used as a marker for magnetic flux tubes).
                 \end{description}
                 \\
  \var{initlncc2} [\code{'zero'}]
               & additional perturbation of passive scalar concentration $c$.
                 Currently valid choices are
                 \begin{description}
                 \item[\code{`zero'}] ($\delta\ln c=0.$),
                 \item[\code{`wave-x'}] (add $x$-directed wave to $\ln c$).
                 \end{description}
                 \\
\var{ampllncc} [$0.1$]
               & amplitude for some types of initial concentration. \\
  \var{ampllncc2} [$0.$]
               & amplitude for some types of concentration perturbation. \\
  \var{kx_lncc} [$1.$], \\
  \var{ky_lncc} [$1.$], \\
  \var{kz_lncc} [$1.$]
               & wave numbers for some types of initial concentration. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{shear_init_pars}} \\
\midrule
  \var{qshear} [$0.$]
               & \label{qshear-init}%
                 degree of shear for shearing-box simulations (the
                 shearing-periodic boundaries are the $x$-boundaries and
                 are sheared in the $y$-direction). The shear velocity
                 is ${\bf U}=-q\varOmega x\,{\hat{\bf y}}$. \\
\bottomrule
%\end{tabular}
\end{longtable}
% ---------------------------------------------------------------------- %

\subsection[Runtime parameters for \File{run.in}]%
{List of runtime parameters for \file{run.in}}
\label{S-all-run-params}

The following table lists all (at the time of writing, September 2002)
namelists used in file \file{run.in}, with the corresponding
parameters and their default values (in square brackets).
Default values marked as [start] are taken from \file{start.in}.
Any variable referred to as a \dfn{flag} can be set to any nonzero value
to switch the corresponding feature on.
Not all parameters are used for a given scenario.
This list is not necessarily up to date; also, in many cases it can only
give an idea of the corresponding setup; to get more insight and
the latest set of parameters, you need to look at the code.


% ---------------------------------------------------------------------- %
\begin{longtable}{lp{0.6\textwidth}}
%\begin{tabular}{lp{0.6\textwidth}}
\toprule
  \multicolumn{1}{c}{\emph{Variable [default value]}}
               & \multicolumn{1}{c}{\emph{Meaning}} \\
\midrule
  \multicolumn{2}{c}{Namelist \name{run_pars}}\\
\midrule
  \var{cvsid} [\code{''}]
               & \name{CVS} identification string, which allows you to
                 keep track of the version of \file{run.in}. \\
  \var{ip} [$14$]
               & (anti-)verbosity level: \code{ip=1} produces lots of
                 additional diagnostic output, \code{ip=14} virtually none. \\
  \var{nt} [$0$]
               & number of time steps to run. This number can be increased
                 or decreased during the run by \cmd{touch RELOAD}.\\
  \var{it1} [$10$]
               & write diagnostic output every \var{it1} time steps 
                 (see Sect.~\ref{diagnostic-IO}). \\
  \var{cdt} [$0.4$]
               & Courant coefficient for advective time step; see
                 \S\ref{time-step}. \\
  \var{cdtv} [$0.08$]
               & Courant coefficient for diffusive time step; see
                 \S\ref{time-step}. \\
  \var{dt} [$0.$]
               & \label{dt-run}%
                 time step; if $\ne 0.$, this overwrites
                 the Courant time step. See \S\ref{time-step} for a
                 discussion of the latter. \\
  \var{dtmin} [$10^{-6}$]
               & abort if time step $\delta t < \delta t_{\rm min}$. \\
  \var{tmax} [$10^{33}$]
               & don't run time steps beyond this time. Useful if you want
                 to run for a given amount of time, but don't know the
                 necessary number of time steps.\\
  \var{isave} [$100$]
               & update current snapshot \file{var.dat} every \var{isave}
                 time steps. \\
  \var{itorder} [$3$]
               & order of time step (1 for Euler; 2 for 3nd-order, 3 for
                 3rd-order Runge--Kutta). \\
  \var{dsnap} [$100.$]
               & save permanent snapshot every \var{dsnap} time units to
                 files \file{VAR$N$}, where $N$ counts from $N=1$ upward.
                 (This information is stored in the file
                 \file[tsnap.dat]{data/tsnap.dat};
                 see the module \var{wsnaps.f90}, which in turn uses the
                 subroutines \var{out1} and \var{out2}). \\
  \var{dvid} [$100.$]
               & write two-dimensional sections for generation of videos
                 every \var{dvid} time units (not timesteps; see the
                 subroutines \var{out1} and \var{out2} in the code). \\
  \var{iwig} [$0$]
               & if $\ne 0$, apply a Nyquist filter (a filter eliminating
                 any signal at the Nyquist frequency, but affecting large
                 scales as little as possible) every \var{iwig} time steps to
                 logarithmic density (sometimes necessary with convection
                 simulations). \\
  \var{ix} [$-1$], 
  \var{iy} [$-1$], 
  \var{iz} [$-1$], 
  \var{iz2} [$-1$]
               & position of slice planes for video files.
                 Any negative value of some of these variables will be
                 overwritten according to the value of
                 \var{slice_position}.
                 See \S~\ref{S-slices}) for details. \\
  \var{slice_position} ['p']
               & symbolic specification of slice position.
                 Currently valid choices are
                 \begin{description}
                 \item[\code{'p'}] (\emph{periphery} of the box)
                 \item[\code{'m'}] (\emph{middle} of the box)
                 \item[\code{'e'}] (\emph{equator} for half-sphere
                   calculations, i.\,e.~$x$, $y$ centered, $z$ bottom)
                 \end{description}
                 These settings are overridden by explicitly setting
                 \var{ix}, \var{iy}, \var{iz} or \var{iz2}.
                 See \S~\ref{S-slices}) for details. \\
  \var{tavg} [$0$]
               & averaging time $\tau_{\rm avg}$ for time averages (if
                 $\ne 0$); at the same time, time interval for writing
                 time averages. See \S~\ref{S-time-averages} for details. \\
  \var{idx_tavg} [$(0,0,\ldots,0)$]
               & indices of variables to time-average.
                 See \S~\ref{S-time-averages} for details. \\
  \var{d2davg} [$100.$]
               & time interval for azimuthal and $z$-averages, i.e.~the
                 averages that produce 2d data.
                 See \S~\ref{S-phi-averages} for details. \\
  \var{ialive} [$0$]
               & if $\ne 0$, each processor writes the current time step
                 to \file{alive.info} every \var{ialive} time steps. 
                 (This can be used to find out which node has crashed
                 if there is a problem and the run is hanging.)\\
  \var{bcx} [(\code{'p'}, \code{'p'}, \ldots)], \\
  \var{bcy} [(\code{'p'}, \code{'p'}, \ldots)], \\
  \var{bcz} [(\code{'p'}, \code{'p'}, \ldots)]
               & boundary conditions. See Sect.~\ref{boundconds} for a
                 discussion of where and how to set these. \\
  \var{random_gen} [start]
               & see start parameters, p.~\pageref{random-gen-init} \\
  \var{lwrite_aux} [start]
               & if set \code{T}, auxiliary variables (those calculated at
                 each step, but not evolved mathematically) to 
                 \file{var.dat} and \file{VAR} files after the evolved 
                 quantities. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{hydro_run_pars}} \\
\midrule
  \var{nu} [$0.$]
               & kinematic viscosity. \\
  \var{ivisc} [\code{'nu-const'}]
               & select form of viscous term (see
                 \S\ref{S-Eqn-of-motion}); currently valid choices are
                 \begin{description}
                 \item[\code{'nu-const'}] -- viscous force for $\nu=\const$,
                   $\Fv_{\rm visc}=\nu(\Laplace\uv+{\textstyle{1\over3}}\grad\Div\uv
                    + 2\mathsf{S}\cdot\grad\ln\varrho)$
                 \item[\code{'rho_nu-const'}] -- viscous force for
                   $\mu\equiv\varrho\nu=\const$,
                   $\Fv_{\rm visc}=(\mu/\varrho)(\Laplace\uv+{\textstyle{1\over3}}\grad\Div\uv)$.
                   With this option, the input parameter \var{nu} actually
                   sets the value of $\mu/\varrho_0$
                   (\var{rho0}=$\varrho_0$ is another input parameter, see
                   pp.~\pageref{cs0-rho0-init} and \pageref{cs0-rho0-run})
                 \item[\code{'simplified'}] -- simplified viscous force
                   $\Fv_{\rm visc}=\nu\Laplace\uv$
                 \end{description}
                 \\
  \var{Omega} [$0.$]
               & magnitude of angular velocity for \name{Coriolis force}
                 (note: the centrifugal force is turned off by default,
                 unless \code{lcentrifugal_force=T} is set).\\
  \var{theta} [$0.$]
               & direction of angular velocity in degrees ($\vartheta=0$ for
                 $z$-direction, $\vartheta=90$ for the negative $x$-direction,
                 corresponding to a box located at the equator of a
                 rotating sphere.
                 Thus, e.g., $\vartheta=60$ corresponds to $30^\circ$ latitude.
                 (Note: prior to April 29, 2007, there was a minus sign in
                 the definition of $\vartheta$.)\\
  \var{ttransient} [$0.$]
               & initial time span for which to do something special
                 (transient).
                 Currently just used to smoothly switch on heating
                 [Should be in \name{run_pars}, rather than here]. \\
  \var{dampu} [$0.$], \\
  \var{tdamp} [$0.$], \\
  \var{ldamp_fade} [F]
               & damp motions during the initial time interval
                 $0<t<t_{\rm damp}$ with a damping term $-\var{dampu}(\uv)$.
                 If \var{ldamp_fade} is set, smoothly reduce damping to
                 zero over the second half of the time interval \var{tdamp}.
                 Initial velocity damping is useful for situations where
                 initial conditions are far from equilibrium. \\
  \var{dampuint} [$0.$], 
               & weighting of damping external to spherical region 
                 (see \var{wdamp}, $\var{damp}_u$ below). \\
  \var{dampuext} [$0.$], 
               & weighting of damping in internal spherical region 
                 (see \var{wdamp}, $\var{damp}_u$ below). \\
  \var{rdampint} [$0.$], 
               & radius of internal damping region  \\
  \var{rdampext} [impossible], 
               & radius of external damping region, used in place of 
                 former variable \var{rdamp} \\
  \var{wdamp} [$0.2$]
               & permanently damp motions in $|\xv|<r_{\rm dampint}$ 
                 with damping term 
                 $-\var{damp}_uint \,\uv\,\chi(r{-}r_{\rm dampint})$
                 or $|\xv|>r_{\rm dampext}$ with damping term 
                 $-\var{damp}_uext \,\uv\,\chi(r{-}r_{\rm dampext})$,
                 where $\chi(\cdot)$ is a smooth profile of width
                 \var{wdamp}. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{density_run_pars}} \\
\midrule
  \var{cs0} [start], \\
  \var{rho0} [start], \\
  \var{gamma} [start]
               & \label{cs0-rho0-run}%
                 see start parameters, p.~\pageref{cs0-rho0-init} \\
  \var{cdiffrho} [$0.$]
               & Coefficient for mass diffusion (diffusion term will be
                 $c_{\rm diffrho}\,\delta x\, {\cs}_0$ . \\
  \var{cs2bot} [start], \\
  \var{cs2top} [start]
               & squared sound speed at bottom and top for boundary
                 condition \option{c2}. \\
  \var{lupw_lnrho} [.false.]
               & use 5th-order upwind derivative operator for the
               advection term $\uv\cdot\grad\ln\varrho$ to avoid spurious
               Nyquist signal (`wiggles'); see \S\ref{S-upwind}. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{entropy_run_pars}} \\
\midrule
  \var{hcond0} [$0.$], \\
  \var{hcond1} [start], \\
  \var{hcond2} [start]
               & specific to the solar convection case
                 \code{initss=piecew-poly}:
                 heat conductivities $K$ in the individual layers.
                 \var{hcond0} is the value $K_{\rm unst}$ in the
                 unstable layer, \var{hcond1} is the \emph{ratio}
                 $K_{\rm stab}/K_{\rm unst}$ for the stable
                 layer, and \var{hcond2} is the \emph{ratio} 
                 $K_{\rm top}/K_{\rm unst}$ for the top layer.
                 The function $K(z)$ is not discontinuous, as the
                 transition between the different values is smoothed over
                 the width \var{widthss}.
                 If \var{hcond1} or \var{hcond2} are not set, they are
                 calculated according to the polytropic indices of the
                 initial profile, $K\propto m{+}1$. \\
  \var{iheatcond} [\code{'K-const'}]
               & select type of heat conduction.
                 Currently valid choices are
                 \begin{description}
                 \item[\code{`K-const'}]   (constant heat conductivity),
                 \item[\code{`chi-const'}] (constant thermal diffusivity),
                 \item[\code{`simple'}]    (somehow simpler version for
                   polytropic stratification [?]),
                 \item[\code{`magnetic'}]  (heat conduction by electrons
                   in magnetic field -- currently still experimental).
                 \end{description}
                 \\
  \var{lcalc_heatcond_constchi} [F]
               & flag for assuming thermal diffusivity
                 $\chi=K/(c_p\varrho)=\const$, rather than $K=\const$
                 (which is the default).
                 \emph{This is currently only correct with
                 \file{noionization.f90}}.
                 Superseded by \var{iheatcond}. \\
  \var{chi} [$0.$]
               & value of $\chi$ when \code{lcalc_heatcond_constchi=T}. \\
  \var{widthss} [start]
               & width of transition region between layers.
                 See start parameters, p.~\pageref{mpoly012-init}. \\
  \var{isothtop} [start]
               & flag for isothermal top layer for solar convection case.
                 See start parameters, p.~\pageref{mpoly012-init}. \\
  \var{luminosity} [$0.$], \\
  \var{wheat} [$0.1$]
               & strength and width of heating region. \\
  \var{cooltype} [\code{'Temp'}]
               & type of cooling; \emph{currently only implemented for
                 spherical geometry}.
                 Currently valid choices are
                 \begin{description}
                 \item[\code{`Temp'},\code{`cs2'}] (cool temperature
                   toward $\cs^2=\var{cs2cool}$) with a cooling term
                   \[
                     -\mathcal{C} = -c_{\rm cool}
                                     \frac{\cs^2-{\cs^2}_{\rm cool}}
                                          {{\cs^2}_{\rm cool}}
                   \])
                 \item[\code{`Temp-rho'},\code{cs2-rho}]
                   (cool temperature toward $\cs^2=\var{cs2cool}$) with a
                   cooling term
                   \[
                     -\mathcal{C} = -c_{\rm cool}\,\varrho\,
                                     \frac{\cs^2-{\cs^2}_{\rm cool}}
                                          {{\cs^2}_{\rm cool}}
                   \]
                   --- this avoids numerical instabilities in low-density
                   regions [currently, the cooling coefficient
                   $c_{\rm cool}\equiv$\var{cool} is not taken into account
                   when the time step is calculated])
                 \item[\code{`entropy'}] (cool entropy toward $0.$).
                 \end{description}
                 \\
  \var{cool} [$0.$], \\
  \var{wcool} [$0.1$]
               & strength $c_{\rm cool}$ and smoothing width of cooling
                 region. \\
  \var{rcool} [$1.$]
               & radius of cooling region: cool for
                 $|\xv| \ge \mbox{\var{rcool}}$.\\
  \var{Fbot} [start]
               & heat flux for bottom boundary condition \option{c1}.
                 For polytropic atmospheres, if \var{Fbot} is not set, it will
                 be calculated from the value of \var{hcond0} in \file{start.x},
                 provided the entropy boundary condition is set to \option{c1}. \\
  \var{chi_t} [$0.$]
               & entropy diffusion coefficient for diffusive term
                 $\partial s/\partial t = \ldots + \chi_{\rm t}\Laplace s$
                 in the entropy equation, that can represent some kind of
                 turbulent (sub-grid) mixing.
                 It is probably a bad idea to combine this with heat
                 conduction $\var{hcond0} \ne 0$. \\
  \var{lupw_ss} [.false.]
               & use 5th-order upwind derivative operator for the
                 advection term $\uv\cdot\grad s$ to avoid spurious Nyquist
                 signal (`wiggles'); see \S\ref{S-upwind}. \\
  \var{tauheat_buffer} [$0.$]
               & time scale for heating to target temperature
                 (=\var{TTheat_buffer}); zero disables the buffer zone. \\
  \var{zheat_buffer} [$0.$]
               & $z$ coordinate of the thermal buffer zone. Buffering
                 is active in $|z|>$\var{TTheat_buffer}. \\
  \var{dheat_buffer1} [$0.$]
               & Inverse thickness of transition to buffered layer.\\
  \var{TTheat_buffer} [$0.$]
               & target temperature in thermal buffer zone ($z$ direction only). \\ 
%
\midrule
  \multicolumn{2}{c}{Namelist \name{magnetic_run_pars}} \\
\midrule
  \var{B_ext} [$(0.,0.,0.)$]
               & uniform background magnetic field (for fully periodic
                 boundary conditions, uniform fields need to be explicitly
                 added, since otherwise the vector potential $\Av$ has a
                 linear $\xv$-dependence which is incompatible with
                 periodicity). \\
  \var{eta} [$0.$]
               & magnetic diffusivity $\eta=1/(\mu_0\sigma)$, where
                 $\sigma$ is the electric conductivity. \\
  \var{height_eta} [$0.$], \\
  \var{eta_out} [$0.$]
               & used to add extra diffusivity in a halo region. \\
  \var{eta_int} [$0.$]
               & used to add extra diffusivity inside sphere of
                 radius \var{r_int}. \\
  \var{eta_ext} [$0.$]
               & used to add extra diffusivity outside sphere of
                 radius \var{r_ext}. \\
  \var{kinflow} [\code{''}]
               & set type of flow fixed with \option{nohydro}. Currently
                 the only recognized value is \code{'ABC'} for an $ABC$
                 flow; all other values lead to $\uv=\zerovect$. \\
  \var{kx} [$1.$], \\
  \var{ky} [$1.$], \\
  \var{kz} [$1.$]
               & wave numbers for $ABC$ flow. \\
  \var{ABC_A} [$1.$], \\
  \var{ABC_B} [$1.$], \\
  \var{ABC_C} [$1.$]
               & amplitudes $A$, $B$ and $C$ for $ABC$ flow. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{pscalar_run_pars}} \\
\midrule
  \var{pscalar_diff} [$0.$]
               & diffusion for passive scalar concentration $c$. \\
  \var{tensor_pscalar_diff} [$0.$]
               & coefficient for non-isotropic diffusion of passive scalar. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{forcing_run_pars}} \\
\midrule
  \var{iforce} [$2$]
               & select form of forcing in the equation of motion;
                 currently valid choices are
                 \begin{description}
                 \item[\code{'zero'}] (no forcing),
                 \item[\code{'irrotational'}] (irrotational forcing),
                 \item[\code{'helical'}] (helical forcing),
                 \item[\code{'fountain'}] (forcing of ``fountain flow'';
                   see Ref.~\cite{BMS95}),
                 \item[\code{'horizontal-shear'}] (forcing localized
                   horizontal sinusoidal shear).
                 \end{description}
                 \\
  \var{iforce2} [$0$]
               & select form of additional forcing in the equation of
                 motion; valid choices are as for \var{iforce}. \\
  \var{force} [$0.$]
               & amplitude of forcing. \\
  \var{relhel} [$1.$]
               & helicity of forcing. The parameter \var{relhel} corresponds
                 to $\sigma$ introduced in Sect.~\ref{SRandomForcingFunction}.
                 ($\sigma=\pm1$ corresponds to maximum helicity
                 of either sign). \\
  \var{height_ff} [$0.$]
               & multiply forcing by $z$-dependent profile of width
               \var{height_ff} (if $\ne 0$) . \\
  \var{r_ff} [$0.$]
               & if $\ne 0$, multiply forcing by spherical cutoff profile
                 (of radius \var{r_ff}) and flip signs of helicity at
                 equatorial plane. \\
  \var{width_ff} [$0.5$]
               & width of vertical and radial profiles for modifying
                 forcing. \\
  \var{kfountain} [$5$]
               & horizontal wavenumber of the fountain flow. \\
  \var{fountain} [$1.$]
               & amplitude of the fountain flow. \\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{grav_run_pars}} \\
\midrule
  \var{zref} [start], \\
  \var{gravz} [start], \\
  \var{grav_profile} [start]
               & see p.~\pageref{zref-init}. \\
  \var{nu_epicycle} [start]
               & see Eq.~(\ref{disc-gravz-init}) in
                 Sect.~\ref{VerticalStratification}.\\
%
\midrule
  \multicolumn{2}{c}{Namelist \name{shear_run_pars}} \\
\midrule
  \var{qshear} [start]
               & See p.~\pageref{qshear-init}. \\
\bottomrule
\end{longtable}
% ---------------------------------------------------------------------- %

\subsection[Parameters for \File{print.in}]%
{List of parameters for \file{print.in}}
\label{S-print.in-params}

The following table lists all possible inputs to the file \file{print.in}
that are documented.

% ---------------------------------------------------------------------- %
\input{diag-table.tex}
% ---------------------------------------------------------------------- %


\subsection[Parameters for \File{video.in}]%
{List of parameters for \file{video.in}}\
\label{S-video.in-params}

The following table lists all (at the time of writing, March 2004)
possible inputs to the file \file{video.in}.

% ---------------------------------------------------------------------- %
\begin{longtable}{lp{0.7\textwidth}}
\toprule
  \multicolumn{1}{c}{\emph{Variable}} & {\emph{Meaning}} \\
%
\midrule
  \multicolumn{2}{c}{Module \file{hydro.f90}} \\
\midrule
  \var{uu}    & velocity vector $\uv$;
                writes all three components separately to files
                \file[]{u[xyz].\{xz,yz,xy,Xy\}}  \\
  \var{oo}    & vorticity vector $\vec{\omega} = \curl\uv$;
                writes \file[]{o[xyz].\{xz,yz,xy,Xy\}}  \\
  \var{o2}    & enstrophy $\omega^2 = |\curl\uv|^2$;
                writes \file[]{o2.\{xz,yz,xy,Xy\}}  \\
  \var{divu}  & $\Div\uv$;
                writes \file[]{divu.\{xz,yz,xy,Xy\}}  \\
%
\midrule
  \multicolumn{2}{c}{Module \file{density.f90}} \\
\midrule
  \var{lnrho} & logarithmic density $\ln\varrho$;
                writes \file[]{lnrho.\{xz,yz,xy,Xy\}}  \\
%
\midrule
  \multicolumn{2}{c}{Module \file{entropy.f90}} \\
\midrule
  \var{ss}    & entropy $s$;
                writes \file[]{ss.\{xz,yz,xy,Xy\}}  \\
%
\midrule
  \multicolumn{2}{c}{Module \file{visc_shock.f90}} \\
\midrule
  \var{shock} & shock viscosity $\nu_{\rm shock}$;
                writes \file[]{shock.\{xz,yz,xy,Xy\}}  \\
%
\midrule
  \multicolumn{2}{c}{Modules \file{ionization.f90} and \file{temperature.f90}}\\
\midrule
  \var{lnTT}  & logarithmic temperature $\ln T$;
                writes \file[]{lnTT.\{xz,yz,xy,Xy\}}.

                \emph{N.B.: This won't work with other modules, but not all
                compilers will flag the array bound violation when
                \code{ilnTT=0}}. \\
%
\midrule
  \multicolumn{2}{c}{Module \file{ionization.f90}} \\
\midrule
  \var{ionization} & ionization fraction $y_{\rm H}$;
                     writes \file[]{yH.\{xz,yz,xy,Xy\}}  \\
%
\midrule
  \multicolumn{2}{c}{Module \file{radiation_ray.f90}} \\
\midrule
  \var{Qrad}  & radiative heating rate $Q_{\rm rad}$;
                writes \file[]{Qrad.\{xz,yz,xy,Xy\}}  \\
  \var{Isurf} & surface intensity $I_{\rm surf}$ (?);
                writes \file[]{Isurf.xz}  \\
%
\midrule
  \multicolumn{2}{c}{Module \file{magnetic.f90}} \\
\midrule
  \var{aa}    & magnetic vector potential $\Av$;
                writes \file[]{a[xyz].\{xz,yz,xy,Xy\}}  \\
  \var{bb}     & magnetic flux density $\Bv$;
                writes \file[]{b[xyz].\{xz,yz,xy,Xy\}}  \\
  \var{b2}    & magnetic energy density $\Bv^2$;
                writes \file[]{b2.\{xz,yz,xy,Xy\}}  \\
%
\midrule
  \multicolumn{2}{c}{Module \file{pscalar.f90}} \\
\midrule
  \var{lncc}  & logarithmic density of passive scalar $\ln c$;
                writes \file[]{lncc.\{xz,yz,xy,Xy\}}  \\
%
\midrule
  \multicolumn{2}{c}{Module \file{cosmicray.f90}} \\
\midrule
  \var{ecr}   & energy $e_{\rm cr}$ of cosmic rays (?);
                writes \file[]{ec.\{xz,yz,xy,Xy\}}  \\
%
\bottomrule
\end{longtable}
% ---------------------------------------------------------------------- %



\subsection[Parameters for \File{phiaver.in}]%
{List of parameters for \file{phiaver.in}}\
\label{S-phiaver.in-params}

The following table lists all (at the time of writing, November 2003)
possible inputs to the file \file{phiaver.in}.

% ---------------------------------------------------------------------- %
\begin{longtable}{lp{0.7\textwidth}}
\toprule
  \multicolumn{1}{c}{\emph{Variable}} & {\emph{Meaning}} \\
%
\midrule
  \multicolumn{2}{c}{Module \file{prints.f90}} \\
\midrule
  \var{rcylmphi} & cylindrical radius $\varpi = \sqrt{x^2+y^2}$
  (useful for debugging azimuthal averages)\\
  \var{phimphi} & azimuthal angle $\varphi = \arctan\frac{y}{x}$
  (useful for debugging)\\
  \var{zmphi} & $z$-coordinate
  (useful for debugging)\\
  \var{rmphi} & spherical radius $r=\sqrt{\varpi^2+z^2}$
  (useful for debugging)\\
%
\midrule
  \multicolumn{2}{c}{Module \file{hydro.f90}} \\
\midrule
  \var{urmphi} & $\left<u_\varpi\right>_\varphi$ [cyl.\ polar coords $(\varpi,\varphi,z)$]\\
  \var{upmphi} & $\left<u_\varphi\right>_\varphi$ \\
  \var{uzmphi} & $\left<u_z\right>_\varphi$ \\
  \var{uumphi} & shorthand for \var{urmphi}, \var{upmphi} and \var{uzmphi}
                 together \\
  \var{u2mphi} & $\left<\uv^2\right>_\varphi$ \\
  \var{oumphi} & $\left<\omv\cdot\uv\right>_\varphi$ \\
%
\midrule
  \multicolumn{2}{c}{Module \file{density.f90}} \\
\midrule
  \var{lnrhomphi} & $\left<\ln\varrho\right>_\varphi$ \\
  \var{rhomphi} & $\left<\varrho\right>_\varphi$ \\
%
\midrule
  \multicolumn{2}{c}{Module \file{entropy.f90}} \\
\midrule
  \var{ssmphi} & $\left<s\right>_\varphi$ \\
%
\midrule
  \multicolumn{2}{c}{Module \file{magnetic.f90}} \\
\midrule
  \var{brmphi} & $\left<B_\varpi\right>_\varphi$ [cyl.\ polar coords $(\varpi,\varphi,z)$]\\
  \var{bpmphi} & $\left<B_\varphi\right>_\varphi$ \\
  \var{bzmphi} & $\left<B_z\right>_\varphi$ \\
  \var{bbmphi} & shorthand for \var{brmphi}, \var{bpmphi} and \var{bzmphi}
                 together \\
  \var{b2mphi} & $\left<\Bv^2\right>_\varphi$ \\
  \var{jbmphi} & $\left<\Jv\cdot\Bv\right>_\varphi$ \\
%
\bottomrule
\end{longtable}
% ---------------------------------------------------------------------- %


\subsection{Initial Condition Parameter Dependence}
\label{S-all-init-depend}

The following tables list which parameters from each Namelist are required 
(\req), optional (\opt) or irrelevant (blank). The distinction is 
made between required and optional where by a parameter requires a setting 
if the default value would give an invalid or degenerate case for the 
initial condition.

% - hydro_init_pars ---------------------------------------------------- %
\begin{tabular}{@{}l|c|c|c|c|c|c|c|c|c|c|c|c}

\toprule
  inituu & 
       \rotatebox{90}{ampluu}   & 
       \rotatebox{90}{widthuu}  & 
       \rotatebox{90}{urand}    & 
       \rotatebox{90}{uu_left}  & 
       \rotatebox{90}{uu_right} & 
       \rotatebox{90}{uu_upper} & 
       \rotatebox{90}{uu_lower} & 
       \rotatebox{90}{uy_left}  & 
       \rotatebox{90}{uy_right} & 
       \rotatebox{90}{kx_uu}    & 
       \rotatebox{90}{ky_uu}    & 
       \rotatebox{90}{kz_uu}    \\ 
\midrule
  zero              & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   gaussian-noise   & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   gaussian-noise-x & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   xjump            & {}   & \opt & {}   & \req & \req & {}   & {}   
                    & \req & \req & {}   & {}   & {}   \\
\midrule
   Beltrami-x       & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   Beltrami-y       & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   Beltrami-z       & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   trilinear-x      & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   trilinear-y      & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   trilinear-z      & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   cos-cos-sin-uz   & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   tor_pert         & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   trilinear-x      & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   sound-wave       & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & \req & {}   & {}   \\
\midrule
   shock-tube       & {}   & \opt & {}   & \req & \req & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   bullets          & \req & \opt & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   Alfven-circ-x    & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & \opt & {}   & {}   \\
\midrule
   const-ux         & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   const-uy         & \req & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   tang-discont-z   & \opt & \req & {}   & {}   & {}   & \req & \req 
                    & {}   & {}   & {}   & {}   & {}   \\
\midrule
   Fourier-trunc    & \req & \opt & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & \req & \req & {}   \\
\midrule
   up-down          & \req & \opt & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   \\
%
\bottomrule
\end{tabular}

% ---------------------------------------------------------------------- %
\begin{longtable}{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}

\toprule
  initss & 
       \rotatebox{90}{ampl_ss}   & 
       \rotatebox{90}{radius_ss}   & 
       \rotatebox{90}{widthss}   & 
       \rotatebox{90}{epsilon_ss}   & 
       \rotatebox{90}{grads0}  & 
       \rotatebox{90}{pertss}   & 
       \rotatebox{90}{ss_left}  & 
       \rotatebox{90}{ss_right} & 
       \rotatebox{90}{ss_const} & 
       \rotatebox{90}{mpoly0} & 
       \rotatebox{90}{mpoly1}  & 
       \rotatebox{90}{mpoly2} & 
       \rotatebox{90}{isothtop}    & 
       \rotatebox{90}{khor_ss}    & 
       \rotatebox{90}{center1_x}    & 
       \rotatebox{90}{center1_y}    & 
       \rotatebox{90}{center1_z}    & 
       \rotatebox{90}{center2_x}    & 
       \rotatebox{90}{center2_y}    & 
       \rotatebox{90}{center2_z}    &
       \rotatebox{90}{thermal_background}    &
       \rotatebox{90}{thermal_peak}    &
       \rotatebox{90}{thermal_scaling}    \\ 
\midrule
  zero              & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  const_ss          & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & \req & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  blob              & \req & \req & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  isothermal        & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  Ferri{\`e}re      & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  xjump             & {}   & {}   & \req & {}   & {}   & {}   & \req 
                    & \req & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  hor-fluxtube      & \req & \req & {}   & \req & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  hor-tube          & \req & \req & {}   & \req & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  sedov             & {}   & \req & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & \req & \req & \req & {}   & {}   & {}   & \req
                    & \req & \req \\
\midrule
  sedov-dual        & {}   & \req & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & \req & \req & \req & \req & \req & \req & \req
                    & \req & \req \\
\midrule
  isobaric          & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  isentropic        & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  linprof           & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  piecew-poly       & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
\midrule
  polytropic        & {}   & {}   & {}   & {}   & {}   & {}   & {}   
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   & {}   & {}   & {}   & {}   & {}
                    & {}   & {}   \\
%
\bottomrule
\end{longtable}

 
% ====================================================================== %

% r e f
\begin{thebibliography}{99}

\bibitem{Abramowitz-Stegun} Abramowitz, A., Stegun, I. A.,
  \emph{Pocketbook of Mathematical Functions\/},
  Harri Deutsch, Frankfurt (1984)

\bibitem{Ref-2} Brandenburg, A.,
  \emph{Astrophys. J.} \textbf{550}, 824--840 (2001)
  ``The inverse cascade and nonlinear alpha-effect in simulations
  of isotropic helical hydromagnetic turbulence''

\bibitem{Ref-1} Brandenburg, A., in \emph{Advances in non-linear dynamos},
  ed.\ A.\ Ferriz-Mas \& M.\ N\'u\~nez Jim\'enez,
  (The Fluid Mechanics of Astrophysics and Geophysics, Vol.\ {\bf9})
  Taylor \& Francis, London and New York, pp.~269--344 (2003);
  \url{http://arXiv.org/abs/astro-ph/0109497}

\bibitem{Ref-4} Brandenburg, A., Dobler, W.,
  \emph{Astron. Astrophys.} \textbf{369}, 329--338 (2001)
  ``Large scale dynamos with helicity loss through boundaries''

\bibitem{BH01} Brandenburg, A., \& Hazlehurst, J.,
  \emph{Astron. Astrophys.} \textbf{370}, 1092--1102 (2001)
  ``Evolution of highly buoyant thermals in a stratified layer''

\bibitem{BDS02}Brandenburg, A., Dobler, W., \& Subramanian, K.,
  \emph{Astron. Nachr.} \textbf{323}, 99--122 (2002)
  ``Magnetic helicity in stellar dynamos: new numerical experiments''

\bibitem{BJNRST96} Brandenburg, A., Jennings, R. L., Nordlund, \AA.,
  Rieutord, M., Stein, R. F., \& Tuominen, I.,
  \emph{J. Fluid Mech.} \textbf{306}, 325--352 (1996)
  ``Magnetic structures in a dynamo simulation''

\bibitem{BMS95}Brandenburg, A., Moss, D., \& Shukurov, A.,
  \emph{MNRAS} \textbf{276}, 651--662 (1995)
  ``Galactic fountains as magnetic pumps''

\bibitem{BNST95} Brandenburg, A., Nordlund, \AA., Stein, R. F.,
  \& Torkelsson, U.,
  \emph{Astrophys. J.} \textbf{446}, 741--754 (1995)
  ``Dynamo-generated turbulence and large scale magnetic fields
  in a Keplerian shear flow''

\bibitem{Collatz66}Collatz, L.,
  \emph{The numerical treatment of differential equations},
  Springer-Verlag, New York, p.\ 164 (1966)

\bibitem{Gammie2001}Gammie, C.~F.,
  \emph{Astrophys. J.} \textbf{553}, 174--183 (2001)
  ``Nonlinear outcome of gravitational instability in cooling, gaseous disks''

\bibitem{GNG87}Goodman, J., Narayan, R. \& Goldreich, P.,
  \emph{Month. Not. Roy. Soc.} \textbf{225}, 695--711 (1987)
  ``The stability of accretion tori -- II. Nonlinear evolution
  to discrete planets''

\bibitem{HockneyEastwood1981}Hockney, R.~W., \& Eastwood, J.~W.,
  \emph{Computer Simulation Using Particles},
  McGraw-Hill, New York (1981)

\bibitem{HTM84}Hurlburt, N. E., Toomre, J., \& Massaguer, J. M.,
  \emph{Astrophys. J.} \textbf{282}, 557--573 (1984)
  ``Two-dimensional compressible convection extending over multiple scale
  heights''

\bibitem{KW90} Kippenhahn, R. \& Weigert, A.
  \emph{Stellar structure and evolution}, Springer: Berlin (1990)

\bibitem{KR80} Krause, F., R\"adler, K.-H.,
  \emph{Mean-Field Magneto\-hy\-dro\-dy\-na\-mics and Dynamo Theory\/},
  Akademie-Verlag, Berlin; also Pergamon Press, Oxford (1980)

\bibitem{Lele92}Lele, S. K.,
  \emph{J. Comp. Phys.} \textbf{103}, 16--42 (1992)
  ``Compact finite difference schemes with spectral-like resolution''

\bibitem{NG95} Nordlund, \AA., \& Galsgaard, K.,
{\it A 3D MHD code for Parallel Computers},
{\url{http://www.astro.ku.dk/~aake/NumericalAstro/papers/kg/mhd.ps.gz}}
(1995)

\bibitem{NS90} Nordlund, \AA., Stein, R. F.,
  \emph{Comput. Phys. Commun.} \textbf{59}, 119 (1990)
  ``3-D simulations of solar and stellar convection and magnetoconvection''

\bibitem{NR} Press, W., Teukolsky, S., Vetterling, W., \& Flannery, B.,
  \emph{Numerical Recipes in Fortran 90}, 2nd ed., Cambridge (1996)

\bibitem{Snodin}
Snodin, A. P., Brandenburg, A., Mee, A. J., \& Shukurov, A.\smn{2005}
{Cosmic ray confinement in the diffusion approximation}
(\astroph{0507176})

\bibitem{SH88} Stanescu, D., Habashi, W. G.,
  \emph{J. Comp. Phys.} \textbf{143}, 674 (1988)
  ``$2N$-storage low dissipation and dispersion Runge--Kutta
  schemes for computational acoustics''

\bibitem{2Nstorage} Williamson, J. H.,
  \emph{J. Comp. Phys.} \textbf{35}, 48 (1980)
  ``Low-storage Runge--Kutta schemes''

\end{thebibliography}


\cleardoublepage
% ====================================================================== %

\part{Indexes}
\printindex[file]
\printindex[var]
\printindex[default][This index contains options, names, definitions and
  commands. Files and variables have their own indexes.]

\ \vfill\bigskip\noindent{\footnotesize\it
$ $Id: manual.tex,v 1.366 2007-08-19 08:55:28 brandenb Exp $ $}


\end{document}

%%% Please leave this for Emacs [wd]:

%% Local Variables:
%% ispell-check-comments: t
%% Local IspellDict: american
%% End:
% LocalWords:  SPH CVS tex wd MHD makeindex pdflatex MPI Dobler nonperiodic src
% LocalWords:  nonmagnetic nomagnetic IDL DX OpenDX csh Perl Perl Cygwin tgz mv
% LocalWords:  tarball unix somewhere cd gunzip xf mhd passwd CVSROOT cshrc sh
% LocalWords:  setenv cvs cvspass code's dP sourceme mkdir tmp struct Weyl vidx
% LocalWords:  isotrop keepaspectratio spher gravz README cparam idl dx nograv
% LocalWords:  nohydro nodensity noentropy rhs noforcing rr noglobal fft FFT io
% LocalWords:  nofft dist mpicomm nompicomm DFUNDERSCORE nodebug mkfile var dat
% LocalWords:  interoperability rall bb aa jj Pvwave param nml param nml param
% LocalWords:  stdout oum zaver bxmz bymz nl timestr lp ip ip iper iperx ztop
% LocalWords:  hcond whcond mpoly isothtop ampl init urand cs nt dt cdt CFL bc
% LocalWords:  cdtv Alfv isave isave iorder iorder Kutta dsnap dsnap dvid dtmin
% LocalWords:  tinit tdamp dampu dampuext rdamp wdamp ivisc cdiffrho tvid std
% LocalWords:  wcool iforce relhel tsnap isothermality initaa jbm Peclet Eq Ra
% LocalWords:  meshpoints jb mn rprint xy Compaq MB resol linux magn noentro nq
% LocalWords:  ukaff jun sourcefile kbd env rm dfn MNRAS url html dobler Exp ic
% LocalWords:  ODEs PDEs iterform ccccccc pscalar nopscalar urms rhom Lxyz xyz
% LocalWords:  cvsid lperi lwrite lnowrite inituu ampluu widthuu uu initlnrho
% LocalWords:  widthlnrho piecew zref ampllnrho const initss linprof pertss ss
% LocalWords:  khor gaussian hor fluxtube fluxlayer Bx Bz fluxrings fluxrings
% LocalWords:  Alfven circ amplaa fring Iring Rring wr epsilonaa widthaa nvar
% LocalWords:  kx ky kz wavenumbers lpress equil initlncc ampllncc lncc qshear
% LocalWords:  tmax itorder iwig ialive bcx bcy bcz ce diffrho kinflow ABC diff
% LocalWords:  ff kfountain nomagn entro jul cdata mkcparam inc Phys Lele al hy
% LocalWords:  Nordlund Comput Commun Astrophys Ferriz Mas enez Stanescu adler
% LocalWords:  Habashi dro dy na mics Akademie polytropes nondimensional mvar
% LocalWords:  Boussinesq Wc newphysics nonewphysics fmax fsum daa nx ldiagnos
% LocalWords:  endif lreset iname nname cname cform enddo Pentium GHz Athlon GB
% LocalWords:  SuSE SGI IRIX Tru noshear RedHat ethernet ds pvert umax ssm dtc
% LocalWords:  rms getconf sourced substep perldoc lmpi llam FC FFLAGS LDMPI mk
% LocalWords:  SunOS UNICOS UX MPP AIX xyaver bmy bmx bxmxy bymxy bzmxy conv dz
% LocalWords:  evol hsect hsections vsect vsections vsections texinfo nxgrid ke
% LocalWords:  nygrid nzgrid ccc widthss brandenb Nils Pariev tcsh ls ln rcool
% LocalWords:  cincinnatus CEST mx mz lnrho pf nprocy ncpus nprocz cooltype cT
% LocalWords:  nprocx MIPSpro sourcing unsetenv alltt wsnaps timesteps kB Fbot
% LocalWords:  ttransient fe aug Gridur isentropy Eqs idxsty uplink equ sep nr
% LocalWords:  zaverages Flannery Teukolsky Vetterling rllrlrll Rasm ptimings
% LocalWords:  Marsaglia Gnuplot gnuplot Mattias vind polytrope Hurlburt Toomre
% LocalWords:  noradiation mpio ux uy discont trunc xjump kb sinxsinz Alfvenz
% LocalWords:  orms omax uxm uym uzm eth ekin brms bmax jrms jmax abm bmz ccmax
% LocalWords:  rhoccm Torkelsson Rieutord Tuominen Hazlehurst Shukurov fidx Ctl
% LocalWords:  logscale oct Nqall ruxm ruym ruzm jm vArms vAmax Massaguer ts ev
% LocalWords:  openDX Eqn videofiles otal vm OPTFLAGS notransform ifc nov cyl
% LocalWords:  convsample Chandrasekhar eqn timestep nonhelical hsection RK pp
% LocalWords:  vsection params remesh Marms Mamax Nstorage NS BNST KR BH BMS GB
% LocalWords:  HTM BDS Subramanian Nachr ajwm cdts postproc decfort idx Hwwsx
% LocalWords:  WorkShop convstar jan reftex varfile datadir Gyr realtodouble ok
% LocalWords:  dspec vel fftpack oned powerb poweru powerbx powerby powerbz sfu
% LocalWords:  powerux poweruy poweruz lsfu lsfb lsfz Elsasser FUNC func sfb lr
% LocalWords:  sfz Saha Sackur Ioni exp umy umx umz mpi comm fno MHDturb feb uz
% LocalWords:  initcond nomag apr giga nfilter ppowercomp Narayan Goldreich sim
% LocalWords:  lupw Collatz Mehrstellen Verfahren hyperviscosity Galsgaard radi
% LocalWords:  Raphson trans Ylm rrr Coord diag Stegun acroread cgs avg mtavg
% LocalWords:  visc walltime ekintot ethtot sedov Abramowitz Harri GNG tavg tst
% LocalWords:  BJNRST Ferri cleanf rundir cvsci gpgrowth mkdatadir mkdotin pacx
% LocalWords:  mkinpars mkproc mkwww scpdatadir backtick pTTss gcc glibc LD IGG
% LocalWords:  Milano IECR ILNTT NOERASE checkin cvsstat pkeff NEWDIR copson ks
% LocalWords:  hyperviscous geodynamo DM coords substeps dec noionization pc ll
% LocalWords:  Kramers Myr Spitzer Kippenhahn Weigert Remeshing remeshing coef
% LocalWords:  phiaver pE pF datafiles PNG MPEG mpeg Secs mphi chiral autoconf
% LocalWords:  automake wd's NEC ifpdf lll vpariev torkel Ulf theine Heinemann
% LocalWords:  Nordita tarek Tarek Yousef snod Snodin pkapyla apyl Oulu Erland
% LocalWords:  Haugen ngrs Sarson mee tOnY Mee mcmillan mattias Christensson ph
% LocalWords:  dorch Bertil Dorch christer Christer Sandin Uppsala Bingert nav
% LocalWords:  amjed Amjed Mohammed sprache english ajohan astro cond bio Lett
% LocalWords:  Astron Mech Geophys Dyn Europhys Bifurc Solitons Fickian Multam
% LocalWords:  protoplanetary aki Astrobiol Stix Anja PPARC Odense userid grav
% LocalWords:  Xmgrace namelist hostname janus llll CFLAGS DFUNDERSC ifort YEXT
% LocalWords:  Absoft LCS pgf Mips mips xlf qsuffix df lequidist sinh coeff mT
% LocalWords:  proc Gnuplot's newrun monoatomic nK kG Upwinding upwinding vT ij
% LocalWords:  iso viewport Gigabit qmax imax HII Htot pT Tetrode ik dI mon ikl
% LocalWords:  jk je nonequidistant stacksize ulimit KMP IOT UTF WD's epsi tini
% LocalWords:  evr xz yz workq subclusters gigabit Gb uplinks OSF Ny Beltrami
% LocalWords:  varphi cccc gd hydrostat lcc regridding isocontours num eff dF
% LocalWords:  upwinded centr hyperdiffusion hyperdiffusive meshpoint kpc uint
% LocalWords:  dampint dampext uext unst irrotational dV cr poly nez
